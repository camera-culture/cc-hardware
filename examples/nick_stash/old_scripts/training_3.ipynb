{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "height = 4\n",
    "width = 4\n",
    "depth = 24\n",
    "\n",
    "start_bin = 4\n",
    "end_bin = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all_hists_rel = np.empty((0, end_bin - start_bin, height, width))\n",
    "combined_labels = np.empty((0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hists = []\n",
    "labels = []\n",
    "zero_hists = []\n",
    "\n",
    "zone_to_label = {\n",
    "    0: [0, 0, 0],\n",
    "    1: [1, 0, 0],\n",
    "    2: [0, 1, 0],\n",
    "    3: [0, 0, 1],\n",
    "    4: [1, 1, 0],\n",
    "    5: [1, 0, 1],\n",
    "    6: [0, 1, 1],\n",
    "    7: [1, 1, 1],\n",
    "}\n",
    "\n",
    "# zone_to_label = {\n",
    "#     0: [0],\n",
    "#     1: [1],\n",
    "#     2: [1],\n",
    "#     3: [1],\n",
    "#     4: [1],\n",
    "#     5: [1],\n",
    "#     6: [1],\n",
    "#     7: [1],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     hists = np.load(f'datasets/display-box-3/histograms_{i}.npy')\n",
    "#     all_hists.append(hists)\n",
    "\n",
    "#     if i == 0:\n",
    "#         zero_hists.append(hists)\n",
    "\n",
    "#     labels += [zone_to_label[i]] * len(hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    hists = np.load(f'../datasets/display-box/display-box-12/histograms_{i}.npy')\n",
    "\n",
    "    hists = hists.reshape(-1, height, width, depth)\n",
    "    # move depth to the front\n",
    "    hists = np.moveaxis(hists, -1, 1)\n",
    "\n",
    "    # look at first 10 bins\n",
    "    # data = np.array(hists[:, :10, :, :])\n",
    "    # data = np.array(hists[:, :, :, :])\n",
    "    data = hists\n",
    "\n",
    "    # Compute the mean and standard deviation for each position (10, 4, 4) across all samples\n",
    "    mean = data.mean(axis=0)  # Shape: (10, 4, 4)\n",
    "    std = data.std(axis=0)    # Shape: (10, 4, 4)\n",
    "\n",
    "    # Compute the threshold for values being within 3 standard deviations\n",
    "    lower_bound = mean - 3 * std\n",
    "    upper_bound = mean + 3 * std\n",
    "\n",
    "    # Only consider the first n values along the 10-axis (shape: n x 4 x 4)\n",
    "    n = 4\n",
    "    data_to_check = data[:, :n, :, :]  # Shape: (4000, n, 4, 4)\n",
    "    lower_bound_check = lower_bound[:n, :, :]  # Shape: (n, 4, 4)\n",
    "    upper_bound_check = upper_bound[:n, :, :]  # Shape: (n, 4, 4)\n",
    "\n",
    "    # Identify samples where all values in the first 3 indices along the 10-axis are within bounds\n",
    "    valid_mask = np.all((data_to_check >= lower_bound_check) & (data_to_check <= upper_bound_check), axis=(1, 2, 3))\n",
    "\n",
    "    # Apply the mask to filter the samples\n",
    "    filtered_data = data[valid_mask]\n",
    "\n",
    "    hists = filtered_data\n",
    "\n",
    "    # sliding window mean smoothing\n",
    "    # Compute the sliding window mean\n",
    "    k = 5\n",
    "    sliding_mean = np.array([\n",
    "        hists[j - k + 1 : j + 1].mean(axis=0)  # Mean of the last k elements\n",
    "        for j in range(k - 1, len(hists))\n",
    "    ])\n",
    "    \n",
    "    hists = sliding_mean\n",
    "\n",
    "    # generate more data by adding gaussian noise with std of the actual data std (changes for bin/pixel)\n",
    "    # std = hists.std(axis=0)\n",
    "    # hists = np.repeat(hists, 5, axis=0)\n",
    "    # hists += np.random.normal(0, std * 0.5, hists.shape)\n",
    "\n",
    "    if i == 0:\n",
    "        zero_hists.append(hists)\n",
    "\n",
    "    else:\n",
    "        all_hists.append(hists)\n",
    "        labels += [zone_to_label[i]] * len(hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_hists = np.concatenate(zero_hists, axis=0)\n",
    "all_hists = np.concatenate(all_hists, axis=0)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# all_hists = all_hists.reshape(-1, height, width, depth)\n",
    "# # move depth to the front\n",
    "# all_hists = np.moveaxis(all_hists, -1, 1)\n",
    "\n",
    "# zero_hists = zero_hists.reshape(-1, height, width, depth)\n",
    "# zero_hists = np.moveaxis(zero_hists, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the mean and standard deviation for each position (24, 4, 4) across all samples\n",
    "# mean = all_hists.mean(axis=0)  # Shape: (24, 4, 4)\n",
    "# std = all_hists.std(axis=0)    # Shape: (24, 4, 4)\n",
    "\n",
    "# # Compute the threshold for values being within 3 standard deviations\n",
    "# lower_bound = mean - 3 * std\n",
    "# upper_bound = mean + 3 * std\n",
    "\n",
    "# # Only consider the first n values along the depth axis (shape: n x 4 x 4)\n",
    "# n = 4\n",
    "# data_to_check = all_hists[:, :n, :, :]  # Shape: (4000, n, 4, 4)\n",
    "# lower_bound_check = lower_bound[:n, :, :]  # Shape: (n, 4, 4)\n",
    "# upper_bound_check = upper_bound[:n, :, :]  # Shape: (n, 4, 4)\n",
    "\n",
    "# # Identify samples where all values in the first 3 indices along the depth axis are within bounds\n",
    "# valid_mask = np.all((data_to_check >= lower_bound_check) & (data_to_check <= upper_bound_check), axis=(1, 2, 3))\n",
    "\n",
    "# # Apply the mask to filter the samples\n",
    "# filtered_all_hists = all_hists[valid_mask]\n",
    "# filtered_labels = labels[valid_mask]\n",
    "\n",
    "# # Check the shapes of the original and filtered arrays\n",
    "# print(f\"Original shape: {all_hists.shape}\")\n",
    "# print(f\"Filtered shape: {filtered_all_hists.shape}\")\n",
    "# all_hists = filtered_all_hists\n",
    "# labels = filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop first bounce bins (first bounce in first 2 bins)\n",
    "# crop bins that are too far and noisy\n",
    "all_hists = all_hists[:, start_bin:end_bin, :, :]\n",
    "zero_hists = zero_hists[:, start_bin:end_bin, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate more data by adding gaussian noise\n",
    "\n",
    "# all_hists = np.repeat(all_hists, 5, axis=0)\n",
    "# labels = np.repeat(labels, 5, axis=0)\n",
    "\n",
    "# add noise\n",
    "# std = 3 is good for general training?\n",
    "# all_hists += np.random.normal(0, 1, all_hists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hists = torch.tensor(all_hists, dtype=torch.float32)\n",
    "zero_hists = torch.tensor(zero_hists, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 20 random zero hists to act as the zero mean\n",
    "num_samples_to_mean = 20\n",
    "\n",
    "random_zero_mean = torch.empty((all_hists.shape[0], all_hists.shape[1], height, width))\n",
    "\n",
    "for i in range(all_hists.shape[0]):\n",
    "    indices = torch.randint(0, zero_hists.shape[0], (num_samples_to_mean,))\n",
    "    random_zero_mean[i] = zero_hists[indices].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize to 0-1 values\n",
    "# all_hist_mins = all_hists.amin(dim=(1, 2, 3), keepdim=True)\n",
    "# all_hist_maxs = all_hists.amax(dim=(1, 2, 3), keepdim=True)\n",
    "# all_hist_ranges = all_hist_maxs - all_hist_mins\n",
    "# all_hist_ranges[all_hist_ranges == 0] = 1\n",
    "# all_hists = (all_hists - all_hist_mins) / all_hist_ranges\n",
    "\n",
    "# # normalize to 0-1 values\n",
    "# random_zero_mean_mins = random_zero_mean.amin(dim=(1, 2, 3), keepdim=True)\n",
    "# random_zero_mean_maxs = random_zero_mean.amax(dim=(1, 2, 3), keepdim=True)\n",
    "# random_zero_mean_ranges = random_zero_mean_maxs - random_zero_mean_mins\n",
    "# random_zero_mean_ranges[random_zero_mean_ranges == 0] = 1\n",
    "# random_zero_mean = (random_zero_mean - random_zero_mean_mins) / random_zero_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hists_rel = all_hists - random_zero_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean all pixels\n",
    "# all_hists_rel = all_hists_rel.mean(dim=(2, 3), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_mean = np.mean(zero_hists, axis=0)\n",
    "\n",
    "# # lower bound at 0\n",
    "# # all_hists = np.maximum(all_hists, 0)\n",
    "# # zero_mean = np.maximum(zero_mean, 0)\n",
    "# all_hists_rel = all_hists - zero_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of all_hists: torch.Size([2769, 12, 4, 4])\n",
      "shape of labels: (2769, 3)\n",
      "shape of all_hists_rel: torch.Size([2769, 12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# chunk sizes\n",
    "print(f'shape of all_hists: {all_hists.shape}')\n",
    "print(f'shape of labels: {labels.shape}')\n",
    "print(f'shape of all_hists_rel: {all_hists_rel.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all_hists_rel = np.concatenate([combined_all_hists_rel, all_hists_rel], axis=0)\n",
    "combined_labels = np.concatenate([combined_labels, labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of all_hists_rel: (2769, 12, 4, 4)\n",
      "shape of labels: (2769, 3)\n"
     ]
    }
   ],
   "source": [
    "# final dataset sizes\n",
    "print(f'shape of all_hists_rel: {combined_all_hists_rel.shape}')\n",
    "print(f'shape of labels: {combined_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "# all_hists_tensor = torch.tensor(all_hists, dtype=torch.float32)\n",
    "# all_hists_tensor = torch.tensor(all_hists_rel, dtype=torch.float32)\n",
    "all_hists_tensor = torch.tensor(combined_all_hists_rel, dtype=torch.float32)\n",
    "# labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(combined_labels, dtype=torch.float32)\n",
    "\n",
    "# all_hists_tensor = torch.sign(all_hists_tensor) * torch.log1p(torch.abs(all_hists_tensor))\n",
    "\n",
    "\n",
    "# epsilon = 0.1  # Smoothing factor\n",
    "# labels_tensor = (1 - epsilon) * labels_tensor + epsilon * 0.5  # Smooth towards uniform distribution\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(all_hists_tensor, labels_tensor)\n",
    "\n",
    "# Define the sizes for training, validation, and test sets\n",
    "train_size = int(0.5 * len(dataset))\n",
    "val_size = int(0.25 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size],\n",
    "                                                        generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x shape: torch.Size([32, 12, 4, 4])\n",
      "batch_y shape: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_loader:\n",
    "    print(f'batch_x shape: {batch_x.shape}')\n",
    "    print(f'batch_y shape: {batch_y.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CounterCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CounterCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=(end_bin - start_bin), out_channels=16, kernel_size=3, padding=1)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "#         # self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "#         self.fc1_bn = nn.BatchNorm1d(128)\n",
    "#         self.fc2 = nn.Linear(128, 3)  # Assuming 10 classes for the labels\n",
    "#         self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "#         self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(f'x shape at start: {x.shape}')\n",
    "#         x = self.relu(self.conv1(x))\n",
    "#         # print(f'x shape after conv1: {x.shape}')\n",
    "#         x = self.batchnorm1(x)\n",
    "#         # x = self.pool(x)\n",
    "#         # print(f'x shape after pool1: {x.shape}')\n",
    "#         x = self.relu(self.conv2(x))\n",
    "#         # print(f'x shape after conv2: {x.shape}')\n",
    "#         x = self.batchnorm2(x)\n",
    "#         # x = self.pool(x)\n",
    "#         # print(f'x shape after pool2: {x.shape}')\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         # print(f'x shape after flatten: {x.shape}')\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc1_bn(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = CounterCNN().to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CounterCNN(\n",
      "  (conv3d): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (batchnorm3d): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
      "  (fc1_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.7, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CounterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CounterCNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(in_channels=(end_bin - start_bin), out_channels=16, kernel_size=3, padding=1)\n",
    "        # self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=1)\n",
    "        # self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        out_channels = 4\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=out_channels, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.batchnorm3d = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels * (end_bin - start_bin) * height * width, 128)\n",
    "\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # Assuming 10 classes for the labels\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # # print(f'x shape at start: {x.shape}')\n",
    "        # x = self.relu(self.conv1(x))\n",
    "        # # print(f'x shape after conv1: {x.shape}')\n",
    "        # x = self.batchnorm1(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # print(f'x shape after pool1: {x.shape}')\n",
    "        # x = self.relu(self.conv2(x))\n",
    "        # # print(f'x shape after conv2: {x.shape}')\n",
    "        # x = self.batchnorm2(x)\n",
    "\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "\n",
    "        # x = self.pool(x)\n",
    "        # print(f'x shape after pool2: {x.shape}')\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CounterCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CounterCNN(\n",
       "  (conv3d): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (batchnorm3d): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (fc1_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, counter=False, clipping=False, debug=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        if len(X) < batch_size:\n",
    "            continue\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # if counter:\n",
    "            # y = y.unsqueeze(1)\n",
    "        # print(f'pred shape: {pred.shape}, y shape: {y.shape}')\n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(f'loss: {loss.item()}')\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        if clipping:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Adjust max_norm as needed\n",
    "        \n",
    "        if debug:\n",
    "            # Inspect gradients for each layer\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:  # Only check if gradient is computed\n",
    "                    print(f\"Layer: {name} | Gradient mean: {param.grad.abs().mean().item()} | Gradient max: {param.grad.abs().max().item()}\")\n",
    "                else:\n",
    "                    print(f\"Layer: {name} has no gradient.\")\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= len(dataloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_counter(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            # y = y.unsqueeze_(1)\n",
    "            # print(X.shape)\n",
    "            # print(y.shape)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # final_pred = torch.round(torch.clamp(pred, min=0, max=1))\n",
    "            final_pred = torch.round(torch.sigmoid(pred))\n",
    "            \n",
    "            # print(final_pred.shape)\n",
    "            # print(\"true\")\n",
    "            # print(y)\n",
    "            # print(\"pred\")\n",
    "            # print(final_pred)\n",
    "            # print(\"diff\")\n",
    "            # print(final_pred - y)\n",
    "            exact_match = torch.all(final_pred == torch.round(y), dim=1)\n",
    "            correct += torch.sum(exact_match).item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_early_stopping(train_loader, val_loader, model, loss_fn, optimizer, \n",
    "    epochs=50, early_stopping=True, patience=5, threshold=0.15, counter=False, clipping=False, debug=False):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss = train(train_loader, model, loss_fn, optimizer, counter=counter, clipping=clipping, debug=debug)\n",
    "        if counter:\n",
    "            val_loss, correct = test_counter(val_loader, model, loss_fn)\n",
    "        else:\n",
    "            val_loss, correct = test(val_loader, model, loss_fn)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                if val_loss / best_val_loss > 1 + threshold:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {t+1}\")\n",
    "                        break\n",
    "        # print(f'patience_counter: {patience_counter}')\n",
    "    return best_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.148295  [   32/42552]\n",
      "loss: 0.522126  [ 3232/42552]\n",
      "loss: 0.415015  [ 6432/42552]\n",
      "loss: 0.480581  [ 9632/42552]\n",
      "loss: 0.557398  [12832/42552]\n",
      "loss: 0.538285  [16032/42552]\n",
      "loss: 0.396060  [19232/42552]\n",
      "loss: 0.586628  [22432/42552]\n",
      "loss: 0.475524  [25632/42552]\n",
      "loss: 0.276104  [28832/42552]\n",
      "loss: 0.646092  [32032/42552]\n",
      "loss: 0.450679  [35232/42552]\n",
      "loss: 0.418510  [38432/42552]\n",
      "loss: 0.482596  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 0.404847 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.544718  [   32/42552]\n",
      "loss: 0.427432  [ 3232/42552]\n",
      "loss: 0.558059  [ 6432/42552]\n",
      "loss: 0.628621  [ 9632/42552]\n",
      "loss: 0.516215  [12832/42552]\n",
      "loss: 0.485799  [16032/42552]\n",
      "loss: 0.483816  [19232/42552]\n",
      "loss: 0.494134  [22432/42552]\n",
      "loss: 0.463896  [25632/42552]\n",
      "loss: 0.497203  [28832/42552]\n",
      "loss: 0.554068  [32032/42552]\n",
      "loss: 0.439509  [35232/42552]\n",
      "loss: 0.462554  [38432/42552]\n",
      "loss: 0.305180  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 0.385199 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.456229  [   32/42552]\n",
      "loss: 0.453589  [ 3232/42552]\n",
      "loss: 0.416302  [ 6432/42552]\n",
      "loss: 0.467624  [ 9632/42552]\n",
      "loss: 0.435325  [12832/42552]\n",
      "loss: 0.412763  [16032/42552]\n",
      "loss: 0.350702  [19232/42552]\n",
      "loss: 0.493309  [22432/42552]\n",
      "loss: 0.351765  [25632/42552]\n",
      "loss: 0.472063  [28832/42552]\n",
      "loss: 0.410169  [32032/42552]\n",
      "loss: 0.372844  [35232/42552]\n",
      "loss: 0.435611  [38432/42552]\n",
      "loss: 0.473402  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 0.381019 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.441717  [   32/42552]\n",
      "loss: 0.593374  [ 3232/42552]\n",
      "loss: 0.529152  [ 6432/42552]\n",
      "loss: 0.355503  [ 9632/42552]\n",
      "loss: 0.449020  [12832/42552]\n",
      "loss: 0.420010  [16032/42552]\n",
      "loss: 0.426999  [19232/42552]\n",
      "loss: 0.323492  [22432/42552]\n",
      "loss: 0.350258  [25632/42552]\n",
      "loss: 0.484959  [28832/42552]\n",
      "loss: 0.505345  [32032/42552]\n",
      "loss: 0.333172  [35232/42552]\n",
      "loss: 0.419726  [38432/42552]\n",
      "loss: 0.393337  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 0.373305 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.380018  [   32/42552]\n",
      "loss: 0.346952  [ 3232/42552]\n",
      "loss: 0.463796  [ 6432/42552]\n",
      "loss: 0.434524  [ 9632/42552]\n",
      "loss: 0.280403  [12832/42552]\n",
      "loss: 0.293925  [16032/42552]\n",
      "loss: 0.387647  [19232/42552]\n",
      "loss: 0.410119  [22432/42552]\n",
      "loss: 0.465787  [25632/42552]\n",
      "loss: 0.486929  [28832/42552]\n",
      "loss: 0.508941  [32032/42552]\n",
      "loss: 0.444742  [35232/42552]\n",
      "loss: 0.417578  [38432/42552]\n",
      "loss: 0.374989  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 0.369235 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.392772  [   32/42552]\n",
      "loss: 0.345625  [ 3232/42552]\n",
      "loss: 0.550200  [ 6432/42552]\n",
      "loss: 0.377653  [ 9632/42552]\n",
      "loss: 0.296091  [12832/42552]\n",
      "loss: 0.468756  [16032/42552]\n",
      "loss: 0.428400  [19232/42552]\n",
      "loss: 0.287475  [22432/42552]\n",
      "loss: 0.399783  [25632/42552]\n",
      "loss: 0.382028  [28832/42552]\n",
      "loss: 0.349699  [32032/42552]\n",
      "loss: 0.352866  [35232/42552]\n",
      "loss: 0.423479  [38432/42552]\n",
      "loss: 0.411529  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 0.361613 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.284081  [   32/42552]\n",
      "loss: 0.476891  [ 3232/42552]\n",
      "loss: 0.394887  [ 6432/42552]\n",
      "loss: 0.519614  [ 9632/42552]\n",
      "loss: 0.346870  [12832/42552]\n",
      "loss: 0.439466  [16032/42552]\n",
      "loss: 0.421608  [19232/42552]\n",
      "loss: 0.463736  [22432/42552]\n",
      "loss: 0.411615  [25632/42552]\n",
      "loss: 0.239837  [28832/42552]\n",
      "loss: 0.378214  [32032/42552]\n",
      "loss: 0.344135  [35232/42552]\n",
      "loss: 0.300661  [38432/42552]\n",
      "loss: 0.429494  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 0.355543 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.455336  [   32/42552]\n",
      "loss: 0.404269  [ 3232/42552]\n",
      "loss: 0.398520  [ 6432/42552]\n",
      "loss: 0.506710  [ 9632/42552]\n",
      "loss: 0.360996  [12832/42552]\n",
      "loss: 0.319518  [16032/42552]\n",
      "loss: 0.396356  [19232/42552]\n",
      "loss: 0.512394  [22432/42552]\n",
      "loss: 0.402274  [25632/42552]\n",
      "loss: 0.489827  [28832/42552]\n",
      "loss: 0.397660  [32032/42552]\n",
      "loss: 0.455657  [35232/42552]\n",
      "loss: 0.286041  [38432/42552]\n",
      "loss: 0.464172  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.364382 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.358835  [   32/42552]\n",
      "loss: 0.366102  [ 3232/42552]\n",
      "loss: 0.365643  [ 6432/42552]\n",
      "loss: 0.451681  [ 9632/42552]\n",
      "loss: 0.339706  [12832/42552]\n",
      "loss: 0.435908  [16032/42552]\n",
      "loss: 0.385560  [19232/42552]\n",
      "loss: 0.401940  [22432/42552]\n",
      "loss: 0.569045  [25632/42552]\n",
      "loss: 0.281978  [28832/42552]\n",
      "loss: 0.472347  [32032/42552]\n",
      "loss: 0.363523  [35232/42552]\n",
      "loss: 0.476493  [38432/42552]\n",
      "loss: 0.410689  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 0.346944 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.467223  [   32/42552]\n",
      "loss: 0.463225  [ 3232/42552]\n",
      "loss: 0.295084  [ 6432/42552]\n",
      "loss: 0.242897  [ 9632/42552]\n",
      "loss: 0.356925  [12832/42552]\n",
      "loss: 0.461473  [16032/42552]\n",
      "loss: 0.461050  [19232/42552]\n",
      "loss: 0.292841  [22432/42552]\n",
      "loss: 0.359166  [25632/42552]\n",
      "loss: 0.275098  [28832/42552]\n",
      "loss: 0.418257  [32032/42552]\n",
      "loss: 0.336054  [35232/42552]\n",
      "loss: 0.408753  [38432/42552]\n",
      "loss: 0.420740  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 0.345513 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.418937  [   32/42552]\n",
      "loss: 0.546217  [ 3232/42552]\n",
      "loss: 0.454612  [ 6432/42552]\n",
      "loss: 0.324808  [ 9632/42552]\n",
      "loss: 0.304168  [12832/42552]\n",
      "loss: 0.375746  [16032/42552]\n",
      "loss: 0.474229  [19232/42552]\n",
      "loss: 0.373768  [22432/42552]\n",
      "loss: 0.529116  [25632/42552]\n",
      "loss: 0.380788  [28832/42552]\n",
      "loss: 0.523292  [32032/42552]\n",
      "loss: 0.390399  [35232/42552]\n",
      "loss: 0.465074  [38432/42552]\n",
      "loss: 0.350928  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 0.355651 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.366131  [   32/42552]\n",
      "loss: 0.321460  [ 3232/42552]\n",
      "loss: 0.344109  [ 6432/42552]\n",
      "loss: 0.401415  [ 9632/42552]\n",
      "loss: 0.382012  [12832/42552]\n",
      "loss: 0.411988  [16032/42552]\n",
      "loss: 0.382751  [19232/42552]\n",
      "loss: 0.477567  [22432/42552]\n",
      "loss: 0.416660  [25632/42552]\n",
      "loss: 0.375703  [28832/42552]\n",
      "loss: 0.300246  [32032/42552]\n",
      "loss: 0.286656  [35232/42552]\n",
      "loss: 0.440714  [38432/42552]\n",
      "loss: 0.378458  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 0.342457 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.449338  [   32/42552]\n",
      "loss: 0.289744  [ 3232/42552]\n",
      "loss: 0.366874  [ 6432/42552]\n",
      "loss: 0.436403  [ 9632/42552]\n",
      "loss: 0.392369  [12832/42552]\n",
      "loss: 0.395290  [16032/42552]\n",
      "loss: 0.320204  [19232/42552]\n",
      "loss: 0.422795  [22432/42552]\n",
      "loss: 0.388232  [25632/42552]\n",
      "loss: 0.417998  [28832/42552]\n",
      "loss: 0.419088  [32032/42552]\n",
      "loss: 0.401667  [35232/42552]\n",
      "loss: 0.308999  [38432/42552]\n",
      "loss: 0.317550  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 0.342774 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.413528  [   32/42552]\n",
      "loss: 0.228363  [ 3232/42552]\n",
      "loss: 0.378682  [ 6432/42552]\n",
      "loss: 0.445455  [ 9632/42552]\n",
      "loss: 0.321829  [12832/42552]\n",
      "loss: 0.352438  [16032/42552]\n",
      "loss: 0.416016  [19232/42552]\n",
      "loss: 0.317480  [22432/42552]\n",
      "loss: 0.373126  [25632/42552]\n",
      "loss: 0.243835  [28832/42552]\n",
      "loss: 0.325592  [32032/42552]\n",
      "loss: 0.369154  [35232/42552]\n",
      "loss: 0.343115  [38432/42552]\n",
      "loss: 0.305771  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 0.334034 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.221306  [   32/42552]\n",
      "loss: 0.264392  [ 3232/42552]\n",
      "loss: 0.599880  [ 6432/42552]\n",
      "loss: 0.335954  [ 9632/42552]\n",
      "loss: 0.373238  [12832/42552]\n",
      "loss: 0.335797  [16032/42552]\n",
      "loss: 0.403877  [19232/42552]\n",
      "loss: 0.317972  [22432/42552]\n",
      "loss: 0.329664  [25632/42552]\n",
      "loss: 0.442860  [28832/42552]\n",
      "loss: 0.342826  [32032/42552]\n",
      "loss: 0.322755  [35232/42552]\n",
      "loss: 0.336806  [38432/42552]\n",
      "loss: 0.466451  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.344650 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.266057  [   32/42552]\n",
      "loss: 0.427435  [ 3232/42552]\n",
      "loss: 0.413899  [ 6432/42552]\n",
      "loss: 0.301653  [ 9632/42552]\n",
      "loss: 0.463260  [12832/42552]\n",
      "loss: 0.280080  [16032/42552]\n",
      "loss: 0.372470  [19232/42552]\n",
      "loss: 0.298821  [22432/42552]\n",
      "loss: 0.356665  [25632/42552]\n",
      "loss: 0.400942  [28832/42552]\n",
      "loss: 0.300268  [32032/42552]\n",
      "loss: 0.395776  [35232/42552]\n",
      "loss: 0.281767  [38432/42552]\n",
      "loss: 0.272507  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.330877 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.393022  [   32/42552]\n",
      "loss: 0.301143  [ 3232/42552]\n",
      "loss: 0.335675  [ 6432/42552]\n",
      "loss: 0.403776  [ 9632/42552]\n",
      "loss: 0.317612  [12832/42552]\n",
      "loss: 0.362685  [16032/42552]\n",
      "loss: 0.479355  [19232/42552]\n",
      "loss: 0.295233  [22432/42552]\n",
      "loss: 0.358593  [25632/42552]\n",
      "loss: 0.462847  [28832/42552]\n",
      "loss: 0.397856  [32032/42552]\n",
      "loss: 0.271291  [35232/42552]\n",
      "loss: 0.324843  [38432/42552]\n",
      "loss: 0.324303  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 0.334031 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.481237  [   32/42552]\n",
      "loss: 0.510442  [ 3232/42552]\n",
      "loss: 0.362681  [ 6432/42552]\n",
      "loss: 0.455306  [ 9632/42552]\n",
      "loss: 0.256596  [12832/42552]\n",
      "loss: 0.308126  [16032/42552]\n",
      "loss: 0.514130  [19232/42552]\n",
      "loss: 0.443481  [22432/42552]\n",
      "loss: 0.368877  [25632/42552]\n",
      "loss: 0.371833  [28832/42552]\n",
      "loss: 0.278505  [32032/42552]\n",
      "loss: 0.428154  [35232/42552]\n",
      "loss: 0.276469  [38432/42552]\n",
      "loss: 0.280340  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.330298 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.349170  [   32/42552]\n",
      "loss: 0.341948  [ 3232/42552]\n",
      "loss: 0.365383  [ 6432/42552]\n",
      "loss: 0.468845  [ 9632/42552]\n",
      "loss: 0.461652  [12832/42552]\n",
      "loss: 0.357915  [16032/42552]\n",
      "loss: 0.375589  [19232/42552]\n",
      "loss: 0.333227  [22432/42552]\n",
      "loss: 0.431138  [25632/42552]\n",
      "loss: 0.479714  [28832/42552]\n",
      "loss: 0.405672  [32032/42552]\n",
      "loss: 0.297668  [35232/42552]\n",
      "loss: 0.320757  [38432/42552]\n",
      "loss: 0.349697  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 0.338358 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.412731  [   32/42552]\n",
      "loss: 0.361764  [ 3232/42552]\n",
      "loss: 0.461238  [ 6432/42552]\n",
      "loss: 0.356481  [ 9632/42552]\n",
      "loss: 0.388218  [12832/42552]\n",
      "loss: 0.375403  [16032/42552]\n",
      "loss: 0.537437  [19232/42552]\n",
      "loss: 0.436784  [22432/42552]\n",
      "loss: 0.293269  [25632/42552]\n",
      "loss: 0.231073  [28832/42552]\n",
      "loss: 0.342989  [32032/42552]\n",
      "loss: 0.267061  [35232/42552]\n",
      "loss: 0.362702  [38432/42552]\n",
      "loss: 0.384763  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 0.324326 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.443467  [   32/42552]\n",
      "loss: 0.510974  [ 3232/42552]\n",
      "loss: 0.345618  [ 6432/42552]\n",
      "loss: 0.314162  [ 9632/42552]\n",
      "loss: 0.275268  [12832/42552]\n",
      "loss: 0.483285  [16032/42552]\n",
      "loss: 0.373785  [19232/42552]\n",
      "loss: 0.439798  [22432/42552]\n",
      "loss: 0.414765  [25632/42552]\n",
      "loss: 0.446798  [28832/42552]\n",
      "loss: 0.336655  [32032/42552]\n",
      "loss: 0.308845  [35232/42552]\n",
      "loss: 0.350337  [38432/42552]\n",
      "loss: 0.312945  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 0.319468 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.399972  [   32/42552]\n",
      "loss: 0.236760  [ 3232/42552]\n",
      "loss: 0.379123  [ 6432/42552]\n",
      "loss: 0.262143  [ 9632/42552]\n",
      "loss: 0.298770  [12832/42552]\n",
      "loss: 0.394283  [16032/42552]\n",
      "loss: 0.384906  [19232/42552]\n",
      "loss: 0.549067  [22432/42552]\n",
      "loss: 0.339500  [25632/42552]\n",
      "loss: 0.381474  [28832/42552]\n",
      "loss: 0.414299  [32032/42552]\n",
      "loss: 0.453634  [35232/42552]\n",
      "loss: 0.466468  [38432/42552]\n",
      "loss: 0.360102  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.326489 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.343621  [   32/42552]\n",
      "loss: 0.355873  [ 3232/42552]\n",
      "loss: 0.177941  [ 6432/42552]\n",
      "loss: 0.298331  [ 9632/42552]\n",
      "loss: 0.253078  [12832/42552]\n",
      "loss: 0.233061  [16032/42552]\n",
      "loss: 0.367303  [19232/42552]\n",
      "loss: 0.324320  [22432/42552]\n",
      "loss: 0.260448  [25632/42552]\n",
      "loss: 0.316094  [28832/42552]\n",
      "loss: 0.378877  [32032/42552]\n",
      "loss: 0.499531  [35232/42552]\n",
      "loss: 0.390650  [38432/42552]\n",
      "loss: 0.421975  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.322280 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.261763  [   32/42552]\n",
      "loss: 0.272574  [ 3232/42552]\n",
      "loss: 0.322458  [ 6432/42552]\n",
      "loss: 0.629716  [ 9632/42552]\n",
      "loss: 0.288924  [12832/42552]\n",
      "loss: 0.284867  [16032/42552]\n",
      "loss: 0.459261  [19232/42552]\n",
      "loss: 0.328850  [22432/42552]\n",
      "loss: 0.384782  [25632/42552]\n",
      "loss: 0.407374  [28832/42552]\n",
      "loss: 0.440457  [32032/42552]\n",
      "loss: 0.338676  [35232/42552]\n",
      "loss: 0.462602  [38432/42552]\n",
      "loss: 0.405711  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 0.322201 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.319588  [   32/42552]\n",
      "loss: 0.390986  [ 3232/42552]\n",
      "loss: 0.448542  [ 6432/42552]\n",
      "loss: 0.387691  [ 9632/42552]\n",
      "loss: 0.359862  [12832/42552]\n",
      "loss: 0.365389  [16032/42552]\n",
      "loss: 0.249982  [19232/42552]\n",
      "loss: 0.325214  [22432/42552]\n",
      "loss: 0.264450  [25632/42552]\n",
      "loss: 0.266362  [28832/42552]\n",
      "loss: 0.332000  [32032/42552]\n",
      "loss: 0.353265  [35232/42552]\n",
      "loss: 0.430304  [38432/42552]\n",
      "loss: 0.351273  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.321821 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.353307  [   32/42552]\n",
      "loss: 0.433428  [ 3232/42552]\n",
      "loss: 0.349284  [ 6432/42552]\n",
      "loss: 0.259263  [ 9632/42552]\n",
      "loss: 0.403126  [12832/42552]\n",
      "loss: 0.327730  [16032/42552]\n",
      "loss: 0.269062  [19232/42552]\n",
      "loss: 0.401550  [22432/42552]\n",
      "loss: 0.413875  [25632/42552]\n",
      "loss: 0.404489  [28832/42552]\n",
      "loss: 0.335169  [32032/42552]\n",
      "loss: 0.348178  [35232/42552]\n",
      "loss: 0.281370  [38432/42552]\n",
      "loss: 0.344419  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.320982 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.331078  [   32/42552]\n",
      "loss: 0.314752  [ 3232/42552]\n",
      "loss: 0.390383  [ 6432/42552]\n",
      "loss: 0.392988  [ 9632/42552]\n",
      "loss: 0.344998  [12832/42552]\n",
      "loss: 0.417921  [16032/42552]\n",
      "loss: 0.287904  [19232/42552]\n",
      "loss: 0.247218  [22432/42552]\n",
      "loss: 0.527385  [25632/42552]\n",
      "loss: 0.305562  [28832/42552]\n",
      "loss: 0.443162  [32032/42552]\n",
      "loss: 0.287919  [35232/42552]\n",
      "loss: 0.306824  [38432/42552]\n",
      "loss: 0.412871  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.316585 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.341547  [   32/42552]\n",
      "loss: 0.372269  [ 3232/42552]\n",
      "loss: 0.585910  [ 6432/42552]\n",
      "loss: 0.297718  [ 9632/42552]\n",
      "loss: 0.252464  [12832/42552]\n",
      "loss: 0.257635  [16032/42552]\n",
      "loss: 0.418566  [19232/42552]\n",
      "loss: 0.458797  [22432/42552]\n",
      "loss: 0.355637  [25632/42552]\n",
      "loss: 0.317396  [28832/42552]\n",
      "loss: 0.273849  [32032/42552]\n",
      "loss: 0.297533  [35232/42552]\n",
      "loss: 0.274883  [38432/42552]\n",
      "loss: 0.394343  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.320231 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.386051  [   32/42552]\n",
      "loss: 0.386404  [ 3232/42552]\n",
      "loss: 0.248029  [ 6432/42552]\n",
      "loss: 0.296247  [ 9632/42552]\n",
      "loss: 0.360006  [12832/42552]\n",
      "loss: 0.447859  [16032/42552]\n",
      "loss: 0.249193  [19232/42552]\n",
      "loss: 0.399934  [22432/42552]\n",
      "loss: 0.337210  [25632/42552]\n",
      "loss: 0.403118  [28832/42552]\n",
      "loss: 0.308071  [32032/42552]\n",
      "loss: 0.499539  [35232/42552]\n",
      "loss: 0.514871  [38432/42552]\n",
      "loss: 0.430628  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.326052 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.340995  [   32/42552]\n",
      "loss: 0.325009  [ 3232/42552]\n",
      "loss: 0.363622  [ 6432/42552]\n",
      "loss: 0.375127  [ 9632/42552]\n",
      "loss: 0.291830  [12832/42552]\n",
      "loss: 0.277363  [16032/42552]\n",
      "loss: 0.580718  [19232/42552]\n",
      "loss: 0.403068  [22432/42552]\n",
      "loss: 0.429385  [25632/42552]\n",
      "loss: 0.323524  [28832/42552]\n",
      "loss: 0.268664  [32032/42552]\n",
      "loss: 0.396962  [35232/42552]\n",
      "loss: 0.362128  [38432/42552]\n",
      "loss: 0.355493  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 0.319110 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.405286  [   32/42552]\n",
      "loss: 0.443545  [ 3232/42552]\n",
      "loss: 0.317465  [ 6432/42552]\n",
      "loss: 0.405140  [ 9632/42552]\n",
      "loss: 0.307315  [12832/42552]\n",
      "loss: 0.353945  [16032/42552]\n",
      "loss: 0.289884  [19232/42552]\n",
      "loss: 0.425091  [22432/42552]\n",
      "loss: 0.257023  [25632/42552]\n",
      "loss: 0.420064  [28832/42552]\n",
      "loss: 0.333394  [32032/42552]\n",
      "loss: 0.642645  [35232/42552]\n",
      "loss: 0.345542  [38432/42552]\n",
      "loss: 0.363077  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.306622 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.304718  [   32/42552]\n",
      "loss: 0.420509  [ 3232/42552]\n",
      "loss: 0.460846  [ 6432/42552]\n",
      "loss: 0.312511  [ 9632/42552]\n",
      "loss: 0.420767  [12832/42552]\n",
      "loss: 0.248911  [16032/42552]\n",
      "loss: 0.454137  [19232/42552]\n",
      "loss: 0.355131  [22432/42552]\n",
      "loss: 0.370854  [25632/42552]\n",
      "loss: 0.470012  [28832/42552]\n",
      "loss: 0.266027  [32032/42552]\n",
      "loss: 0.222384  [35232/42552]\n",
      "loss: 0.347999  [38432/42552]\n",
      "loss: 0.386072  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.312270 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.338591  [   32/42552]\n",
      "loss: 0.373440  [ 3232/42552]\n",
      "loss: 0.413673  [ 6432/42552]\n",
      "loss: 0.360305  [ 9632/42552]\n",
      "loss: 0.312465  [12832/42552]\n",
      "loss: 0.223800  [16032/42552]\n",
      "loss: 0.354129  [19232/42552]\n",
      "loss: 0.330313  [22432/42552]\n",
      "loss: 0.233162  [25632/42552]\n",
      "loss: 0.541516  [28832/42552]\n",
      "loss: 0.300660  [32032/42552]\n",
      "loss: 0.369380  [35232/42552]\n",
      "loss: 0.284482  [38432/42552]\n",
      "loss: 0.383470  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.309164 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.348922  [   32/42552]\n",
      "loss: 0.526001  [ 3232/42552]\n",
      "loss: 0.324149  [ 6432/42552]\n",
      "loss: 0.228997  [ 9632/42552]\n",
      "loss: 0.399096  [12832/42552]\n",
      "loss: 0.330072  [16032/42552]\n",
      "loss: 0.326772  [19232/42552]\n",
      "loss: 0.318039  [22432/42552]\n",
      "loss: 0.364024  [25632/42552]\n",
      "loss: 0.388270  [28832/42552]\n",
      "loss: 0.506106  [32032/42552]\n",
      "loss: 0.418215  [35232/42552]\n",
      "loss: 0.275706  [38432/42552]\n",
      "loss: 0.484271  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.313332 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.291885  [   32/42552]\n",
      "loss: 0.339796  [ 3232/42552]\n",
      "loss: 0.336762  [ 6432/42552]\n",
      "loss: 0.340277  [ 9632/42552]\n",
      "loss: 0.261364  [12832/42552]\n",
      "loss: 0.348246  [16032/42552]\n",
      "loss: 0.432197  [19232/42552]\n",
      "loss: 0.432528  [22432/42552]\n",
      "loss: 0.360725  [25632/42552]\n",
      "loss: 0.446050  [28832/42552]\n",
      "loss: 0.326891  [32032/42552]\n",
      "loss: 0.300788  [35232/42552]\n",
      "loss: 0.306620  [38432/42552]\n",
      "loss: 0.378183  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.308672 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.482808  [   32/42552]\n",
      "loss: 0.260044  [ 3232/42552]\n",
      "loss: 0.353803  [ 6432/42552]\n",
      "loss: 0.218760  [ 9632/42552]\n",
      "loss: 0.441265  [12832/42552]\n",
      "loss: 0.401189  [16032/42552]\n",
      "loss: 0.310838  [19232/42552]\n",
      "loss: 0.252962  [22432/42552]\n",
      "loss: 0.275336  [25632/42552]\n",
      "loss: 0.349777  [28832/42552]\n",
      "loss: 0.295957  [32032/42552]\n",
      "loss: 0.244222  [35232/42552]\n",
      "loss: 0.573443  [38432/42552]\n",
      "loss: 0.307271  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.308465 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.338558  [   32/42552]\n",
      "loss: 0.207248  [ 3232/42552]\n",
      "loss: 0.314584  [ 6432/42552]\n",
      "loss: 0.283426  [ 9632/42552]\n",
      "loss: 0.237531  [12832/42552]\n",
      "loss: 0.246131  [16032/42552]\n",
      "loss: 0.341357  [19232/42552]\n",
      "loss: 0.478645  [22432/42552]\n",
      "loss: 0.219868  [25632/42552]\n",
      "loss: 0.414543  [28832/42552]\n",
      "loss: 0.240588  [32032/42552]\n",
      "loss: 0.343274  [35232/42552]\n",
      "loss: 0.264010  [38432/42552]\n",
      "loss: 0.419767  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.310584 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.523983  [   32/42552]\n",
      "loss: 0.272917  [ 3232/42552]\n",
      "loss: 0.496696  [ 6432/42552]\n",
      "loss: 0.371814  [ 9632/42552]\n",
      "loss: 0.310774  [12832/42552]\n",
      "loss: 0.385963  [16032/42552]\n",
      "loss: 0.405432  [19232/42552]\n",
      "loss: 0.370609  [22432/42552]\n",
      "loss: 0.133537  [25632/42552]\n",
      "loss: 0.375888  [28832/42552]\n",
      "loss: 0.298926  [32032/42552]\n",
      "loss: 0.355059  [35232/42552]\n",
      "loss: 0.283339  [38432/42552]\n",
      "loss: 0.443950  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.316342 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.425056  [   32/42552]\n",
      "loss: 0.195814  [ 3232/42552]\n",
      "loss: 0.335512  [ 6432/42552]\n",
      "loss: 0.245773  [ 9632/42552]\n",
      "loss: 0.294230  [12832/42552]\n",
      "loss: 0.413013  [16032/42552]\n",
      "loss: 0.435823  [19232/42552]\n",
      "loss: 0.346367  [22432/42552]\n",
      "loss: 0.273733  [25632/42552]\n",
      "loss: 0.357986  [28832/42552]\n",
      "loss: 0.260817  [32032/42552]\n",
      "loss: 0.357308  [35232/42552]\n",
      "loss: 0.309802  [38432/42552]\n",
      "loss: 0.401567  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.306464 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.277621  [   32/42552]\n",
      "loss: 0.311685  [ 3232/42552]\n",
      "loss: 0.264774  [ 6432/42552]\n",
      "loss: 0.291628  [ 9632/42552]\n",
      "loss: 0.302575  [12832/42552]\n",
      "loss: 0.336882  [16032/42552]\n",
      "loss: 0.342869  [19232/42552]\n",
      "loss: 0.323184  [22432/42552]\n",
      "loss: 0.230317  [25632/42552]\n",
      "loss: 0.360654  [28832/42552]\n",
      "loss: 0.416305  [32032/42552]\n",
      "loss: 0.268874  [35232/42552]\n",
      "loss: 0.429317  [38432/42552]\n",
      "loss: 0.415690  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.302148 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.210227  [   32/42552]\n",
      "loss: 0.418394  [ 3232/42552]\n",
      "loss: 0.512874  [ 6432/42552]\n",
      "loss: 0.372341  [ 9632/42552]\n",
      "loss: 0.449761  [12832/42552]\n",
      "loss: 0.364739  [16032/42552]\n",
      "loss: 0.229120  [19232/42552]\n",
      "loss: 0.326050  [22432/42552]\n",
      "loss: 0.284481  [25632/42552]\n",
      "loss: 0.444865  [28832/42552]\n",
      "loss: 0.290735  [32032/42552]\n",
      "loss: 0.304281  [35232/42552]\n",
      "loss: 0.485975  [38432/42552]\n",
      "loss: 0.281566  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.305707 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.248283  [   32/42552]\n",
      "loss: 0.287057  [ 3232/42552]\n",
      "loss: 0.370762  [ 6432/42552]\n",
      "loss: 0.227273  [ 9632/42552]\n",
      "loss: 0.410116  [12832/42552]\n",
      "loss: 0.272615  [16032/42552]\n",
      "loss: 0.397344  [19232/42552]\n",
      "loss: 0.309875  [22432/42552]\n",
      "loss: 0.297086  [25632/42552]\n",
      "loss: 0.332162  [28832/42552]\n",
      "loss: 0.379697  [32032/42552]\n",
      "loss: 0.357535  [35232/42552]\n",
      "loss: 0.316296  [38432/42552]\n",
      "loss: 0.332776  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.310910 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.353784  [   32/42552]\n",
      "loss: 0.349397  [ 3232/42552]\n",
      "loss: 0.270093  [ 6432/42552]\n",
      "loss: 0.329998  [ 9632/42552]\n",
      "loss: 0.187035  [12832/42552]\n",
      "loss: 0.325443  [16032/42552]\n",
      "loss: 0.383984  [19232/42552]\n",
      "loss: 0.297674  [22432/42552]\n",
      "loss: 0.284176  [25632/42552]\n",
      "loss: 0.282063  [28832/42552]\n",
      "loss: 0.372657  [32032/42552]\n",
      "loss: 0.395985  [35232/42552]\n",
      "loss: 0.404801  [38432/42552]\n",
      "loss: 0.365568  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.306449 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.401880  [   32/42552]\n",
      "loss: 0.463617  [ 3232/42552]\n",
      "loss: 0.215169  [ 6432/42552]\n",
      "loss: 0.312200  [ 9632/42552]\n",
      "loss: 0.331505  [12832/42552]\n",
      "loss: 0.252100  [16032/42552]\n",
      "loss: 0.298495  [19232/42552]\n",
      "loss: 0.339305  [22432/42552]\n",
      "loss: 0.280009  [25632/42552]\n",
      "loss: 0.384236  [28832/42552]\n",
      "loss: 0.332722  [32032/42552]\n",
      "loss: 0.267137  [35232/42552]\n",
      "loss: 0.313154  [38432/42552]\n",
      "loss: 0.245644  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.297445 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.261503  [   32/42552]\n",
      "loss: 0.208168  [ 3232/42552]\n",
      "loss: 0.375806  [ 6432/42552]\n",
      "loss: 0.322085  [ 9632/42552]\n",
      "loss: 0.212308  [12832/42552]\n",
      "loss: 0.285028  [16032/42552]\n",
      "loss: 0.342597  [19232/42552]\n",
      "loss: 0.330816  [22432/42552]\n",
      "loss: 0.476003  [25632/42552]\n",
      "loss: 0.351548  [28832/42552]\n",
      "loss: 0.335840  [32032/42552]\n",
      "loss: 0.475243  [35232/42552]\n",
      "loss: 0.370973  [38432/42552]\n",
      "loss: 0.238558  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.308501 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.337983  [   32/42552]\n",
      "loss: 0.294499  [ 3232/42552]\n",
      "loss: 0.289519  [ 6432/42552]\n",
      "loss: 0.413508  [ 9632/42552]\n",
      "loss: 0.457252  [12832/42552]\n",
      "loss: 0.328240  [16032/42552]\n",
      "loss: 0.354894  [19232/42552]\n",
      "loss: 0.363610  [22432/42552]\n",
      "loss: 0.395196  [25632/42552]\n",
      "loss: 0.357559  [28832/42552]\n",
      "loss: 0.416698  [32032/42552]\n",
      "loss: 0.451172  [35232/42552]\n",
      "loss: 0.265714  [38432/42552]\n",
      "loss: 0.285013  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.296419 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.179129  [   32/42552]\n",
      "loss: 0.228910  [ 3232/42552]\n",
      "loss: 0.345016  [ 6432/42552]\n",
      "loss: 0.345938  [ 9632/42552]\n",
      "loss: 0.245045  [12832/42552]\n",
      "loss: 0.352107  [16032/42552]\n",
      "loss: 0.291948  [19232/42552]\n",
      "loss: 0.245164  [22432/42552]\n",
      "loss: 0.576445  [25632/42552]\n",
      "loss: 0.262506  [28832/42552]\n",
      "loss: 0.318882  [32032/42552]\n",
      "loss: 0.269673  [35232/42552]\n",
      "loss: 0.296084  [38432/42552]\n",
      "loss: 0.342993  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.299483 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.254331  [   32/42552]\n",
      "loss: 0.355022  [ 3232/42552]\n",
      "loss: 0.282818  [ 6432/42552]\n",
      "loss: 0.280887  [ 9632/42552]\n",
      "loss: 0.161350  [12832/42552]\n",
      "loss: 0.311750  [16032/42552]\n",
      "loss: 0.328804  [19232/42552]\n",
      "loss: 0.387522  [22432/42552]\n",
      "loss: 0.297230  [25632/42552]\n",
      "loss: 0.369690  [28832/42552]\n",
      "loss: 0.404303  [32032/42552]\n",
      "loss: 0.459581  [35232/42552]\n",
      "loss: 0.573839  [38432/42552]\n",
      "loss: 0.252853  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.309822 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.246693  [   32/42552]\n",
      "loss: 0.400589  [ 3232/42552]\n",
      "loss: 0.346694  [ 6432/42552]\n",
      "loss: 0.248914  [ 9632/42552]\n",
      "loss: 0.262282  [12832/42552]\n",
      "loss: 0.400193  [16032/42552]\n",
      "loss: 0.295077  [19232/42552]\n",
      "loss: 0.352033  [22432/42552]\n",
      "loss: 0.439625  [25632/42552]\n",
      "loss: 0.321850  [28832/42552]\n",
      "loss: 0.376661  [32032/42552]\n",
      "loss: 0.388460  [35232/42552]\n",
      "loss: 0.261386  [38432/42552]\n",
      "loss: 0.307018  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.298346 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.361321  [   32/42552]\n",
      "loss: 0.250560  [ 3232/42552]\n",
      "loss: 0.314744  [ 6432/42552]\n",
      "loss: 0.229062  [ 9632/42552]\n",
      "loss: 0.309236  [12832/42552]\n",
      "loss: 0.342239  [16032/42552]\n",
      "loss: 0.183826  [19232/42552]\n",
      "loss: 0.411055  [22432/42552]\n",
      "loss: 0.333644  [25632/42552]\n",
      "loss: 0.269879  [28832/42552]\n",
      "loss: 0.394582  [32032/42552]\n",
      "loss: 0.372413  [35232/42552]\n",
      "loss: 0.223803  [38432/42552]\n",
      "loss: 0.255088  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.298943 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.255214  [   32/42552]\n",
      "loss: 0.208291  [ 3232/42552]\n",
      "loss: 0.292454  [ 6432/42552]\n",
      "loss: 0.340192  [ 9632/42552]\n",
      "loss: 0.371588  [12832/42552]\n",
      "loss: 0.232060  [16032/42552]\n",
      "loss: 0.342563  [19232/42552]\n",
      "loss: 0.359951  [22432/42552]\n",
      "loss: 0.360027  [25632/42552]\n",
      "loss: 0.308463  [28832/42552]\n",
      "loss: 0.486503  [32032/42552]\n",
      "loss: 0.240833  [35232/42552]\n",
      "loss: 0.552184  [38432/42552]\n",
      "loss: 0.313185  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.292261 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.340371  [   32/42552]\n",
      "loss: 0.334736  [ 3232/42552]\n",
      "loss: 0.206854  [ 6432/42552]\n",
      "loss: 0.263748  [ 9632/42552]\n",
      "loss: 0.255593  [12832/42552]\n",
      "loss: 0.383114  [16032/42552]\n",
      "loss: 0.298660  [19232/42552]\n",
      "loss: 0.309105  [22432/42552]\n",
      "loss: 0.262932  [25632/42552]\n",
      "loss: 0.319990  [28832/42552]\n",
      "loss: 0.482663  [32032/42552]\n",
      "loss: 0.393541  [35232/42552]\n",
      "loss: 0.248531  [38432/42552]\n",
      "loss: 0.385194  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.300454 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.372188  [   32/42552]\n",
      "loss: 0.376727  [ 3232/42552]\n",
      "loss: 0.445770  [ 6432/42552]\n",
      "loss: 0.275981  [ 9632/42552]\n",
      "loss: 0.363303  [12832/42552]\n",
      "loss: 0.349404  [16032/42552]\n",
      "loss: 0.258753  [19232/42552]\n",
      "loss: 0.513853  [22432/42552]\n",
      "loss: 0.176449  [25632/42552]\n",
      "loss: 0.289220  [28832/42552]\n",
      "loss: 0.298519  [32032/42552]\n",
      "loss: 0.314432  [35232/42552]\n",
      "loss: 0.240231  [38432/42552]\n",
      "loss: 0.380905  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.305622 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.632679  [   32/42552]\n",
      "loss: 0.469718  [ 3232/42552]\n",
      "loss: 0.236590  [ 6432/42552]\n",
      "loss: 0.465522  [ 9632/42552]\n",
      "loss: 0.328053  [12832/42552]\n",
      "loss: 0.366382  [16032/42552]\n",
      "loss: 0.265335  [19232/42552]\n",
      "loss: 0.326440  [22432/42552]\n",
      "loss: 0.221489  [25632/42552]\n",
      "loss: 0.280036  [28832/42552]\n",
      "loss: 0.244845  [32032/42552]\n",
      "loss: 0.346903  [35232/42552]\n",
      "loss: 0.237747  [38432/42552]\n",
      "loss: 0.411248  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.299274 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.497104  [   32/42552]\n",
      "loss: 0.405922  [ 3232/42552]\n",
      "loss: 0.253155  [ 6432/42552]\n",
      "loss: 0.336626  [ 9632/42552]\n",
      "loss: 0.309054  [12832/42552]\n",
      "loss: 0.272850  [16032/42552]\n",
      "loss: 0.366234  [19232/42552]\n",
      "loss: 0.296362  [22432/42552]\n",
      "loss: 0.490202  [25632/42552]\n",
      "loss: 0.313132  [28832/42552]\n",
      "loss: 0.330132  [32032/42552]\n",
      "loss: 0.451302  [35232/42552]\n",
      "loss: 0.208272  [38432/42552]\n",
      "loss: 0.250940  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.294294 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.211312  [   32/42552]\n",
      "loss: 0.251062  [ 3232/42552]\n",
      "loss: 0.395073  [ 6432/42552]\n",
      "loss: 0.337863  [ 9632/42552]\n",
      "loss: 0.259367  [12832/42552]\n",
      "loss: 0.257067  [16032/42552]\n",
      "loss: 0.300442  [19232/42552]\n",
      "loss: 0.463320  [22432/42552]\n",
      "loss: 0.280298  [25632/42552]\n",
      "loss: 0.245368  [28832/42552]\n",
      "loss: 0.383537  [32032/42552]\n",
      "loss: 0.270000  [35232/42552]\n",
      "loss: 0.305542  [38432/42552]\n",
      "loss: 0.311289  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.299072 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.220787  [   32/42552]\n",
      "loss: 0.222792  [ 3232/42552]\n",
      "loss: 0.331272  [ 6432/42552]\n",
      "loss: 0.364208  [ 9632/42552]\n",
      "loss: 0.415487  [12832/42552]\n",
      "loss: 0.304337  [16032/42552]\n",
      "loss: 0.294520  [19232/42552]\n",
      "loss: 0.481358  [22432/42552]\n",
      "loss: 0.398166  [25632/42552]\n",
      "loss: 0.307909  [28832/42552]\n",
      "loss: 0.346442  [32032/42552]\n",
      "loss: 0.326026  [35232/42552]\n",
      "loss: 0.279344  [38432/42552]\n",
      "loss: 0.516966  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.295226 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.281829  [   32/42552]\n",
      "loss: 0.258571  [ 3232/42552]\n",
      "loss: 0.377593  [ 6432/42552]\n",
      "loss: 0.332571  [ 9632/42552]\n",
      "loss: 0.319367  [12832/42552]\n",
      "loss: 0.315525  [16032/42552]\n",
      "loss: 0.425309  [19232/42552]\n",
      "loss: 0.265752  [22432/42552]\n",
      "loss: 0.308291  [25632/42552]\n",
      "loss: 0.399285  [28832/42552]\n",
      "loss: 0.341485  [32032/42552]\n",
      "loss: 0.399075  [35232/42552]\n",
      "loss: 0.392994  [38432/42552]\n",
      "loss: 0.286980  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.305075 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.336078  [   32/42552]\n",
      "loss: 0.419311  [ 3232/42552]\n",
      "loss: 0.264250  [ 6432/42552]\n",
      "loss: 0.294108  [ 9632/42552]\n",
      "loss: 0.294344  [12832/42552]\n",
      "loss: 0.271580  [16032/42552]\n",
      "loss: 0.248460  [19232/42552]\n",
      "loss: 0.261564  [22432/42552]\n",
      "loss: 0.381505  [25632/42552]\n",
      "loss: 0.310963  [28832/42552]\n",
      "loss: 0.294969  [32032/42552]\n",
      "loss: 0.344084  [35232/42552]\n",
      "loss: 0.234886  [38432/42552]\n",
      "loss: 0.287279  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.289817 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.272846  [   32/42552]\n",
      "loss: 0.391983  [ 3232/42552]\n",
      "loss: 0.590785  [ 6432/42552]\n",
      "loss: 0.317285  [ 9632/42552]\n",
      "loss: 0.347384  [12832/42552]\n",
      "loss: 0.286528  [16032/42552]\n",
      "loss: 0.308103  [19232/42552]\n",
      "loss: 0.411427  [22432/42552]\n",
      "loss: 0.288381  [25632/42552]\n",
      "loss: 0.287457  [28832/42552]\n",
      "loss: 0.341225  [32032/42552]\n",
      "loss: 0.380664  [35232/42552]\n",
      "loss: 0.418731  [38432/42552]\n",
      "loss: 0.453750  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.288154 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.231055  [   32/42552]\n",
      "loss: 0.268271  [ 3232/42552]\n",
      "loss: 0.336973  [ 6432/42552]\n",
      "loss: 0.361781  [ 9632/42552]\n",
      "loss: 0.356720  [12832/42552]\n",
      "loss: 0.460961  [16032/42552]\n",
      "loss: 0.316025  [19232/42552]\n",
      "loss: 0.307472  [22432/42552]\n",
      "loss: 0.262975  [25632/42552]\n",
      "loss: 0.390745  [28832/42552]\n",
      "loss: 0.390529  [32032/42552]\n",
      "loss: 0.332349  [35232/42552]\n",
      "loss: 0.282766  [38432/42552]\n",
      "loss: 0.364378  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.291920 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.318785  [   32/42552]\n",
      "loss: 0.375537  [ 3232/42552]\n",
      "loss: 0.433009  [ 6432/42552]\n",
      "loss: 0.397644  [ 9632/42552]\n",
      "loss: 0.393461  [12832/42552]\n",
      "loss: 0.272606  [16032/42552]\n",
      "loss: 0.306508  [19232/42552]\n",
      "loss: 0.361043  [22432/42552]\n",
      "loss: 0.315937  [25632/42552]\n",
      "loss: 0.319642  [28832/42552]\n",
      "loss: 0.274076  [32032/42552]\n",
      "loss: 0.255551  [35232/42552]\n",
      "loss: 0.321401  [38432/42552]\n",
      "loss: 0.332627  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.291765 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.358426  [   32/42552]\n",
      "loss: 0.408980  [ 3232/42552]\n",
      "loss: 0.335421  [ 6432/42552]\n",
      "loss: 0.360860  [ 9632/42552]\n",
      "loss: 0.390225  [12832/42552]\n",
      "loss: 0.396136  [16032/42552]\n",
      "loss: 0.224954  [19232/42552]\n",
      "loss: 0.254361  [22432/42552]\n",
      "loss: 0.467259  [25632/42552]\n",
      "loss: 0.285664  [28832/42552]\n",
      "loss: 0.326680  [32032/42552]\n",
      "loss: 0.349352  [35232/42552]\n",
      "loss: 0.366491  [38432/42552]\n",
      "loss: 0.368960  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.287899 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.276625  [   32/42552]\n",
      "loss: 0.334652  [ 3232/42552]\n",
      "loss: 0.393452  [ 6432/42552]\n",
      "loss: 0.278265  [ 9632/42552]\n",
      "loss: 0.334580  [12832/42552]\n",
      "loss: 0.530158  [16032/42552]\n",
      "loss: 0.275156  [19232/42552]\n",
      "loss: 0.363632  [22432/42552]\n",
      "loss: 0.180812  [25632/42552]\n",
      "loss: 0.301681  [28832/42552]\n",
      "loss: 0.418927  [32032/42552]\n",
      "loss: 0.373669  [35232/42552]\n",
      "loss: 0.304249  [38432/42552]\n",
      "loss: 0.458372  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.292567 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.168732  [   32/42552]\n",
      "loss: 0.272220  [ 3232/42552]\n",
      "loss: 0.232310  [ 6432/42552]\n",
      "loss: 0.288357  [ 9632/42552]\n",
      "loss: 0.494406  [12832/42552]\n",
      "loss: 0.456551  [16032/42552]\n",
      "loss: 0.283948  [19232/42552]\n",
      "loss: 0.303391  [22432/42552]\n",
      "loss: 0.414008  [25632/42552]\n",
      "loss: 0.425584  [28832/42552]\n",
      "loss: 0.350942  [32032/42552]\n",
      "loss: 0.443413  [35232/42552]\n",
      "loss: 0.393215  [38432/42552]\n",
      "loss: 0.311334  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.288392 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.311521  [   32/42552]\n",
      "loss: 0.302956  [ 3232/42552]\n",
      "loss: 0.292478  [ 6432/42552]\n",
      "loss: 0.279636  [ 9632/42552]\n",
      "loss: 0.411255  [12832/42552]\n",
      "loss: 0.392742  [16032/42552]\n",
      "loss: 0.423161  [19232/42552]\n",
      "loss: 0.354343  [22432/42552]\n",
      "loss: 0.368323  [25632/42552]\n",
      "loss: 0.269473  [28832/42552]\n",
      "loss: 0.236194  [32032/42552]\n",
      "loss: 0.400184  [35232/42552]\n",
      "loss: 0.356467  [38432/42552]\n",
      "loss: 0.348709  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.286188 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.306757  [   32/42552]\n",
      "loss: 0.385897  [ 3232/42552]\n",
      "loss: 0.302354  [ 6432/42552]\n",
      "loss: 0.284145  [ 9632/42552]\n",
      "loss: 0.370240  [12832/42552]\n",
      "loss: 0.262930  [16032/42552]\n",
      "loss: 0.296025  [19232/42552]\n",
      "loss: 0.259325  [22432/42552]\n",
      "loss: 0.344496  [25632/42552]\n",
      "loss: 0.313653  [28832/42552]\n",
      "loss: 0.282284  [32032/42552]\n",
      "loss: 0.334032  [35232/42552]\n",
      "loss: 0.352027  [38432/42552]\n",
      "loss: 0.350986  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.288479 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.257460  [   32/42552]\n",
      "loss: 0.254416  [ 3232/42552]\n",
      "loss: 0.296435  [ 6432/42552]\n",
      "loss: 0.446780  [ 9632/42552]\n",
      "loss: 0.425316  [12832/42552]\n",
      "loss: 0.307336  [16032/42552]\n",
      "loss: 0.399167  [19232/42552]\n",
      "loss: 0.366231  [22432/42552]\n",
      "loss: 0.373663  [25632/42552]\n",
      "loss: 0.232152  [28832/42552]\n",
      "loss: 0.241761  [32032/42552]\n",
      "loss: 0.263056  [35232/42552]\n",
      "loss: 0.438479  [38432/42552]\n",
      "loss: 0.327725  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.291711 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.230645  [   32/42552]\n",
      "loss: 0.339002  [ 3232/42552]\n",
      "loss: 0.304439  [ 6432/42552]\n",
      "loss: 0.387030  [ 9632/42552]\n",
      "loss: 0.229993  [12832/42552]\n",
      "loss: 0.286311  [16032/42552]\n",
      "loss: 0.313763  [19232/42552]\n",
      "loss: 0.318956  [22432/42552]\n",
      "loss: 0.379502  [25632/42552]\n",
      "loss: 0.304621  [28832/42552]\n",
      "loss: 0.323672  [32032/42552]\n",
      "loss: 0.229782  [35232/42552]\n",
      "loss: 0.346903  [38432/42552]\n",
      "loss: 0.402518  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.289683 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.271559  [   32/42552]\n",
      "loss: 0.403777  [ 3232/42552]\n",
      "loss: 0.373427  [ 6432/42552]\n",
      "loss: 0.287631  [ 9632/42552]\n",
      "loss: 0.245457  [12832/42552]\n",
      "loss: 0.166378  [16032/42552]\n",
      "loss: 0.389560  [19232/42552]\n",
      "loss: 0.325719  [22432/42552]\n",
      "loss: 0.255566  [25632/42552]\n",
      "loss: 0.214310  [28832/42552]\n",
      "loss: 0.406918  [32032/42552]\n",
      "loss: 0.329228  [35232/42552]\n",
      "loss: 0.262206  [38432/42552]\n",
      "loss: 0.430605  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.290731 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.338841  [   32/42552]\n",
      "loss: 0.316292  [ 3232/42552]\n",
      "loss: 0.403804  [ 6432/42552]\n",
      "loss: 0.262276  [ 9632/42552]\n",
      "loss: 0.367060  [12832/42552]\n",
      "loss: 0.417379  [16032/42552]\n",
      "loss: 0.377102  [19232/42552]\n",
      "loss: 0.356943  [22432/42552]\n",
      "loss: 0.307378  [25632/42552]\n",
      "loss: 0.316159  [28832/42552]\n",
      "loss: 0.361399  [32032/42552]\n",
      "loss: 0.317867  [35232/42552]\n",
      "loss: 0.318731  [38432/42552]\n",
      "loss: 0.347340  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.285520 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.312323  [   32/42552]\n",
      "loss: 0.278610  [ 3232/42552]\n",
      "loss: 0.158895  [ 6432/42552]\n",
      "loss: 0.313220  [ 9632/42552]\n",
      "loss: 0.477574  [12832/42552]\n",
      "loss: 0.313472  [16032/42552]\n",
      "loss: 0.220452  [19232/42552]\n",
      "loss: 0.371589  [22432/42552]\n",
      "loss: 0.368586  [25632/42552]\n",
      "loss: 0.599806  [28832/42552]\n",
      "loss: 0.220570  [32032/42552]\n",
      "loss: 0.444935  [35232/42552]\n",
      "loss: 0.350185  [38432/42552]\n",
      "loss: 0.301348  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.291876 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.334574  [   32/42552]\n",
      "loss: 0.407414  [ 3232/42552]\n",
      "loss: 0.160968  [ 6432/42552]\n",
      "loss: 0.275838  [ 9632/42552]\n",
      "loss: 0.236802  [12832/42552]\n",
      "loss: 0.347004  [16032/42552]\n",
      "loss: 0.275189  [19232/42552]\n",
      "loss: 0.308569  [22432/42552]\n",
      "loss: 0.323121  [25632/42552]\n",
      "loss: 0.330149  [28832/42552]\n",
      "loss: 0.370378  [32032/42552]\n",
      "loss: 0.286165  [35232/42552]\n",
      "loss: 0.223836  [38432/42552]\n",
      "loss: 0.433612  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.287764 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.454270  [   32/42552]\n",
      "loss: 0.313380  [ 3232/42552]\n",
      "loss: 0.227506  [ 6432/42552]\n",
      "loss: 0.320545  [ 9632/42552]\n",
      "loss: 0.269554  [12832/42552]\n",
      "loss: 0.404180  [16032/42552]\n",
      "loss: 0.443342  [19232/42552]\n",
      "loss: 0.369075  [22432/42552]\n",
      "loss: 0.467217  [25632/42552]\n",
      "loss: 0.364110  [28832/42552]\n",
      "loss: 0.300841  [32032/42552]\n",
      "loss: 0.315810  [35232/42552]\n",
      "loss: 0.207364  [38432/42552]\n",
      "loss: 0.312912  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.298926 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.211516  [   32/42552]\n",
      "loss: 0.289847  [ 3232/42552]\n",
      "loss: 0.299250  [ 6432/42552]\n",
      "loss: 0.275378  [ 9632/42552]\n",
      "loss: 0.283233  [12832/42552]\n",
      "loss: 0.204916  [16032/42552]\n",
      "loss: 0.372802  [19232/42552]\n",
      "loss: 0.314369  [22432/42552]\n",
      "loss: 0.275099  [25632/42552]\n",
      "loss: 0.273688  [28832/42552]\n",
      "loss: 0.409216  [32032/42552]\n",
      "loss: 0.392885  [35232/42552]\n",
      "loss: 0.256463  [38432/42552]\n",
      "loss: 0.520697  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.283346 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.291038  [   32/42552]\n",
      "loss: 0.233138  [ 3232/42552]\n",
      "loss: 0.290991  [ 6432/42552]\n",
      "loss: 0.222225  [ 9632/42552]\n",
      "loss: 0.330606  [12832/42552]\n",
      "loss: 0.197113  [16032/42552]\n",
      "loss: 0.228148  [19232/42552]\n",
      "loss: 0.333486  [22432/42552]\n",
      "loss: 0.199379  [25632/42552]\n",
      "loss: 0.366025  [28832/42552]\n",
      "loss: 0.249281  [32032/42552]\n",
      "loss: 0.326859  [35232/42552]\n",
      "loss: 0.263420  [38432/42552]\n",
      "loss: 0.440031  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.283524 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.281609  [   32/42552]\n",
      "loss: 0.336181  [ 3232/42552]\n",
      "loss: 0.362316  [ 6432/42552]\n",
      "loss: 0.496203  [ 9632/42552]\n",
      "loss: 0.292108  [12832/42552]\n",
      "loss: 0.327867  [16032/42552]\n",
      "loss: 0.327382  [19232/42552]\n",
      "loss: 0.350713  [22432/42552]\n",
      "loss: 0.340146  [25632/42552]\n",
      "loss: 0.328891  [28832/42552]\n",
      "loss: 0.306569  [32032/42552]\n",
      "loss: 0.278459  [35232/42552]\n",
      "loss: 0.375616  [38432/42552]\n",
      "loss: 0.245290  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.284041 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.284786  [   32/42552]\n",
      "loss: 0.343189  [ 3232/42552]\n",
      "loss: 0.189188  [ 6432/42552]\n",
      "loss: 0.190991  [ 9632/42552]\n",
      "loss: 0.430069  [12832/42552]\n",
      "loss: 0.289574  [16032/42552]\n",
      "loss: 0.302368  [19232/42552]\n",
      "loss: 0.300976  [22432/42552]\n",
      "loss: 0.307519  [25632/42552]\n",
      "loss: 0.333260  [28832/42552]\n",
      "loss: 0.296201  [32032/42552]\n",
      "loss: 0.276502  [35232/42552]\n",
      "loss: 0.381228  [38432/42552]\n",
      "loss: 0.245843  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.283845 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.264520  [   32/42552]\n",
      "loss: 0.212251  [ 3232/42552]\n",
      "loss: 0.357879  [ 6432/42552]\n",
      "loss: 0.284495  [ 9632/42552]\n",
      "loss: 0.211149  [12832/42552]\n",
      "loss: 0.260923  [16032/42552]\n",
      "loss: 0.311755  [19232/42552]\n",
      "loss: 0.264160  [22432/42552]\n",
      "loss: 0.558562  [25632/42552]\n",
      "loss: 0.383738  [28832/42552]\n",
      "loss: 0.266943  [32032/42552]\n",
      "loss: 0.349670  [35232/42552]\n",
      "loss: 0.227880  [38432/42552]\n",
      "loss: 0.225739  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.287628 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.405780  [   32/42552]\n",
      "loss: 0.291474  [ 3232/42552]\n",
      "loss: 0.310556  [ 6432/42552]\n",
      "loss: 0.353558  [ 9632/42552]\n",
      "loss: 0.274271  [12832/42552]\n",
      "loss: 0.420887  [16032/42552]\n",
      "loss: 0.243860  [19232/42552]\n",
      "loss: 0.334314  [22432/42552]\n",
      "loss: 0.395775  [25632/42552]\n",
      "loss: 0.248072  [28832/42552]\n",
      "loss: 0.297530  [32032/42552]\n",
      "loss: 0.271291  [35232/42552]\n",
      "loss: 0.259882  [38432/42552]\n",
      "loss: 0.401005  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.283190 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.353051  [   32/42552]\n",
      "loss: 0.250776  [ 3232/42552]\n",
      "loss: 0.465550  [ 6432/42552]\n",
      "loss: 0.344713  [ 9632/42552]\n",
      "loss: 0.396619  [12832/42552]\n",
      "loss: 0.222598  [16032/42552]\n",
      "loss: 0.335670  [19232/42552]\n",
      "loss: 0.428803  [22432/42552]\n",
      "loss: 0.346911  [25632/42552]\n",
      "loss: 0.330493  [28832/42552]\n",
      "loss: 0.297844  [32032/42552]\n",
      "loss: 0.243154  [35232/42552]\n",
      "loss: 0.282221  [38432/42552]\n",
      "loss: 0.358129  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.278873 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.344301  [   32/42552]\n",
      "loss: 0.400154  [ 3232/42552]\n",
      "loss: 0.309739  [ 6432/42552]\n",
      "loss: 0.353907  [ 9632/42552]\n",
      "loss: 0.248623  [12832/42552]\n",
      "loss: 0.254941  [16032/42552]\n",
      "loss: 0.220632  [19232/42552]\n",
      "loss: 0.321079  [22432/42552]\n",
      "loss: 0.368054  [25632/42552]\n",
      "loss: 0.277054  [28832/42552]\n",
      "loss: 0.239849  [32032/42552]\n",
      "loss: 0.225798  [35232/42552]\n",
      "loss: 0.279872  [38432/42552]\n",
      "loss: 0.280034  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.283740 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.597090  [   32/42552]\n",
      "loss: 0.364060  [ 3232/42552]\n",
      "loss: 0.390758  [ 6432/42552]\n",
      "loss: 0.162691  [ 9632/42552]\n",
      "loss: 0.297162  [12832/42552]\n",
      "loss: 0.270049  [16032/42552]\n",
      "loss: 0.396528  [19232/42552]\n",
      "loss: 0.492878  [22432/42552]\n",
      "loss: 0.289354  [25632/42552]\n",
      "loss: 0.334229  [28832/42552]\n",
      "loss: 0.357600  [32032/42552]\n",
      "loss: 0.368771  [35232/42552]\n",
      "loss: 0.249282  [38432/42552]\n",
      "loss: 0.249053  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.279719 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.247589  [   32/42552]\n",
      "loss: 0.311714  [ 3232/42552]\n",
      "loss: 0.281248  [ 6432/42552]\n",
      "loss: 0.240047  [ 9632/42552]\n",
      "loss: 0.289349  [12832/42552]\n",
      "loss: 0.253509  [16032/42552]\n",
      "loss: 0.223037  [19232/42552]\n",
      "loss: 0.233435  [22432/42552]\n",
      "loss: 0.296238  [25632/42552]\n",
      "loss: 0.359508  [28832/42552]\n",
      "loss: 0.318034  [32032/42552]\n",
      "loss: 0.317235  [35232/42552]\n",
      "loss: 0.244152  [38432/42552]\n",
      "loss: 0.382988  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.273298 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.307295  [   32/42552]\n",
      "loss: 0.471585  [ 3232/42552]\n",
      "loss: 0.259545  [ 6432/42552]\n",
      "loss: 0.321195  [ 9632/42552]\n",
      "loss: 0.226283  [12832/42552]\n",
      "loss: 0.410358  [16032/42552]\n",
      "loss: 0.388187  [19232/42552]\n",
      "loss: 0.400316  [22432/42552]\n",
      "loss: 0.162555  [25632/42552]\n",
      "loss: 0.344201  [28832/42552]\n",
      "loss: 0.378133  [32032/42552]\n",
      "loss: 0.257002  [35232/42552]\n",
      "loss: 0.231522  [38432/42552]\n",
      "loss: 0.202557  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.287355 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.356235  [   32/42552]\n",
      "loss: 0.532875  [ 3232/42552]\n",
      "loss: 0.288459  [ 6432/42552]\n",
      "loss: 0.387358  [ 9632/42552]\n",
      "loss: 0.347442  [12832/42552]\n",
      "loss: 0.267691  [16032/42552]\n",
      "loss: 0.227729  [19232/42552]\n",
      "loss: 0.413256  [22432/42552]\n",
      "loss: 0.301277  [25632/42552]\n",
      "loss: 0.320584  [28832/42552]\n",
      "loss: 0.356905  [32032/42552]\n",
      "loss: 0.418979  [35232/42552]\n",
      "loss: 0.271458  [38432/42552]\n",
      "loss: 0.259574  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.286641 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.296494  [   32/42552]\n",
      "loss: 0.489845  [ 3232/42552]\n",
      "loss: 0.257383  [ 6432/42552]\n",
      "loss: 0.258996  [ 9632/42552]\n",
      "loss: 0.280243  [12832/42552]\n",
      "loss: 0.323884  [16032/42552]\n",
      "loss: 0.295284  [19232/42552]\n",
      "loss: 0.436823  [22432/42552]\n",
      "loss: 0.240242  [25632/42552]\n",
      "loss: 0.293565  [28832/42552]\n",
      "loss: 0.294536  [32032/42552]\n",
      "loss: 0.220702  [35232/42552]\n",
      "loss: 0.291371  [38432/42552]\n",
      "loss: 0.316351  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.280861 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.311047  [   32/42552]\n",
      "loss: 0.252743  [ 3232/42552]\n",
      "loss: 0.325172  [ 6432/42552]\n",
      "loss: 0.366878  [ 9632/42552]\n",
      "loss: 0.460036  [12832/42552]\n",
      "loss: 0.170078  [16032/42552]\n",
      "loss: 0.275475  [19232/42552]\n",
      "loss: 0.299874  [22432/42552]\n",
      "loss: 0.374872  [25632/42552]\n",
      "loss: 0.425271  [28832/42552]\n",
      "loss: 0.302386  [32032/42552]\n",
      "loss: 0.256081  [35232/42552]\n",
      "loss: 0.360026  [38432/42552]\n",
      "loss: 0.299658  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.275498 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.306027  [   32/42552]\n",
      "loss: 0.196629  [ 3232/42552]\n",
      "loss: 0.365971  [ 6432/42552]\n",
      "loss: 0.290513  [ 9632/42552]\n",
      "loss: 0.353137  [12832/42552]\n",
      "loss: 0.378141  [16032/42552]\n",
      "loss: 0.376367  [19232/42552]\n",
      "loss: 0.264687  [22432/42552]\n",
      "loss: 0.328732  [25632/42552]\n",
      "loss: 0.506585  [28832/42552]\n",
      "loss: 0.482245  [32032/42552]\n",
      "loss: 0.394312  [35232/42552]\n",
      "loss: 0.379678  [38432/42552]\n",
      "loss: 0.280915  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.280104 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.360654  [   32/42552]\n",
      "loss: 0.290009  [ 3232/42552]\n",
      "loss: 0.395355  [ 6432/42552]\n",
      "loss: 0.184271  [ 9632/42552]\n",
      "loss: 0.412718  [12832/42552]\n",
      "loss: 0.330110  [16032/42552]\n",
      "loss: 0.235420  [19232/42552]\n",
      "loss: 0.318851  [22432/42552]\n",
      "loss: 0.280189  [25632/42552]\n",
      "loss: 0.369577  [28832/42552]\n",
      "loss: 0.277364  [32032/42552]\n",
      "loss: 0.438478  [35232/42552]\n",
      "loss: 0.336278  [38432/42552]\n",
      "loss: 0.296993  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.282926 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.180860  [   32/42552]\n",
      "loss: 0.265679  [ 3232/42552]\n",
      "loss: 0.328282  [ 6432/42552]\n",
      "loss: 0.285927  [ 9632/42552]\n",
      "loss: 0.309668  [12832/42552]\n",
      "loss: 0.236601  [16032/42552]\n",
      "loss: 0.170397  [19232/42552]\n",
      "loss: 0.307086  [22432/42552]\n",
      "loss: 0.368627  [25632/42552]\n",
      "loss: 0.272562  [28832/42552]\n",
      "loss: 0.374393  [32032/42552]\n",
      "loss: 0.217271  [35232/42552]\n",
      "loss: 0.299705  [38432/42552]\n",
      "loss: 0.386804  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.282463 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.262031  [   32/42552]\n",
      "loss: 0.292971  [ 3232/42552]\n",
      "loss: 0.318308  [ 6432/42552]\n",
      "loss: 0.414711  [ 9632/42552]\n",
      "loss: 0.406671  [12832/42552]\n",
      "loss: 0.188462  [16032/42552]\n",
      "loss: 0.185118  [19232/42552]\n",
      "loss: 0.186378  [22432/42552]\n",
      "loss: 0.262949  [25632/42552]\n",
      "loss: 0.203742  [28832/42552]\n",
      "loss: 0.232300  [32032/42552]\n",
      "loss: 0.296943  [35232/42552]\n",
      "loss: 0.547461  [38432/42552]\n",
      "loss: 0.268221  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.289983 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.401623  [   32/42552]\n",
      "loss: 0.445963  [ 3232/42552]\n",
      "loss: 0.377285  [ 6432/42552]\n",
      "loss: 0.376529  [ 9632/42552]\n",
      "loss: 0.327434  [12832/42552]\n",
      "loss: 0.334205  [16032/42552]\n",
      "loss: 0.386786  [19232/42552]\n",
      "loss: 0.261911  [22432/42552]\n",
      "loss: 0.309353  [25632/42552]\n",
      "loss: 0.298320  [28832/42552]\n",
      "loss: 0.490318  [32032/42552]\n",
      "loss: 0.372797  [35232/42552]\n",
      "loss: 0.226780  [38432/42552]\n",
      "loss: 0.241632  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.274513 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.276891  [   32/42552]\n",
      "loss: 0.427435  [ 3232/42552]\n",
      "loss: 0.335349  [ 6432/42552]\n",
      "loss: 0.385759  [ 9632/42552]\n",
      "loss: 0.231763  [12832/42552]\n",
      "loss: 0.357544  [16032/42552]\n",
      "loss: 0.258882  [19232/42552]\n",
      "loss: 0.227844  [22432/42552]\n",
      "loss: 0.229291  [25632/42552]\n",
      "loss: 0.414714  [28832/42552]\n",
      "loss: 0.337409  [32032/42552]\n",
      "loss: 0.289278  [35232/42552]\n",
      "loss: 0.224362  [38432/42552]\n",
      "loss: 0.318944  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.276419 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.217849  [   32/42552]\n",
      "loss: 0.502813  [ 3232/42552]\n",
      "loss: 0.193402  [ 6432/42552]\n",
      "loss: 0.213385  [ 9632/42552]\n",
      "loss: 0.268465  [12832/42552]\n",
      "loss: 0.379263  [16032/42552]\n",
      "loss: 0.295111  [19232/42552]\n",
      "loss: 0.257007  [22432/42552]\n",
      "loss: 0.203042  [25632/42552]\n",
      "loss: 0.199817  [28832/42552]\n",
      "loss: 0.258651  [32032/42552]\n",
      "loss: 0.245457  [35232/42552]\n",
      "loss: 0.255832  [38432/42552]\n",
      "loss: 0.291913  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.281121 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.367799  [   32/42552]\n",
      "loss: 0.217179  [ 3232/42552]\n",
      "loss: 0.290308  [ 6432/42552]\n",
      "loss: 0.290329  [ 9632/42552]\n",
      "loss: 0.307380  [12832/42552]\n",
      "loss: 0.337595  [16032/42552]\n",
      "loss: 0.472392  [19232/42552]\n",
      "loss: 0.391922  [22432/42552]\n",
      "loss: 0.276757  [25632/42552]\n",
      "loss: 0.258476  [28832/42552]\n",
      "loss: 0.357192  [32032/42552]\n",
      "loss: 0.367050  [35232/42552]\n",
      "loss: 0.342129  [38432/42552]\n",
      "loss: 0.337683  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.272456 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.485183  [   32/42552]\n",
      "loss: 0.356205  [ 3232/42552]\n",
      "loss: 0.271778  [ 6432/42552]\n",
      "loss: 0.169937  [ 9632/42552]\n",
      "loss: 0.450748  [12832/42552]\n",
      "loss: 0.311290  [16032/42552]\n",
      "loss: 0.381836  [19232/42552]\n",
      "loss: 0.262729  [22432/42552]\n",
      "loss: 0.451098  [25632/42552]\n",
      "loss: 0.265362  [28832/42552]\n",
      "loss: 0.480758  [32032/42552]\n",
      "loss: 0.441566  [35232/42552]\n",
      "loss: 0.380228  [38432/42552]\n",
      "loss: 0.191726  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.276215 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.346500  [   32/42552]\n",
      "loss: 0.272124  [ 3232/42552]\n",
      "loss: 0.397656  [ 6432/42552]\n",
      "loss: 0.246423  [ 9632/42552]\n",
      "loss: 0.194749  [12832/42552]\n",
      "loss: 0.268911  [16032/42552]\n",
      "loss: 0.473307  [19232/42552]\n",
      "loss: 0.284935  [22432/42552]\n",
      "loss: 0.344689  [25632/42552]\n",
      "loss: 0.378944  [28832/42552]\n",
      "loss: 0.488589  [32032/42552]\n",
      "loss: 0.244099  [35232/42552]\n",
      "loss: 0.416839  [38432/42552]\n",
      "loss: 0.318216  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.275524 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.252605  [   32/42552]\n",
      "loss: 0.491931  [ 3232/42552]\n",
      "loss: 0.296739  [ 6432/42552]\n",
      "loss: 0.352680  [ 9632/42552]\n",
      "loss: 0.304225  [12832/42552]\n",
      "loss: 0.414068  [16032/42552]\n",
      "loss: 0.275414  [19232/42552]\n",
      "loss: 0.311943  [22432/42552]\n",
      "loss: 0.314654  [25632/42552]\n",
      "loss: 0.238715  [28832/42552]\n",
      "loss: 0.315871  [32032/42552]\n",
      "loss: 0.221805  [35232/42552]\n",
      "loss: 0.314961  [38432/42552]\n",
      "loss: 0.329144  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.280452 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.256272  [   32/42552]\n",
      "loss: 0.271655  [ 3232/42552]\n",
      "loss: 0.386661  [ 6432/42552]\n",
      "loss: 0.313473  [ 9632/42552]\n",
      "loss: 0.233217  [12832/42552]\n",
      "loss: 0.206410  [16032/42552]\n",
      "loss: 0.298516  [19232/42552]\n",
      "loss: 0.304128  [22432/42552]\n",
      "loss: 0.290748  [25632/42552]\n",
      "loss: 0.211511  [28832/42552]\n",
      "loss: 0.328971  [32032/42552]\n",
      "loss: 0.409677  [35232/42552]\n",
      "loss: 0.310495  [38432/42552]\n",
      "loss: 0.257978  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.268961 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.355341  [   32/42552]\n",
      "loss: 0.296288  [ 3232/42552]\n",
      "loss: 0.312594  [ 6432/42552]\n",
      "loss: 0.325614  [ 9632/42552]\n",
      "loss: 0.313003  [12832/42552]\n",
      "loss: 0.258612  [16032/42552]\n",
      "loss: 0.428622  [19232/42552]\n",
      "loss: 0.275104  [22432/42552]\n",
      "loss: 0.278372  [25632/42552]\n",
      "loss: 0.358091  [28832/42552]\n",
      "loss: 0.220958  [32032/42552]\n",
      "loss: 0.250307  [35232/42552]\n",
      "loss: 0.281914  [38432/42552]\n",
      "loss: 0.281597  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.274893 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.305106  [   32/42552]\n",
      "loss: 0.276179  [ 3232/42552]\n",
      "loss: 0.331866  [ 6432/42552]\n",
      "loss: 0.259452  [ 9632/42552]\n",
      "loss: 0.286550  [12832/42552]\n",
      "loss: 0.443658  [16032/42552]\n",
      "loss: 0.226450  [19232/42552]\n",
      "loss: 0.340810  [22432/42552]\n",
      "loss: 0.254406  [25632/42552]\n",
      "loss: 0.162370  [28832/42552]\n",
      "loss: 0.277267  [32032/42552]\n",
      "loss: 0.336877  [35232/42552]\n",
      "loss: 0.342283  [38432/42552]\n",
      "loss: 0.253255  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.275565 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.273161  [   32/42552]\n",
      "loss: 0.350980  [ 3232/42552]\n",
      "loss: 0.294898  [ 6432/42552]\n",
      "loss: 0.212989  [ 9632/42552]\n",
      "loss: 0.344818  [12832/42552]\n",
      "loss: 0.341637  [16032/42552]\n",
      "loss: 0.403377  [19232/42552]\n",
      "loss: 0.223747  [22432/42552]\n",
      "loss: 0.403717  [25632/42552]\n",
      "loss: 0.221333  [28832/42552]\n",
      "loss: 0.332961  [32032/42552]\n",
      "loss: 0.372854  [35232/42552]\n",
      "loss: 0.354775  [38432/42552]\n",
      "loss: 0.393674  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.279442 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.306303  [   32/42552]\n",
      "loss: 0.388464  [ 3232/42552]\n",
      "loss: 0.326060  [ 6432/42552]\n",
      "loss: 0.176964  [ 9632/42552]\n",
      "loss: 0.242466  [12832/42552]\n",
      "loss: 0.269629  [16032/42552]\n",
      "loss: 0.316552  [19232/42552]\n",
      "loss: 0.311672  [22432/42552]\n",
      "loss: 0.325473  [25632/42552]\n",
      "loss: 0.337280  [28832/42552]\n",
      "loss: 0.357325  [32032/42552]\n",
      "loss: 0.253719  [35232/42552]\n",
      "loss: 0.308490  [38432/42552]\n",
      "loss: 0.303018  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.278149 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.370141  [   32/42552]\n",
      "loss: 0.263855  [ 3232/42552]\n",
      "loss: 0.249728  [ 6432/42552]\n",
      "loss: 0.346218  [ 9632/42552]\n",
      "loss: 0.337992  [12832/42552]\n",
      "loss: 0.375370  [16032/42552]\n",
      "loss: 0.384961  [19232/42552]\n",
      "loss: 0.251313  [22432/42552]\n",
      "loss: 0.428430  [25632/42552]\n",
      "loss: 0.371848  [28832/42552]\n",
      "loss: 0.355355  [32032/42552]\n",
      "loss: 0.243663  [35232/42552]\n",
      "loss: 0.352360  [38432/42552]\n",
      "loss: 0.332278  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.274940 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.398572  [   32/42552]\n",
      "loss: 0.212723  [ 3232/42552]\n",
      "loss: 0.368676  [ 6432/42552]\n",
      "loss: 0.233810  [ 9632/42552]\n",
      "loss: 0.324115  [12832/42552]\n",
      "loss: 0.237681  [16032/42552]\n",
      "loss: 0.268392  [19232/42552]\n",
      "loss: 0.223151  [22432/42552]\n",
      "loss: 0.372735  [25632/42552]\n",
      "loss: 0.288521  [28832/42552]\n",
      "loss: 0.293124  [32032/42552]\n",
      "loss: 0.318328  [35232/42552]\n",
      "loss: 0.291524  [38432/42552]\n",
      "loss: 0.366393  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.270900 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.201692  [   32/42552]\n",
      "loss: 0.394544  [ 3232/42552]\n",
      "loss: 0.246638  [ 6432/42552]\n",
      "loss: 0.273470  [ 9632/42552]\n",
      "loss: 0.397829  [12832/42552]\n",
      "loss: 0.303736  [16032/42552]\n",
      "loss: 0.345628  [19232/42552]\n",
      "loss: 0.289816  [22432/42552]\n",
      "loss: 0.325967  [25632/42552]\n",
      "loss: 0.360334  [28832/42552]\n",
      "loss: 0.500792  [32032/42552]\n",
      "loss: 0.351026  [35232/42552]\n",
      "loss: 0.242201  [38432/42552]\n",
      "loss: 0.247880  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.278118 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.222459  [   32/42552]\n",
      "loss: 0.350274  [ 3232/42552]\n",
      "loss: 0.398244  [ 6432/42552]\n",
      "loss: 0.333873  [ 9632/42552]\n",
      "loss: 0.354143  [12832/42552]\n",
      "loss: 0.337235  [16032/42552]\n",
      "loss: 0.227752  [19232/42552]\n",
      "loss: 0.240921  [22432/42552]\n",
      "loss: 0.168202  [25632/42552]\n",
      "loss: 0.290174  [28832/42552]\n",
      "loss: 0.394987  [32032/42552]\n",
      "loss: 0.311226  [35232/42552]\n",
      "loss: 0.238967  [38432/42552]\n",
      "loss: 0.368828  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.272214 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.299872  [   32/42552]\n",
      "loss: 0.197954  [ 3232/42552]\n",
      "loss: 0.287022  [ 6432/42552]\n",
      "loss: 0.275437  [ 9632/42552]\n",
      "loss: 0.505551  [12832/42552]\n",
      "loss: 0.327062  [16032/42552]\n",
      "loss: 0.346642  [19232/42552]\n",
      "loss: 0.425006  [22432/42552]\n",
      "loss: 0.360554  [25632/42552]\n",
      "loss: 0.349719  [28832/42552]\n",
      "loss: 0.418927  [32032/42552]\n",
      "loss: 0.325813  [35232/42552]\n",
      "loss: 0.201358  [38432/42552]\n",
      "loss: 0.290866  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.270637 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.314735  [   32/42552]\n",
      "loss: 0.272316  [ 3232/42552]\n",
      "loss: 0.294368  [ 6432/42552]\n",
      "loss: 0.297611  [ 9632/42552]\n",
      "loss: 0.194550  [12832/42552]\n",
      "loss: 0.397385  [16032/42552]\n",
      "loss: 0.310052  [19232/42552]\n",
      "loss: 0.433507  [22432/42552]\n",
      "loss: 0.257904  [25632/42552]\n",
      "loss: 0.438308  [28832/42552]\n",
      "loss: 0.362737  [32032/42552]\n",
      "loss: 0.364763  [35232/42552]\n",
      "loss: 0.329688  [38432/42552]\n",
      "loss: 0.375998  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.265287 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.266641  [   32/42552]\n",
      "loss: 0.327312  [ 3232/42552]\n",
      "loss: 0.292207  [ 6432/42552]\n",
      "loss: 0.341140  [ 9632/42552]\n",
      "loss: 0.326533  [12832/42552]\n",
      "loss: 0.256984  [16032/42552]\n",
      "loss: 0.270681  [19232/42552]\n",
      "loss: 0.382482  [22432/42552]\n",
      "loss: 0.285034  [25632/42552]\n",
      "loss: 0.237039  [28832/42552]\n",
      "loss: 0.259538  [32032/42552]\n",
      "loss: 0.213661  [35232/42552]\n",
      "loss: 0.316445  [38432/42552]\n",
      "loss: 0.305895  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.271111 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.435879  [   32/42552]\n",
      "loss: 0.371858  [ 3232/42552]\n",
      "loss: 0.255612  [ 6432/42552]\n",
      "loss: 0.247563  [ 9632/42552]\n",
      "loss: 0.243540  [12832/42552]\n",
      "loss: 0.315702  [16032/42552]\n",
      "loss: 0.303739  [19232/42552]\n",
      "loss: 0.388406  [22432/42552]\n",
      "loss: 0.249732  [25632/42552]\n",
      "loss: 0.391188  [28832/42552]\n",
      "loss: 0.283951  [32032/42552]\n",
      "loss: 0.317525  [35232/42552]\n",
      "loss: 0.177299  [38432/42552]\n",
      "loss: 0.452193  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.271074 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.253419  [   32/42552]\n",
      "loss: 0.435441  [ 3232/42552]\n",
      "loss: 0.221835  [ 6432/42552]\n",
      "loss: 0.334759  [ 9632/42552]\n",
      "loss: 0.333279  [12832/42552]\n",
      "loss: 0.287278  [16032/42552]\n",
      "loss: 0.356125  [19232/42552]\n",
      "loss: 0.392230  [22432/42552]\n",
      "loss: 0.249285  [25632/42552]\n",
      "loss: 0.241835  [28832/42552]\n",
      "loss: 0.220163  [32032/42552]\n",
      "loss: 0.367927  [35232/42552]\n",
      "loss: 0.192175  [38432/42552]\n",
      "loss: 0.244140  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.269226 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.335017  [   32/42552]\n",
      "loss: 0.335322  [ 3232/42552]\n",
      "loss: 0.252984  [ 6432/42552]\n",
      "loss: 0.338252  [ 9632/42552]\n",
      "loss: 0.398258  [12832/42552]\n",
      "loss: 0.207860  [16032/42552]\n",
      "loss: 0.251818  [19232/42552]\n",
      "loss: 0.192871  [22432/42552]\n",
      "loss: 0.409522  [25632/42552]\n",
      "loss: 0.306721  [28832/42552]\n",
      "loss: 0.358096  [32032/42552]\n",
      "loss: 0.282102  [35232/42552]\n",
      "loss: 0.274105  [38432/42552]\n",
      "loss: 0.407487  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.264133 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.369841  [   32/42552]\n",
      "loss: 0.238979  [ 3232/42552]\n",
      "loss: 0.368576  [ 6432/42552]\n",
      "loss: 0.330672  [ 9632/42552]\n",
      "loss: 0.240451  [12832/42552]\n",
      "loss: 0.393025  [16032/42552]\n",
      "loss: 0.372025  [19232/42552]\n",
      "loss: 0.340907  [22432/42552]\n",
      "loss: 0.337253  [25632/42552]\n",
      "loss: 0.361489  [28832/42552]\n",
      "loss: 0.358575  [32032/42552]\n",
      "loss: 0.258683  [35232/42552]\n",
      "loss: 0.232853  [38432/42552]\n",
      "loss: 0.574904  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.267979 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.415459  [   32/42552]\n",
      "loss: 0.501838  [ 3232/42552]\n",
      "loss: 0.308167  [ 6432/42552]\n",
      "loss: 0.295717  [ 9632/42552]\n",
      "loss: 0.418382  [12832/42552]\n",
      "loss: 0.220118  [16032/42552]\n",
      "loss: 0.208424  [19232/42552]\n",
      "loss: 0.304670  [22432/42552]\n",
      "loss: 0.328641  [25632/42552]\n",
      "loss: 0.292129  [28832/42552]\n",
      "loss: 0.350783  [32032/42552]\n",
      "loss: 0.351981  [35232/42552]\n",
      "loss: 0.286212  [38432/42552]\n",
      "loss: 0.301529  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.277167 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.328096  [   32/42552]\n",
      "loss: 0.245466  [ 3232/42552]\n",
      "loss: 0.184861  [ 6432/42552]\n",
      "loss: 0.484665  [ 9632/42552]\n",
      "loss: 0.136586  [12832/42552]\n",
      "loss: 0.326611  [16032/42552]\n",
      "loss: 0.238473  [19232/42552]\n",
      "loss: 0.192341  [22432/42552]\n",
      "loss: 0.258470  [25632/42552]\n",
      "loss: 0.514207  [28832/42552]\n",
      "loss: 0.306910  [32032/42552]\n",
      "loss: 0.309078  [35232/42552]\n",
      "loss: 0.216673  [38432/42552]\n",
      "loss: 0.407641  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.269661 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.255483  [   32/42552]\n",
      "loss: 0.328907  [ 3232/42552]\n",
      "loss: 0.374507  [ 6432/42552]\n",
      "loss: 0.356886  [ 9632/42552]\n",
      "loss: 0.265694  [12832/42552]\n",
      "loss: 0.290264  [16032/42552]\n",
      "loss: 0.346272  [19232/42552]\n",
      "loss: 0.404858  [22432/42552]\n",
      "loss: 0.261795  [25632/42552]\n",
      "loss: 0.275988  [28832/42552]\n",
      "loss: 0.280214  [32032/42552]\n",
      "loss: 0.288753  [35232/42552]\n",
      "loss: 0.453443  [38432/42552]\n",
      "loss: 0.401524  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.267502 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.196132  [   32/42552]\n",
      "loss: 0.268642  [ 3232/42552]\n",
      "loss: 0.240939  [ 6432/42552]\n",
      "loss: 0.290735  [ 9632/42552]\n",
      "loss: 0.325592  [12832/42552]\n",
      "loss: 0.210384  [16032/42552]\n",
      "loss: 0.285365  [19232/42552]\n",
      "loss: 0.253295  [22432/42552]\n",
      "loss: 0.279755  [25632/42552]\n",
      "loss: 0.407782  [28832/42552]\n",
      "loss: 0.283058  [32032/42552]\n",
      "loss: 0.341140  [35232/42552]\n",
      "loss: 0.291532  [38432/42552]\n",
      "loss: 0.286071  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.268845 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.354821  [   32/42552]\n",
      "loss: 0.259866  [ 3232/42552]\n",
      "loss: 0.236360  [ 6432/42552]\n",
      "loss: 0.359868  [ 9632/42552]\n",
      "loss: 0.413837  [12832/42552]\n",
      "loss: 0.310371  [16032/42552]\n",
      "loss: 0.252565  [19232/42552]\n",
      "loss: 0.248636  [22432/42552]\n",
      "loss: 0.379306  [25632/42552]\n",
      "loss: 0.497879  [28832/42552]\n",
      "loss: 0.362085  [32032/42552]\n",
      "loss: 0.307384  [35232/42552]\n",
      "loss: 0.228150  [38432/42552]\n",
      "loss: 0.284005  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.269610 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.297855  [   32/42552]\n",
      "loss: 0.208148  [ 3232/42552]\n",
      "loss: 0.301675  [ 6432/42552]\n",
      "loss: 0.236924  [ 9632/42552]\n",
      "loss: 0.510132  [12832/42552]\n",
      "loss: 0.411926  [16032/42552]\n",
      "loss: 0.236238  [19232/42552]\n",
      "loss: 0.335230  [22432/42552]\n",
      "loss: 0.518518  [25632/42552]\n",
      "loss: 0.259695  [28832/42552]\n",
      "loss: 0.319503  [32032/42552]\n",
      "loss: 0.287402  [35232/42552]\n",
      "loss: 0.219898  [38432/42552]\n",
      "loss: 0.507119  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.274671 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.269046  [   32/42552]\n",
      "loss: 0.229957  [ 3232/42552]\n",
      "loss: 0.425952  [ 6432/42552]\n",
      "loss: 0.337700  [ 9632/42552]\n",
      "loss: 0.367933  [12832/42552]\n",
      "loss: 0.383822  [16032/42552]\n",
      "loss: 0.333229  [19232/42552]\n",
      "loss: 0.390323  [22432/42552]\n",
      "loss: 0.348467  [25632/42552]\n",
      "loss: 0.376033  [28832/42552]\n",
      "loss: 0.337818  [32032/42552]\n",
      "loss: 0.313358  [35232/42552]\n",
      "loss: 0.269759  [38432/42552]\n",
      "loss: 0.228481  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.268506 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.315796  [   32/42552]\n",
      "loss: 0.318817  [ 3232/42552]\n",
      "loss: 0.280809  [ 6432/42552]\n",
      "loss: 0.432129  [ 9632/42552]\n",
      "loss: 0.286643  [12832/42552]\n",
      "loss: 0.321452  [16032/42552]\n",
      "loss: 0.337182  [19232/42552]\n",
      "loss: 0.349506  [22432/42552]\n",
      "loss: 0.261263  [25632/42552]\n",
      "loss: 0.286189  [28832/42552]\n",
      "loss: 0.315811  [32032/42552]\n",
      "loss: 0.287429  [35232/42552]\n",
      "loss: 0.220619  [38432/42552]\n",
      "loss: 0.303624  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.265172 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.240088  [   32/42552]\n",
      "loss: 0.271265  [ 3232/42552]\n",
      "loss: 0.382172  [ 6432/42552]\n",
      "loss: 0.330220  [ 9632/42552]\n",
      "loss: 0.291052  [12832/42552]\n",
      "loss: 0.150698  [16032/42552]\n",
      "loss: 0.307832  [19232/42552]\n",
      "loss: 0.342169  [22432/42552]\n",
      "loss: 0.373060  [25632/42552]\n",
      "loss: 0.249247  [28832/42552]\n",
      "loss: 0.340670  [32032/42552]\n",
      "loss: 0.299611  [35232/42552]\n",
      "loss: 0.216346  [38432/42552]\n",
      "loss: 0.311784  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.279354 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.321980  [   32/42552]\n",
      "loss: 0.368081  [ 3232/42552]\n",
      "loss: 0.543056  [ 6432/42552]\n",
      "loss: 0.274781  [ 9632/42552]\n",
      "loss: 0.329491  [12832/42552]\n",
      "loss: 0.298052  [16032/42552]\n",
      "loss: 0.292462  [19232/42552]\n",
      "loss: 0.387253  [22432/42552]\n",
      "loss: 0.368697  [25632/42552]\n",
      "loss: 0.323731  [28832/42552]\n",
      "loss: 0.290374  [32032/42552]\n",
      "loss: 0.287847  [35232/42552]\n",
      "loss: 0.237371  [38432/42552]\n",
      "loss: 0.327272  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.275421 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.270764  [   32/42552]\n",
      "loss: 0.347155  [ 3232/42552]\n",
      "loss: 0.298783  [ 6432/42552]\n",
      "loss: 0.299617  [ 9632/42552]\n",
      "loss: 0.304970  [12832/42552]\n",
      "loss: 0.373315  [16032/42552]\n",
      "loss: 0.299669  [19232/42552]\n",
      "loss: 0.414556  [22432/42552]\n",
      "loss: 0.596922  [25632/42552]\n",
      "loss: 0.280587  [28832/42552]\n",
      "loss: 0.277415  [32032/42552]\n",
      "loss: 0.244843  [35232/42552]\n",
      "loss: 0.343450  [38432/42552]\n",
      "loss: 0.228195  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.267554 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.354815  [   32/42552]\n",
      "loss: 0.567923  [ 3232/42552]\n",
      "loss: 0.284844  [ 6432/42552]\n",
      "loss: 0.216632  [ 9632/42552]\n",
      "loss: 0.261505  [12832/42552]\n",
      "loss: 0.295696  [16032/42552]\n",
      "loss: 0.429899  [19232/42552]\n",
      "loss: 0.226494  [22432/42552]\n",
      "loss: 0.390463  [25632/42552]\n",
      "loss: 0.214114  [28832/42552]\n",
      "loss: 0.473271  [32032/42552]\n",
      "loss: 0.389576  [35232/42552]\n",
      "loss: 0.335640  [38432/42552]\n",
      "loss: 0.347705  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.272611 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.524267  [   32/42552]\n",
      "loss: 0.312219  [ 3232/42552]\n",
      "loss: 0.289705  [ 6432/42552]\n",
      "loss: 0.276849  [ 9632/42552]\n",
      "loss: 0.352031  [12832/42552]\n",
      "loss: 0.300408  [16032/42552]\n",
      "loss: 0.242400  [19232/42552]\n",
      "loss: 0.414393  [22432/42552]\n",
      "loss: 0.451673  [25632/42552]\n",
      "loss: 0.499818  [28832/42552]\n",
      "loss: 0.400982  [32032/42552]\n",
      "loss: 0.324697  [35232/42552]\n",
      "loss: 0.380892  [38432/42552]\n",
      "loss: 0.390094  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.266115 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.342209  [   32/42552]\n",
      "loss: 0.252239  [ 3232/42552]\n",
      "loss: 0.353349  [ 6432/42552]\n",
      "loss: 0.316345  [ 9632/42552]\n",
      "loss: 0.244965  [12832/42552]\n",
      "loss: 0.517915  [16032/42552]\n",
      "loss: 0.384454  [19232/42552]\n",
      "loss: 0.359806  [22432/42552]\n",
      "loss: 0.258420  [25632/42552]\n",
      "loss: 0.225420  [28832/42552]\n",
      "loss: 0.218573  [32032/42552]\n",
      "loss: 0.380790  [35232/42552]\n",
      "loss: 0.311183  [38432/42552]\n",
      "loss: 0.213255  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.262196 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.189137  [   32/42552]\n",
      "loss: 0.231765  [ 3232/42552]\n",
      "loss: 0.193410  [ 6432/42552]\n",
      "loss: 0.245673  [ 9632/42552]\n",
      "loss: 0.291450  [12832/42552]\n",
      "loss: 0.343466  [16032/42552]\n",
      "loss: 0.321634  [19232/42552]\n",
      "loss: 0.242639  [22432/42552]\n",
      "loss: 0.326947  [25632/42552]\n",
      "loss: 0.340853  [28832/42552]\n",
      "loss: 0.268892  [32032/42552]\n",
      "loss: 0.183611  [35232/42552]\n",
      "loss: 0.219198  [38432/42552]\n",
      "loss: 0.235676  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.259146 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.294231  [   32/42552]\n",
      "loss: 0.357914  [ 3232/42552]\n",
      "loss: 0.163263  [ 6432/42552]\n",
      "loss: 0.360580  [ 9632/42552]\n",
      "loss: 0.300524  [12832/42552]\n",
      "loss: 0.359495  [16032/42552]\n",
      "loss: 0.361983  [19232/42552]\n",
      "loss: 0.451550  [22432/42552]\n",
      "loss: 0.338198  [25632/42552]\n",
      "loss: 0.320175  [28832/42552]\n",
      "loss: 0.175965  [32032/42552]\n",
      "loss: 0.541819  [35232/42552]\n",
      "loss: 0.329545  [38432/42552]\n",
      "loss: 0.184272  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.266710 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.298030  [   32/42552]\n",
      "loss: 0.298560  [ 3232/42552]\n",
      "loss: 0.245724  [ 6432/42552]\n",
      "loss: 0.260676  [ 9632/42552]\n",
      "loss: 0.262305  [12832/42552]\n",
      "loss: 0.327586  [16032/42552]\n",
      "loss: 0.247126  [19232/42552]\n",
      "loss: 0.439229  [22432/42552]\n",
      "loss: 0.228369  [25632/42552]\n",
      "loss: 0.288732  [28832/42552]\n",
      "loss: 0.277973  [32032/42552]\n",
      "loss: 0.324736  [35232/42552]\n",
      "loss: 0.300527  [38432/42552]\n",
      "loss: 0.387100  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.261543 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.241237  [   32/42552]\n",
      "loss: 0.336109  [ 3232/42552]\n",
      "loss: 0.303294  [ 6432/42552]\n",
      "loss: 0.301845  [ 9632/42552]\n",
      "loss: 0.284876  [12832/42552]\n",
      "loss: 0.165336  [16032/42552]\n",
      "loss: 0.289082  [19232/42552]\n",
      "loss: 0.304811  [22432/42552]\n",
      "loss: 0.262864  [25632/42552]\n",
      "loss: 0.226264  [28832/42552]\n",
      "loss: 0.221957  [32032/42552]\n",
      "loss: 0.320777  [35232/42552]\n",
      "loss: 0.392754  [38432/42552]\n",
      "loss: 0.250809  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.272815 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.245809  [   32/42552]\n",
      "loss: 0.363973  [ 3232/42552]\n",
      "loss: 0.342275  [ 6432/42552]\n",
      "loss: 0.284790  [ 9632/42552]\n",
      "loss: 0.312373  [12832/42552]\n",
      "loss: 0.218280  [16032/42552]\n",
      "loss: 0.334733  [19232/42552]\n",
      "loss: 0.398988  [22432/42552]\n",
      "loss: 0.316387  [25632/42552]\n",
      "loss: 0.499055  [28832/42552]\n",
      "loss: 0.486210  [32032/42552]\n",
      "loss: 0.395946  [35232/42552]\n",
      "loss: 0.269494  [38432/42552]\n",
      "loss: 0.369608  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.276119 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.383721  [   32/42552]\n",
      "loss: 0.416765  [ 3232/42552]\n",
      "loss: 0.431574  [ 6432/42552]\n",
      "loss: 0.263563  [ 9632/42552]\n",
      "loss: 0.221058  [12832/42552]\n",
      "loss: 0.253886  [16032/42552]\n",
      "loss: 0.225210  [19232/42552]\n",
      "loss: 0.500879  [22432/42552]\n",
      "loss: 0.366325  [25632/42552]\n",
      "loss: 0.264372  [28832/42552]\n",
      "loss: 0.290440  [32032/42552]\n",
      "loss: 0.370659  [35232/42552]\n",
      "loss: 0.228783  [38432/42552]\n",
      "loss: 0.482609  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.269598 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.275267  [   32/42552]\n",
      "loss: 0.346847  [ 3232/42552]\n",
      "loss: 0.381773  [ 6432/42552]\n",
      "loss: 0.257596  [ 9632/42552]\n",
      "loss: 0.242142  [12832/42552]\n",
      "loss: 0.238736  [16032/42552]\n",
      "loss: 0.421064  [19232/42552]\n",
      "loss: 0.415783  [22432/42552]\n",
      "loss: 0.205707  [25632/42552]\n",
      "loss: 0.308345  [28832/42552]\n",
      "loss: 0.266482  [32032/42552]\n",
      "loss: 0.171038  [35232/42552]\n",
      "loss: 0.265412  [38432/42552]\n",
      "loss: 0.251299  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.264587 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.261526  [   32/42552]\n",
      "loss: 0.273731  [ 3232/42552]\n",
      "loss: 0.220991  [ 6432/42552]\n",
      "loss: 0.235348  [ 9632/42552]\n",
      "loss: 0.310217  [12832/42552]\n",
      "loss: 0.328116  [16032/42552]\n",
      "loss: 0.260607  [19232/42552]\n",
      "loss: 0.373081  [22432/42552]\n",
      "loss: 0.206029  [25632/42552]\n",
      "loss: 0.232588  [28832/42552]\n",
      "loss: 0.270584  [32032/42552]\n",
      "loss: 0.212835  [35232/42552]\n",
      "loss: 0.438868  [38432/42552]\n",
      "loss: 0.796491  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.268326 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.202282  [   32/42552]\n",
      "loss: 0.254613  [ 3232/42552]\n",
      "loss: 0.169608  [ 6432/42552]\n",
      "loss: 0.255654  [ 9632/42552]\n",
      "loss: 0.245352  [12832/42552]\n",
      "loss: 0.439127  [16032/42552]\n",
      "loss: 0.173305  [19232/42552]\n",
      "loss: 0.208460  [22432/42552]\n",
      "loss: 0.297399  [25632/42552]\n",
      "loss: 0.346286  [28832/42552]\n",
      "loss: 0.224133  [32032/42552]\n",
      "loss: 0.252793  [35232/42552]\n",
      "loss: 0.254705  [38432/42552]\n",
      "loss: 0.227309  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.262527 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.257289  [   32/42552]\n",
      "loss: 0.265565  [ 3232/42552]\n",
      "loss: 0.231418  [ 6432/42552]\n",
      "loss: 0.297119  [ 9632/42552]\n",
      "loss: 0.219983  [12832/42552]\n",
      "loss: 0.395542  [16032/42552]\n",
      "loss: 0.234151  [19232/42552]\n",
      "loss: 0.265057  [22432/42552]\n",
      "loss: 0.325023  [25632/42552]\n",
      "loss: 0.202458  [28832/42552]\n",
      "loss: 0.325902  [32032/42552]\n",
      "loss: 0.284741  [35232/42552]\n",
      "loss: 0.210572  [38432/42552]\n",
      "loss: 0.225023  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.267000 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.186517  [   32/42552]\n",
      "loss: 0.361361  [ 3232/42552]\n",
      "loss: 0.296050  [ 6432/42552]\n",
      "loss: 0.297427  [ 9632/42552]\n",
      "loss: 0.351658  [12832/42552]\n",
      "loss: 0.305260  [16032/42552]\n",
      "loss: 0.378237  [19232/42552]\n",
      "loss: 0.554218  [22432/42552]\n",
      "loss: 0.271171  [25632/42552]\n",
      "loss: 0.280292  [28832/42552]\n",
      "loss: 0.436554  [32032/42552]\n",
      "loss: 0.230553  [35232/42552]\n",
      "loss: 0.405678  [38432/42552]\n",
      "loss: 0.319208  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.263057 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.329288  [   32/42552]\n",
      "loss: 0.251239  [ 3232/42552]\n",
      "loss: 0.256137  [ 6432/42552]\n",
      "loss: 0.246306  [ 9632/42552]\n",
      "loss: 0.199643  [12832/42552]\n",
      "loss: 0.320756  [16032/42552]\n",
      "loss: 0.265137  [19232/42552]\n",
      "loss: 0.358965  [22432/42552]\n",
      "loss: 0.349965  [25632/42552]\n",
      "loss: 0.575488  [28832/42552]\n",
      "loss: 0.535304  [32032/42552]\n",
      "loss: 0.206098  [35232/42552]\n",
      "loss: 0.217605  [38432/42552]\n",
      "loss: 0.210713  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.254721 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.317343  [   32/42552]\n",
      "loss: 0.304998  [ 3232/42552]\n",
      "loss: 0.274368  [ 6432/42552]\n",
      "loss: 0.372178  [ 9632/42552]\n",
      "loss: 0.211721  [12832/42552]\n",
      "loss: 0.384815  [16032/42552]\n",
      "loss: 0.318248  [19232/42552]\n",
      "loss: 0.293591  [22432/42552]\n",
      "loss: 0.397746  [25632/42552]\n",
      "loss: 0.354561  [28832/42552]\n",
      "loss: 0.269467  [32032/42552]\n",
      "loss: 0.375794  [35232/42552]\n",
      "loss: 0.344269  [38432/42552]\n",
      "loss: 0.280442  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.269477 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.275416  [   32/42552]\n",
      "loss: 0.255052  [ 3232/42552]\n",
      "loss: 0.339775  [ 6432/42552]\n",
      "loss: 0.282124  [ 9632/42552]\n",
      "loss: 0.227068  [12832/42552]\n",
      "loss: 0.339015  [16032/42552]\n",
      "loss: 0.391685  [19232/42552]\n",
      "loss: 0.296619  [22432/42552]\n",
      "loss: 0.226970  [25632/42552]\n",
      "loss: 0.459688  [28832/42552]\n",
      "loss: 0.243142  [32032/42552]\n",
      "loss: 0.357162  [35232/42552]\n",
      "loss: 0.270294  [38432/42552]\n",
      "loss: 0.355572  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.265238 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.269251  [   32/42552]\n",
      "loss: 0.254605  [ 3232/42552]\n",
      "loss: 0.452406  [ 6432/42552]\n",
      "loss: 0.378069  [ 9632/42552]\n",
      "loss: 0.326971  [12832/42552]\n",
      "loss: 0.228408  [16032/42552]\n",
      "loss: 0.270024  [19232/42552]\n",
      "loss: 0.328200  [22432/42552]\n",
      "loss: 0.287797  [25632/42552]\n",
      "loss: 0.355078  [28832/42552]\n",
      "loss: 0.360530  [32032/42552]\n",
      "loss: 0.281377  [35232/42552]\n",
      "loss: 0.237767  [38432/42552]\n",
      "loss: 0.307747  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.259844 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.253161  [   32/42552]\n",
      "loss: 0.271137  [ 3232/42552]\n",
      "loss: 0.390390  [ 6432/42552]\n",
      "loss: 0.292152  [ 9632/42552]\n",
      "loss: 0.270144  [12832/42552]\n",
      "loss: 0.400436  [16032/42552]\n",
      "loss: 0.271951  [19232/42552]\n",
      "loss: 0.197605  [22432/42552]\n",
      "loss: 0.226596  [25632/42552]\n",
      "loss: 0.432811  [28832/42552]\n",
      "loss: 0.324850  [32032/42552]\n",
      "loss: 0.306195  [35232/42552]\n",
      "loss: 0.417201  [38432/42552]\n",
      "loss: 0.323712  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.263719 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.178760  [   32/42552]\n",
      "loss: 0.363567  [ 3232/42552]\n",
      "loss: 0.190028  [ 6432/42552]\n",
      "loss: 0.345757  [ 9632/42552]\n",
      "loss: 0.239951  [12832/42552]\n",
      "loss: 0.406143  [16032/42552]\n",
      "loss: 0.180472  [19232/42552]\n",
      "loss: 0.185495  [22432/42552]\n",
      "loss: 0.359652  [25632/42552]\n",
      "loss: 0.354801  [28832/42552]\n",
      "loss: 0.219164  [32032/42552]\n",
      "loss: 0.275797  [35232/42552]\n",
      "loss: 0.323805  [38432/42552]\n",
      "loss: 0.290018  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.265337 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.366037  [   32/42552]\n",
      "loss: 0.143009  [ 3232/42552]\n",
      "loss: 0.286866  [ 6432/42552]\n",
      "loss: 0.391883  [ 9632/42552]\n",
      "loss: 0.308943  [12832/42552]\n",
      "loss: 0.195864  [16032/42552]\n",
      "loss: 0.263857  [19232/42552]\n",
      "loss: 0.385783  [22432/42552]\n",
      "loss: 0.250794  [25632/42552]\n",
      "loss: 0.418160  [28832/42552]\n",
      "loss: 0.295982  [32032/42552]\n",
      "loss: 0.344238  [35232/42552]\n",
      "loss: 0.293553  [38432/42552]\n",
      "loss: 0.230975  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.266777 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.354126  [   32/42552]\n",
      "loss: 0.545803  [ 3232/42552]\n",
      "loss: 0.459513  [ 6432/42552]\n",
      "loss: 0.318143  [ 9632/42552]\n",
      "loss: 0.369633  [12832/42552]\n",
      "loss: 0.261469  [16032/42552]\n",
      "loss: 0.367223  [19232/42552]\n",
      "loss: 0.191108  [22432/42552]\n",
      "loss: 0.303143  [25632/42552]\n",
      "loss: 0.390957  [28832/42552]\n",
      "loss: 0.233170  [32032/42552]\n",
      "loss: 0.243505  [35232/42552]\n",
      "loss: 0.312664  [38432/42552]\n",
      "loss: 0.367612  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.261346 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.354273  [   32/42552]\n",
      "loss: 0.208825  [ 3232/42552]\n",
      "loss: 0.293004  [ 6432/42552]\n",
      "loss: 0.348477  [ 9632/42552]\n",
      "loss: 0.294538  [12832/42552]\n",
      "loss: 0.221641  [16032/42552]\n",
      "loss: 0.240118  [19232/42552]\n",
      "loss: 0.306855  [22432/42552]\n",
      "loss: 0.264780  [25632/42552]\n",
      "loss: 0.205638  [28832/42552]\n",
      "loss: 0.271004  [32032/42552]\n",
      "loss: 0.199552  [35232/42552]\n",
      "loss: 0.269518  [38432/42552]\n",
      "loss: 0.249644  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.267991 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.237774  [   32/42552]\n",
      "loss: 0.331598  [ 3232/42552]\n",
      "loss: 0.328749  [ 6432/42552]\n",
      "loss: 0.323745  [ 9632/42552]\n",
      "loss: 0.219534  [12832/42552]\n",
      "loss: 0.355273  [16032/42552]\n",
      "loss: 0.424182  [19232/42552]\n",
      "loss: 0.316814  [22432/42552]\n",
      "loss: 0.332565  [25632/42552]\n",
      "loss: 0.259145  [28832/42552]\n",
      "loss: 0.325020  [32032/42552]\n",
      "loss: 0.307189  [35232/42552]\n",
      "loss: 0.216076  [38432/42552]\n",
      "loss: 0.337569  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.259812 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.386721  [   32/42552]\n",
      "loss: 0.262327  [ 3232/42552]\n",
      "loss: 0.331684  [ 6432/42552]\n",
      "loss: 0.425510  [ 9632/42552]\n",
      "loss: 0.368166  [12832/42552]\n",
      "loss: 0.442543  [16032/42552]\n",
      "loss: 0.321070  [19232/42552]\n",
      "loss: 0.366244  [22432/42552]\n",
      "loss: 0.333730  [25632/42552]\n",
      "loss: 0.331513  [28832/42552]\n",
      "loss: 0.216369  [32032/42552]\n",
      "loss: 0.232748  [35232/42552]\n",
      "loss: 0.440164  [38432/42552]\n",
      "loss: 0.413571  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.261084 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.338085  [   32/42552]\n",
      "loss: 0.464087  [ 3232/42552]\n",
      "loss: 0.300891  [ 6432/42552]\n",
      "loss: 0.291826  [ 9632/42552]\n",
      "loss: 0.271652  [12832/42552]\n",
      "loss: 0.194095  [16032/42552]\n",
      "loss: 0.152977  [19232/42552]\n",
      "loss: 0.315855  [22432/42552]\n",
      "loss: 0.145426  [25632/42552]\n",
      "loss: 0.320212  [28832/42552]\n",
      "loss: 0.347689  [32032/42552]\n",
      "loss: 0.307227  [35232/42552]\n",
      "loss: 0.375781  [38432/42552]\n",
      "loss: 0.321697  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.263792 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.258998  [   32/42552]\n",
      "loss: 0.240714  [ 3232/42552]\n",
      "loss: 0.272913  [ 6432/42552]\n",
      "loss: 0.244908  [ 9632/42552]\n",
      "loss: 0.260877  [12832/42552]\n",
      "loss: 0.288364  [16032/42552]\n",
      "loss: 0.385780  [19232/42552]\n",
      "loss: 0.303615  [22432/42552]\n",
      "loss: 0.371406  [25632/42552]\n",
      "loss: 0.369596  [28832/42552]\n",
      "loss: 0.310946  [32032/42552]\n",
      "loss: 0.384049  [35232/42552]\n",
      "loss: 0.196648  [38432/42552]\n",
      "loss: 0.393716  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.257308 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.199851  [   32/42552]\n",
      "loss: 0.298164  [ 3232/42552]\n",
      "loss: 0.221355  [ 6432/42552]\n",
      "loss: 0.358324  [ 9632/42552]\n",
      "loss: 0.212921  [12832/42552]\n",
      "loss: 0.284216  [16032/42552]\n",
      "loss: 0.277978  [19232/42552]\n",
      "loss: 0.256448  [22432/42552]\n",
      "loss: 0.369339  [25632/42552]\n",
      "loss: 0.299006  [28832/42552]\n",
      "loss: 0.422565  [32032/42552]\n",
      "loss: 0.184314  [35232/42552]\n",
      "loss: 0.341768  [38432/42552]\n",
      "loss: 0.227434  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.255914 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.325362  [   32/42552]\n",
      "loss: 0.405140  [ 3232/42552]\n",
      "loss: 0.234623  [ 6432/42552]\n",
      "loss: 0.383666  [ 9632/42552]\n",
      "loss: 0.175411  [12832/42552]\n",
      "loss: 0.231991  [16032/42552]\n",
      "loss: 0.281013  [19232/42552]\n",
      "loss: 0.176802  [22432/42552]\n",
      "loss: 0.170626  [25632/42552]\n",
      "loss: 0.399548  [28832/42552]\n",
      "loss: 0.380140  [32032/42552]\n",
      "loss: 0.330695  [35232/42552]\n",
      "loss: 0.452229  [38432/42552]\n",
      "loss: 0.256868  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.260549 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.236408  [   32/42552]\n",
      "loss: 0.234353  [ 3232/42552]\n",
      "loss: 0.259718  [ 6432/42552]\n",
      "loss: 0.400927  [ 9632/42552]\n",
      "loss: 0.234768  [12832/42552]\n",
      "loss: 0.233681  [16032/42552]\n",
      "loss: 0.355058  [19232/42552]\n",
      "loss: 0.236378  [22432/42552]\n",
      "loss: 0.461212  [25632/42552]\n",
      "loss: 0.392492  [28832/42552]\n",
      "loss: 0.264587  [32032/42552]\n",
      "loss: 0.460526  [35232/42552]\n",
      "loss: 0.259194  [38432/42552]\n",
      "loss: 0.453926  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.263900 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.266466  [   32/42552]\n",
      "loss: 0.240084  [ 3232/42552]\n",
      "loss: 0.328018  [ 6432/42552]\n",
      "loss: 0.250454  [ 9632/42552]\n",
      "loss: 0.359912  [12832/42552]\n",
      "loss: 0.139680  [16032/42552]\n",
      "loss: 0.325140  [19232/42552]\n",
      "loss: 0.460430  [22432/42552]\n",
      "loss: 0.380025  [25632/42552]\n",
      "loss: 0.314661  [28832/42552]\n",
      "loss: 0.152042  [32032/42552]\n",
      "loss: 0.328985  [35232/42552]\n",
      "loss: 0.376172  [38432/42552]\n",
      "loss: 0.366932  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.255000 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.418147  [   32/42552]\n",
      "loss: 0.339340  [ 3232/42552]\n",
      "loss: 0.363332  [ 6432/42552]\n",
      "loss: 0.299400  [ 9632/42552]\n",
      "loss: 0.355715  [12832/42552]\n",
      "loss: 0.281136  [16032/42552]\n",
      "loss: 0.239838  [19232/42552]\n",
      "loss: 0.253504  [22432/42552]\n",
      "loss: 0.382404  [25632/42552]\n",
      "loss: 0.371814  [28832/42552]\n",
      "loss: 0.392682  [32032/42552]\n",
      "loss: 0.487170  [35232/42552]\n",
      "loss: 0.279988  [38432/42552]\n",
      "loss: 0.301823  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.268536 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.327462  [   32/42552]\n",
      "loss: 0.199332  [ 3232/42552]\n",
      "loss: 0.241525  [ 6432/42552]\n",
      "loss: 0.431200  [ 9632/42552]\n",
      "loss: 0.337177  [12832/42552]\n",
      "loss: 0.193593  [16032/42552]\n",
      "loss: 0.287652  [19232/42552]\n",
      "loss: 0.217359  [22432/42552]\n",
      "loss: 0.432576  [25632/42552]\n",
      "loss: 0.222238  [28832/42552]\n",
      "loss: 0.246553  [32032/42552]\n",
      "loss: 0.313518  [35232/42552]\n",
      "loss: 0.265778  [38432/42552]\n",
      "loss: 0.171893  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.256523 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.382612  [   32/42552]\n",
      "loss: 0.214768  [ 3232/42552]\n",
      "loss: 0.397279  [ 6432/42552]\n",
      "loss: 0.257559  [ 9632/42552]\n",
      "loss: 0.210190  [12832/42552]\n",
      "loss: 0.355410  [16032/42552]\n",
      "loss: 0.359927  [19232/42552]\n",
      "loss: 0.276346  [22432/42552]\n",
      "loss: 0.217562  [25632/42552]\n",
      "loss: 0.166804  [28832/42552]\n",
      "loss: 0.365668  [32032/42552]\n",
      "loss: 0.354598  [35232/42552]\n",
      "loss: 0.269196  [38432/42552]\n",
      "loss: 0.315307  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.254085 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.283865  [   32/42552]\n",
      "loss: 0.293268  [ 3232/42552]\n",
      "loss: 0.313942  [ 6432/42552]\n",
      "loss: 0.333286  [ 9632/42552]\n",
      "loss: 0.304384  [12832/42552]\n",
      "loss: 0.243561  [16032/42552]\n",
      "loss: 0.402251  [19232/42552]\n",
      "loss: 0.310778  [22432/42552]\n",
      "loss: 0.288132  [25632/42552]\n",
      "loss: 0.280738  [28832/42552]\n",
      "loss: 0.105145  [32032/42552]\n",
      "loss: 0.375138  [35232/42552]\n",
      "loss: 0.285410  [38432/42552]\n",
      "loss: 0.340114  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.258254 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.232112  [   32/42552]\n",
      "loss: 0.240592  [ 3232/42552]\n",
      "loss: 0.290673  [ 6432/42552]\n",
      "loss: 0.398440  [ 9632/42552]\n",
      "loss: 0.380458  [12832/42552]\n",
      "loss: 0.278409  [16032/42552]\n",
      "loss: 0.241447  [19232/42552]\n",
      "loss: 0.240184  [22432/42552]\n",
      "loss: 0.288650  [25632/42552]\n",
      "loss: 0.333837  [28832/42552]\n",
      "loss: 0.245178  [32032/42552]\n",
      "loss: 0.345090  [35232/42552]\n",
      "loss: 0.540994  [38432/42552]\n",
      "loss: 0.274395  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.257793 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.296541  [   32/42552]\n",
      "loss: 0.214735  [ 3232/42552]\n",
      "loss: 0.252357  [ 6432/42552]\n",
      "loss: 0.331367  [ 9632/42552]\n",
      "loss: 0.237571  [12832/42552]\n",
      "loss: 0.453995  [16032/42552]\n",
      "loss: 0.303966  [19232/42552]\n",
      "loss: 0.272208  [22432/42552]\n",
      "loss: 0.296737  [25632/42552]\n",
      "loss: 0.216905  [28832/42552]\n",
      "loss: 0.305230  [32032/42552]\n",
      "loss: 0.266641  [35232/42552]\n",
      "loss: 0.202660  [38432/42552]\n",
      "loss: 0.326861  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.261491 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.183642  [   32/42552]\n",
      "loss: 0.233074  [ 3232/42552]\n",
      "loss: 0.234912  [ 6432/42552]\n",
      "loss: 0.421129  [ 9632/42552]\n",
      "loss: 0.533184  [12832/42552]\n",
      "loss: 0.235364  [16032/42552]\n",
      "loss: 0.436110  [19232/42552]\n",
      "loss: 0.320055  [22432/42552]\n",
      "loss: 0.249422  [25632/42552]\n",
      "loss: 0.152647  [28832/42552]\n",
      "loss: 0.254188  [32032/42552]\n",
      "loss: 0.292467  [35232/42552]\n",
      "loss: 0.583513  [38432/42552]\n",
      "loss: 0.139124  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.258236 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.223962  [   32/42552]\n",
      "loss: 0.280348  [ 3232/42552]\n",
      "loss: 0.345925  [ 6432/42552]\n",
      "loss: 0.332560  [ 9632/42552]\n",
      "loss: 0.322569  [12832/42552]\n",
      "loss: 0.268676  [16032/42552]\n",
      "loss: 0.294828  [19232/42552]\n",
      "loss: 0.300340  [22432/42552]\n",
      "loss: 0.260881  [25632/42552]\n",
      "loss: 0.305960  [28832/42552]\n",
      "loss: 0.225861  [32032/42552]\n",
      "loss: 0.387761  [35232/42552]\n",
      "loss: 0.324241  [38432/42552]\n",
      "loss: 0.234891  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.253946 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.307361  [   32/42552]\n",
      "loss: 0.244012  [ 3232/42552]\n",
      "loss: 0.265713  [ 6432/42552]\n",
      "loss: 0.266361  [ 9632/42552]\n",
      "loss: 0.298889  [12832/42552]\n",
      "loss: 0.272236  [16032/42552]\n",
      "loss: 0.326160  [19232/42552]\n",
      "loss: 0.253227  [22432/42552]\n",
      "loss: 0.262337  [25632/42552]\n",
      "loss: 0.328134  [28832/42552]\n",
      "loss: 0.217581  [32032/42552]\n",
      "loss: 0.227234  [35232/42552]\n",
      "loss: 0.242820  [38432/42552]\n",
      "loss: 0.223803  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.257335 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.406624  [   32/42552]\n",
      "loss: 0.199161  [ 3232/42552]\n",
      "loss: 0.348505  [ 6432/42552]\n",
      "loss: 0.336381  [ 9632/42552]\n",
      "loss: 0.268059  [12832/42552]\n",
      "loss: 0.366892  [16032/42552]\n",
      "loss: 0.288697  [19232/42552]\n",
      "loss: 0.189423  [22432/42552]\n",
      "loss: 0.377389  [25632/42552]\n",
      "loss: 0.472848  [28832/42552]\n",
      "loss: 0.206095  [32032/42552]\n",
      "loss: 0.339020  [35232/42552]\n",
      "loss: 0.280472  [38432/42552]\n",
      "loss: 0.255046  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.256025 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.191938  [   32/42552]\n",
      "loss: 0.243022  [ 3232/42552]\n",
      "loss: 0.426185  [ 6432/42552]\n",
      "loss: 0.240867  [ 9632/42552]\n",
      "loss: 0.239156  [12832/42552]\n",
      "loss: 0.234016  [16032/42552]\n",
      "loss: 0.285476  [19232/42552]\n",
      "loss: 0.282219  [22432/42552]\n",
      "loss: 0.230884  [25632/42552]\n",
      "loss: 0.177710  [28832/42552]\n",
      "loss: 0.294818  [32032/42552]\n",
      "loss: 0.309422  [35232/42552]\n",
      "loss: 0.278251  [38432/42552]\n",
      "loss: 0.185365  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.263795 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.316494  [   32/42552]\n",
      "loss: 0.494839  [ 3232/42552]\n",
      "loss: 0.239141  [ 6432/42552]\n",
      "loss: 0.285507  [ 9632/42552]\n",
      "loss: 0.397584  [12832/42552]\n",
      "loss: 0.464813  [16032/42552]\n",
      "loss: 0.216750  [19232/42552]\n",
      "loss: 0.293189  [22432/42552]\n",
      "loss: 0.229047  [25632/42552]\n",
      "loss: 0.337307  [28832/42552]\n",
      "loss: 0.234596  [32032/42552]\n",
      "loss: 0.191132  [35232/42552]\n",
      "loss: 0.210177  [38432/42552]\n",
      "loss: 0.397257  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.275003 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.209307  [   32/42552]\n",
      "loss: 0.274179  [ 3232/42552]\n",
      "loss: 0.322465  [ 6432/42552]\n",
      "loss: 0.310993  [ 9632/42552]\n",
      "loss: 0.277089  [12832/42552]\n",
      "loss: 0.235748  [16032/42552]\n",
      "loss: 0.379426  [19232/42552]\n",
      "loss: 0.458355  [22432/42552]\n",
      "loss: 0.537571  [25632/42552]\n",
      "loss: 0.264331  [28832/42552]\n",
      "loss: 0.352107  [32032/42552]\n",
      "loss: 0.349202  [35232/42552]\n",
      "loss: 0.416867  [38432/42552]\n",
      "loss: 0.228886  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.260563 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.405061  [   32/42552]\n",
      "loss: 0.215444  [ 3232/42552]\n",
      "loss: 0.203965  [ 6432/42552]\n",
      "loss: 0.303389  [ 9632/42552]\n",
      "loss: 0.267979  [12832/42552]\n",
      "loss: 0.334019  [16032/42552]\n",
      "loss: 0.236708  [19232/42552]\n",
      "loss: 0.318506  [22432/42552]\n",
      "loss: 0.253410  [25632/42552]\n",
      "loss: 0.241833  [28832/42552]\n",
      "loss: 0.363065  [32032/42552]\n",
      "loss: 0.448360  [35232/42552]\n",
      "loss: 0.186796  [38432/42552]\n",
      "loss: 0.460792  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.260502 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.281697  [   32/42552]\n",
      "loss: 0.224591  [ 3232/42552]\n",
      "loss: 0.328573  [ 6432/42552]\n",
      "loss: 0.350178  [ 9632/42552]\n",
      "loss: 0.313660  [12832/42552]\n",
      "loss: 0.308471  [16032/42552]\n",
      "loss: 0.246471  [19232/42552]\n",
      "loss: 0.235732  [22432/42552]\n",
      "loss: 0.327098  [25632/42552]\n",
      "loss: 0.268233  [28832/42552]\n",
      "loss: 0.292588  [32032/42552]\n",
      "loss: 0.285761  [35232/42552]\n",
      "loss: 0.324706  [38432/42552]\n",
      "loss: 0.325416  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.256390 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.292428  [   32/42552]\n",
      "loss: 0.377349  [ 3232/42552]\n",
      "loss: 0.355014  [ 6432/42552]\n",
      "loss: 0.332123  [ 9632/42552]\n",
      "loss: 0.388748  [12832/42552]\n",
      "loss: 0.256140  [16032/42552]\n",
      "loss: 0.157923  [19232/42552]\n",
      "loss: 0.263910  [22432/42552]\n",
      "loss: 0.462409  [25632/42552]\n",
      "loss: 0.303887  [28832/42552]\n",
      "loss: 0.336546  [32032/42552]\n",
      "loss: 0.357692  [35232/42552]\n",
      "loss: 0.348960  [38432/42552]\n",
      "loss: 0.340456  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.263415 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.373560  [   32/42552]\n",
      "loss: 0.258232  [ 3232/42552]\n",
      "loss: 0.292962  [ 6432/42552]\n",
      "loss: 0.251416  [ 9632/42552]\n",
      "loss: 0.356095  [12832/42552]\n",
      "loss: 0.173779  [16032/42552]\n",
      "loss: 0.313660  [19232/42552]\n",
      "loss: 0.280991  [22432/42552]\n",
      "loss: 0.246862  [25632/42552]\n",
      "loss: 0.231593  [28832/42552]\n",
      "loss: 0.239031  [32032/42552]\n",
      "loss: 0.357378  [35232/42552]\n",
      "loss: 0.306912  [38432/42552]\n",
      "loss: 0.208741  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.258307 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.262433  [   32/42552]\n",
      "loss: 0.357336  [ 3232/42552]\n",
      "loss: 0.482872  [ 6432/42552]\n",
      "loss: 0.277120  [ 9632/42552]\n",
      "loss: 0.248150  [12832/42552]\n",
      "loss: 0.328550  [16032/42552]\n",
      "loss: 0.283124  [19232/42552]\n",
      "loss: 0.430993  [22432/42552]\n",
      "loss: 0.165859  [25632/42552]\n",
      "loss: 0.294833  [28832/42552]\n",
      "loss: 0.276170  [32032/42552]\n",
      "loss: 0.415645  [35232/42552]\n",
      "loss: 0.293848  [38432/42552]\n",
      "loss: 0.423844  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.260460 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.277496  [   32/42552]\n",
      "loss: 0.208744  [ 3232/42552]\n",
      "loss: 0.247840  [ 6432/42552]\n",
      "loss: 0.359831  [ 9632/42552]\n",
      "loss: 0.261828  [12832/42552]\n",
      "loss: 0.443887  [16032/42552]\n",
      "loss: 0.238393  [19232/42552]\n",
      "loss: 0.298106  [22432/42552]\n",
      "loss: 0.198168  [25632/42552]\n",
      "loss: 0.320301  [28832/42552]\n",
      "loss: 0.281291  [32032/42552]\n",
      "loss: 0.224434  [35232/42552]\n",
      "loss: 0.303759  [38432/42552]\n",
      "loss: 0.170699  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.261526 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.364032  [   32/42552]\n",
      "loss: 0.221577  [ 3232/42552]\n",
      "loss: 0.234580  [ 6432/42552]\n",
      "loss: 0.399842  [ 9632/42552]\n",
      "loss: 0.442202  [12832/42552]\n",
      "loss: 0.304638  [16032/42552]\n",
      "loss: 0.352905  [19232/42552]\n",
      "loss: 0.255372  [22432/42552]\n",
      "loss: 0.345967  [25632/42552]\n",
      "loss: 0.306792  [28832/42552]\n",
      "loss: 0.460724  [32032/42552]\n",
      "loss: 0.304834  [35232/42552]\n",
      "loss: 0.292861  [38432/42552]\n",
      "loss: 0.225715  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.254509 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.302005  [   32/42552]\n",
      "loss: 0.290296  [ 3232/42552]\n",
      "loss: 0.144745  [ 6432/42552]\n",
      "loss: 0.332931  [ 9632/42552]\n",
      "loss: 0.177892  [12832/42552]\n",
      "loss: 0.276886  [16032/42552]\n",
      "loss: 0.263249  [19232/42552]\n",
      "loss: 0.250726  [22432/42552]\n",
      "loss: 0.167545  [25632/42552]\n",
      "loss: 0.250186  [28832/42552]\n",
      "loss: 0.260031  [32032/42552]\n",
      "loss: 0.283399  [35232/42552]\n",
      "loss: 0.246894  [38432/42552]\n",
      "loss: 0.237996  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.254103 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.258103  [   32/42552]\n",
      "loss: 0.265022  [ 3232/42552]\n",
      "loss: 0.319734  [ 6432/42552]\n",
      "loss: 0.323712  [ 9632/42552]\n",
      "loss: 0.277220  [12832/42552]\n",
      "loss: 0.197078  [16032/42552]\n",
      "loss: 0.199376  [19232/42552]\n",
      "loss: 0.337142  [22432/42552]\n",
      "loss: 0.411484  [25632/42552]\n",
      "loss: 0.326619  [28832/42552]\n",
      "loss: 0.209219  [32032/42552]\n",
      "loss: 0.314847  [35232/42552]\n",
      "loss: 0.375883  [38432/42552]\n",
      "loss: 0.281942  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.254987 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.202392  [   32/42552]\n",
      "loss: 0.212283  [ 3232/42552]\n",
      "loss: 0.236619  [ 6432/42552]\n",
      "loss: 0.447454  [ 9632/42552]\n",
      "loss: 0.206082  [12832/42552]\n",
      "loss: 0.311734  [16032/42552]\n",
      "loss: 0.227639  [19232/42552]\n",
      "loss: 0.282783  [22432/42552]\n",
      "loss: 0.260337  [25632/42552]\n",
      "loss: 0.203014  [28832/42552]\n",
      "loss: 0.250404  [32032/42552]\n",
      "loss: 0.324574  [35232/42552]\n",
      "loss: 0.238438  [38432/42552]\n",
      "loss: 0.300942  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.251072 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.172323  [   32/42552]\n",
      "loss: 0.316302  [ 3232/42552]\n",
      "loss: 0.310783  [ 6432/42552]\n",
      "loss: 0.277090  [ 9632/42552]\n",
      "loss: 0.281188  [12832/42552]\n",
      "loss: 0.318999  [16032/42552]\n",
      "loss: 0.264693  [19232/42552]\n",
      "loss: 0.267927  [22432/42552]\n",
      "loss: 0.512645  [25632/42552]\n",
      "loss: 0.425861  [28832/42552]\n",
      "loss: 0.301199  [32032/42552]\n",
      "loss: 0.243947  [35232/42552]\n",
      "loss: 0.182320  [38432/42552]\n",
      "loss: 0.251660  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.253839 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.294207  [   32/42552]\n",
      "loss: 0.441880  [ 3232/42552]\n",
      "loss: 0.256329  [ 6432/42552]\n",
      "loss: 0.345595  [ 9632/42552]\n",
      "loss: 0.270372  [12832/42552]\n",
      "loss: 0.318655  [16032/42552]\n",
      "loss: 0.369061  [19232/42552]\n",
      "loss: 0.252416  [22432/42552]\n",
      "loss: 0.211464  [25632/42552]\n",
      "loss: 0.273020  [28832/42552]\n",
      "loss: 0.266347  [32032/42552]\n",
      "loss: 0.245572  [35232/42552]\n",
      "loss: 0.211159  [38432/42552]\n",
      "loss: 0.266961  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.250551 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.401457  [   32/42552]\n",
      "loss: 0.382471  [ 3232/42552]\n",
      "loss: 0.257011  [ 6432/42552]\n",
      "loss: 0.428793  [ 9632/42552]\n",
      "loss: 0.419915  [12832/42552]\n",
      "loss: 0.201734  [16032/42552]\n",
      "loss: 0.261289  [19232/42552]\n",
      "loss: 0.309115  [22432/42552]\n",
      "loss: 0.361997  [25632/42552]\n",
      "loss: 0.342530  [28832/42552]\n",
      "loss: 0.239141  [32032/42552]\n",
      "loss: 0.310006  [35232/42552]\n",
      "loss: 0.235642  [38432/42552]\n",
      "loss: 0.311277  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.269879 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.500010  [   32/42552]\n",
      "loss: 0.281230  [ 3232/42552]\n",
      "loss: 0.368930  [ 6432/42552]\n",
      "loss: 0.349448  [ 9632/42552]\n",
      "loss: 0.196200  [12832/42552]\n",
      "loss: 0.399833  [16032/42552]\n",
      "loss: 0.273674  [19232/42552]\n",
      "loss: 0.322710  [22432/42552]\n",
      "loss: 0.346000  [25632/42552]\n",
      "loss: 0.234241  [28832/42552]\n",
      "loss: 0.185943  [32032/42552]\n",
      "loss: 0.469649  [35232/42552]\n",
      "loss: 0.195600  [38432/42552]\n",
      "loss: 0.368159  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.255856 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.378262  [   32/42552]\n",
      "loss: 0.198517  [ 3232/42552]\n",
      "loss: 0.170908  [ 6432/42552]\n",
      "loss: 0.374030  [ 9632/42552]\n",
      "loss: 0.284267  [12832/42552]\n",
      "loss: 0.222156  [16032/42552]\n",
      "loss: 0.276210  [19232/42552]\n",
      "loss: 0.217261  [22432/42552]\n",
      "loss: 0.243874  [25632/42552]\n",
      "loss: 0.337285  [28832/42552]\n",
      "loss: 0.291391  [32032/42552]\n",
      "loss: 0.277050  [35232/42552]\n",
      "loss: 0.349767  [38432/42552]\n",
      "loss: 0.329800  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.252574 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.390330  [   32/42552]\n",
      "loss: 0.162550  [ 3232/42552]\n",
      "loss: 0.297570  [ 6432/42552]\n",
      "loss: 0.302171  [ 9632/42552]\n",
      "loss: 0.293913  [12832/42552]\n",
      "loss: 0.261960  [16032/42552]\n",
      "loss: 0.389246  [19232/42552]\n",
      "loss: 0.265524  [22432/42552]\n",
      "loss: 0.304457  [25632/42552]\n",
      "loss: 0.308447  [28832/42552]\n",
      "loss: 0.200660  [32032/42552]\n",
      "loss: 0.231152  [35232/42552]\n",
      "loss: 0.266384  [38432/42552]\n",
      "loss: 0.408707  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.266188 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.255158  [   32/42552]\n",
      "loss: 0.234841  [ 3232/42552]\n",
      "loss: 0.411476  [ 6432/42552]\n",
      "loss: 0.328524  [ 9632/42552]\n",
      "loss: 0.343660  [12832/42552]\n",
      "loss: 0.222880  [16032/42552]\n",
      "loss: 0.284484  [19232/42552]\n",
      "loss: 0.397024  [22432/42552]\n",
      "loss: 0.304084  [25632/42552]\n",
      "loss: 0.238848  [28832/42552]\n",
      "loss: 0.312983  [32032/42552]\n",
      "loss: 0.260839  [35232/42552]\n",
      "loss: 0.304602  [38432/42552]\n",
      "loss: 0.243920  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.257551 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.309703  [   32/42552]\n",
      "loss: 0.293201  [ 3232/42552]\n",
      "loss: 0.376830  [ 6432/42552]\n",
      "loss: 0.313463  [ 9632/42552]\n",
      "loss: 0.300844  [12832/42552]\n",
      "loss: 0.243235  [16032/42552]\n",
      "loss: 0.274488  [19232/42552]\n",
      "loss: 0.164221  [22432/42552]\n",
      "loss: 0.459898  [25632/42552]\n",
      "loss: 0.228438  [28832/42552]\n",
      "loss: 0.174478  [32032/42552]\n",
      "loss: 0.231278  [35232/42552]\n",
      "loss: 0.192509  [38432/42552]\n",
      "loss: 0.235915  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.252598 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.483468  [   32/42552]\n",
      "loss: 0.423541  [ 3232/42552]\n",
      "loss: 0.300810  [ 6432/42552]\n",
      "loss: 0.422029  [ 9632/42552]\n",
      "loss: 0.316724  [12832/42552]\n",
      "loss: 0.241310  [16032/42552]\n",
      "loss: 0.464534  [19232/42552]\n",
      "loss: 0.230836  [22432/42552]\n",
      "loss: 0.373205  [25632/42552]\n",
      "loss: 0.289623  [28832/42552]\n",
      "loss: 0.226547  [32032/42552]\n",
      "loss: 0.436195  [35232/42552]\n",
      "loss: 0.228943  [38432/42552]\n",
      "loss: 0.292172  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.250772 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.289545  [   32/42552]\n",
      "loss: 0.360642  [ 3232/42552]\n",
      "loss: 0.379228  [ 6432/42552]\n",
      "loss: 0.266085  [ 9632/42552]\n",
      "loss: 0.244751  [12832/42552]\n",
      "loss: 0.286389  [16032/42552]\n",
      "loss: 0.185407  [19232/42552]\n",
      "loss: 0.313416  [22432/42552]\n",
      "loss: 0.363518  [25632/42552]\n",
      "loss: 0.249452  [28832/42552]\n",
      "loss: 0.323735  [32032/42552]\n",
      "loss: 0.219821  [35232/42552]\n",
      "loss: 0.206760  [38432/42552]\n",
      "loss: 0.375308  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.257994 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.351278  [   32/42552]\n",
      "loss: 0.232125  [ 3232/42552]\n",
      "loss: 0.291824  [ 6432/42552]\n",
      "loss: 0.335060  [ 9632/42552]\n",
      "loss: 0.211537  [12832/42552]\n",
      "loss: 0.322696  [16032/42552]\n",
      "loss: 0.197691  [19232/42552]\n",
      "loss: 0.233102  [22432/42552]\n",
      "loss: 0.316350  [25632/42552]\n",
      "loss: 0.248950  [28832/42552]\n",
      "loss: 0.287960  [32032/42552]\n",
      "loss: 0.374835  [35232/42552]\n",
      "loss: 0.138831  [38432/42552]\n",
      "loss: 0.317578  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.245997 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.319647  [   32/42552]\n",
      "loss: 0.223985  [ 3232/42552]\n",
      "loss: 0.416707  [ 6432/42552]\n",
      "loss: 0.344959  [ 9632/42552]\n",
      "loss: 0.290542  [12832/42552]\n",
      "loss: 0.376286  [16032/42552]\n",
      "loss: 0.243442  [19232/42552]\n",
      "loss: 0.204983  [22432/42552]\n",
      "loss: 0.248724  [25632/42552]\n",
      "loss: 0.350604  [28832/42552]\n",
      "loss: 0.208968  [32032/42552]\n",
      "loss: 0.316887  [35232/42552]\n",
      "loss: 0.299916  [38432/42552]\n",
      "loss: 0.324344  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.247885 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.325403  [   32/42552]\n",
      "loss: 0.320046  [ 3232/42552]\n",
      "loss: 0.418970  [ 6432/42552]\n",
      "loss: 0.218957  [ 9632/42552]\n",
      "loss: 0.287486  [12832/42552]\n",
      "loss: 0.419257  [16032/42552]\n",
      "loss: 0.399535  [19232/42552]\n",
      "loss: 0.277421  [22432/42552]\n",
      "loss: 0.325358  [25632/42552]\n",
      "loss: 0.164579  [28832/42552]\n",
      "loss: 0.367177  [32032/42552]\n",
      "loss: 0.289996  [35232/42552]\n",
      "loss: 0.252764  [38432/42552]\n",
      "loss: 0.200602  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.254622 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.315599  [   32/42552]\n",
      "loss: 0.237467  [ 3232/42552]\n",
      "loss: 0.290887  [ 6432/42552]\n",
      "loss: 0.194482  [ 9632/42552]\n",
      "loss: 0.314192  [12832/42552]\n",
      "loss: 0.368203  [16032/42552]\n",
      "loss: 0.371006  [19232/42552]\n",
      "loss: 0.316088  [22432/42552]\n",
      "loss: 0.285600  [25632/42552]\n",
      "loss: 0.240794  [28832/42552]\n",
      "loss: 0.249573  [32032/42552]\n",
      "loss: 0.266272  [35232/42552]\n",
      "loss: 0.149027  [38432/42552]\n",
      "loss: 0.240138  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.250836 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.295768  [   32/42552]\n",
      "loss: 0.285383  [ 3232/42552]\n",
      "loss: 0.243156  [ 6432/42552]\n",
      "loss: 0.241598  [ 9632/42552]\n",
      "loss: 0.315386  [12832/42552]\n",
      "loss: 0.292934  [16032/42552]\n",
      "loss: 0.306746  [19232/42552]\n",
      "loss: 0.324143  [22432/42552]\n",
      "loss: 0.325078  [25632/42552]\n",
      "loss: 0.254084  [28832/42552]\n",
      "loss: 0.411919  [32032/42552]\n",
      "loss: 0.286119  [35232/42552]\n",
      "loss: 0.248807  [38432/42552]\n",
      "loss: 0.239633  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.247745 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.533022  [   32/42552]\n",
      "loss: 0.238043  [ 3232/42552]\n",
      "loss: 0.153796  [ 6432/42552]\n",
      "loss: 0.360174  [ 9632/42552]\n",
      "loss: 0.323105  [12832/42552]\n",
      "loss: 0.270126  [16032/42552]\n",
      "loss: 0.271289  [19232/42552]\n",
      "loss: 0.207886  [22432/42552]\n",
      "loss: 0.185349  [25632/42552]\n",
      "loss: 0.378455  [28832/42552]\n",
      "loss: 0.265399  [32032/42552]\n",
      "loss: 0.246157  [35232/42552]\n",
      "loss: 0.262459  [38432/42552]\n",
      "loss: 0.283178  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.249843 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.371497  [   32/42552]\n",
      "loss: 0.310439  [ 3232/42552]\n",
      "loss: 0.412396  [ 6432/42552]\n",
      "loss: 0.345953  [ 9632/42552]\n",
      "loss: 0.447573  [12832/42552]\n",
      "loss: 0.358075  [16032/42552]\n",
      "loss: 0.213166  [19232/42552]\n",
      "loss: 0.331779  [22432/42552]\n",
      "loss: 0.320117  [25632/42552]\n",
      "loss: 0.458297  [28832/42552]\n",
      "loss: 0.224450  [32032/42552]\n",
      "loss: 0.302429  [35232/42552]\n",
      "loss: 0.198379  [38432/42552]\n",
      "loss: 0.264445  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.248622 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.242835  [   32/42552]\n",
      "loss: 0.148328  [ 3232/42552]\n",
      "loss: 0.463041  [ 6432/42552]\n",
      "loss: 0.377559  [ 9632/42552]\n",
      "loss: 0.278528  [12832/42552]\n",
      "loss: 0.277624  [16032/42552]\n",
      "loss: 0.203040  [19232/42552]\n",
      "loss: 0.297829  [22432/42552]\n",
      "loss: 0.357319  [25632/42552]\n",
      "loss: 0.304586  [28832/42552]\n",
      "loss: 0.385438  [32032/42552]\n",
      "loss: 0.194052  [35232/42552]\n",
      "loss: 0.213104  [38432/42552]\n",
      "loss: 0.311401  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.261781 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.443571  [   32/42552]\n",
      "loss: 0.379844  [ 3232/42552]\n",
      "loss: 0.216279  [ 6432/42552]\n",
      "loss: 0.199440  [ 9632/42552]\n",
      "loss: 0.281059  [12832/42552]\n",
      "loss: 0.209579  [16032/42552]\n",
      "loss: 0.319223  [19232/42552]\n",
      "loss: 0.165712  [22432/42552]\n",
      "loss: 0.322120  [25632/42552]\n",
      "loss: 0.215434  [28832/42552]\n",
      "loss: 0.204453  [32032/42552]\n",
      "loss: 0.289574  [35232/42552]\n",
      "loss: 0.289980  [38432/42552]\n",
      "loss: 0.379450  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.254994 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.244721  [   32/42552]\n",
      "loss: 0.380224  [ 3232/42552]\n",
      "loss: 0.253844  [ 6432/42552]\n",
      "loss: 0.279459  [ 9632/42552]\n",
      "loss: 0.312676  [12832/42552]\n",
      "loss: 0.357720  [16032/42552]\n",
      "loss: 0.311290  [19232/42552]\n",
      "loss: 0.239705  [22432/42552]\n",
      "loss: 0.394481  [25632/42552]\n",
      "loss: 0.266090  [28832/42552]\n",
      "loss: 0.205152  [32032/42552]\n",
      "loss: 0.342097  [35232/42552]\n",
      "loss: 0.289705  [38432/42552]\n",
      "loss: 0.219138  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.245624 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.219917  [   32/42552]\n",
      "loss: 0.362807  [ 3232/42552]\n",
      "loss: 0.189217  [ 6432/42552]\n",
      "loss: 0.425711  [ 9632/42552]\n",
      "loss: 0.290670  [12832/42552]\n",
      "loss: 0.240566  [16032/42552]\n",
      "loss: 0.339998  [19232/42552]\n",
      "loss: 0.433534  [22432/42552]\n",
      "loss: 0.410453  [25632/42552]\n",
      "loss: 0.367336  [28832/42552]\n",
      "loss: 0.144121  [32032/42552]\n",
      "loss: 0.206452  [35232/42552]\n",
      "loss: 0.214041  [38432/42552]\n",
      "loss: 0.222020  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.247830 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.243307  [   32/42552]\n",
      "loss: 0.404103  [ 3232/42552]\n",
      "loss: 0.112254  [ 6432/42552]\n",
      "loss: 0.263900  [ 9632/42552]\n",
      "loss: 0.389865  [12832/42552]\n",
      "loss: 0.218931  [16032/42552]\n",
      "loss: 0.248686  [19232/42552]\n",
      "loss: 0.317318  [22432/42552]\n",
      "loss: 0.137179  [25632/42552]\n",
      "loss: 0.178575  [28832/42552]\n",
      "loss: 0.228377  [32032/42552]\n",
      "loss: 0.226061  [35232/42552]\n",
      "loss: 0.253504  [38432/42552]\n",
      "loss: 0.277716  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.251492 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.428642  [   32/42552]\n",
      "loss: 0.283793  [ 3232/42552]\n",
      "loss: 0.254607  [ 6432/42552]\n",
      "loss: 0.280307  [ 9632/42552]\n",
      "loss: 0.445123  [12832/42552]\n",
      "loss: 0.166546  [16032/42552]\n",
      "loss: 0.464308  [19232/42552]\n",
      "loss: 0.209470  [22432/42552]\n",
      "loss: 0.281364  [25632/42552]\n",
      "loss: 0.327096  [28832/42552]\n",
      "loss: 0.401078  [32032/42552]\n",
      "loss: 0.518762  [35232/42552]\n",
      "loss: 0.320409  [38432/42552]\n",
      "loss: 0.364713  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.249266 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.201243  [   32/42552]\n",
      "loss: 0.358399  [ 3232/42552]\n",
      "loss: 0.196468  [ 6432/42552]\n",
      "loss: 0.376883  [ 9632/42552]\n",
      "loss: 0.330966  [12832/42552]\n",
      "loss: 0.256765  [16032/42552]\n",
      "loss: 0.227859  [19232/42552]\n",
      "loss: 0.381023  [22432/42552]\n",
      "loss: 0.373958  [25632/42552]\n",
      "loss: 0.287289  [28832/42552]\n",
      "loss: 0.295587  [32032/42552]\n",
      "loss: 0.261313  [35232/42552]\n",
      "loss: 0.304218  [38432/42552]\n",
      "loss: 0.374598  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.244559 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.224459  [   32/42552]\n",
      "loss: 0.357057  [ 3232/42552]\n",
      "loss: 0.304838  [ 6432/42552]\n",
      "loss: 0.314011  [ 9632/42552]\n",
      "loss: 0.335467  [12832/42552]\n",
      "loss: 0.301375  [16032/42552]\n",
      "loss: 0.190335  [19232/42552]\n",
      "loss: 0.288266  [22432/42552]\n",
      "loss: 0.212078  [25632/42552]\n",
      "loss: 0.209964  [28832/42552]\n",
      "loss: 0.250700  [32032/42552]\n",
      "loss: 0.302671  [35232/42552]\n",
      "loss: 0.313195  [38432/42552]\n",
      "loss: 0.386106  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.252389 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.306266  [   32/42552]\n",
      "loss: 0.364207  [ 3232/42552]\n",
      "loss: 0.244997  [ 6432/42552]\n",
      "loss: 0.300589  [ 9632/42552]\n",
      "loss: 0.374213  [12832/42552]\n",
      "loss: 0.348656  [16032/42552]\n",
      "loss: 0.247652  [19232/42552]\n",
      "loss: 0.388835  [22432/42552]\n",
      "loss: 0.230363  [25632/42552]\n",
      "loss: 0.214726  [28832/42552]\n",
      "loss: 0.357814  [32032/42552]\n",
      "loss: 0.407888  [35232/42552]\n",
      "loss: 0.423294  [38432/42552]\n",
      "loss: 0.395305  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.253272 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.279027  [   32/42552]\n",
      "loss: 0.271582  [ 3232/42552]\n",
      "loss: 0.268822  [ 6432/42552]\n",
      "loss: 0.299794  [ 9632/42552]\n",
      "loss: 0.418948  [12832/42552]\n",
      "loss: 0.338906  [16032/42552]\n",
      "loss: 0.210599  [19232/42552]\n",
      "loss: 0.470497  [22432/42552]\n",
      "loss: 0.424379  [25632/42552]\n",
      "loss: 0.459725  [28832/42552]\n",
      "loss: 0.234332  [32032/42552]\n",
      "loss: 0.238360  [35232/42552]\n",
      "loss: 0.468395  [38432/42552]\n",
      "loss: 0.341768  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.248862 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.284793  [   32/42552]\n",
      "loss: 0.292655  [ 3232/42552]\n",
      "loss: 0.299843  [ 6432/42552]\n",
      "loss: 0.222920  [ 9632/42552]\n",
      "loss: 0.175714  [12832/42552]\n",
      "loss: 0.361517  [16032/42552]\n",
      "loss: 0.205998  [19232/42552]\n",
      "loss: 0.267725  [22432/42552]\n",
      "loss: 0.196401  [25632/42552]\n",
      "loss: 0.449398  [28832/42552]\n",
      "loss: 0.282452  [32032/42552]\n",
      "loss: 0.315629  [35232/42552]\n",
      "loss: 0.200604  [38432/42552]\n",
      "loss: 0.323266  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.248205 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.309294  [   32/42552]\n",
      "loss: 0.359105  [ 3232/42552]\n",
      "loss: 0.186196  [ 6432/42552]\n",
      "loss: 0.384546  [ 9632/42552]\n",
      "loss: 0.341050  [12832/42552]\n",
      "loss: 0.258854  [16032/42552]\n",
      "loss: 0.256682  [19232/42552]\n",
      "loss: 0.306835  [22432/42552]\n",
      "loss: 0.122325  [25632/42552]\n",
      "loss: 0.371211  [28832/42552]\n",
      "loss: 0.135016  [32032/42552]\n",
      "loss: 0.212750  [35232/42552]\n",
      "loss: 0.330646  [38432/42552]\n",
      "loss: 0.226403  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.248460 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.352742  [   32/42552]\n",
      "loss: 0.212065  [ 3232/42552]\n",
      "loss: 0.517740  [ 6432/42552]\n",
      "loss: 0.145149  [ 9632/42552]\n",
      "loss: 0.421398  [12832/42552]\n",
      "loss: 0.218706  [16032/42552]\n",
      "loss: 0.202920  [19232/42552]\n",
      "loss: 0.237515  [22432/42552]\n",
      "loss: 0.246158  [25632/42552]\n",
      "loss: 0.330985  [28832/42552]\n",
      "loss: 0.267953  [32032/42552]\n",
      "loss: 0.249186  [35232/42552]\n",
      "loss: 0.347031  [38432/42552]\n",
      "loss: 0.314139  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.247753 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.260770  [   32/42552]\n",
      "loss: 0.299119  [ 3232/42552]\n",
      "loss: 0.448571  [ 6432/42552]\n",
      "loss: 0.252336  [ 9632/42552]\n",
      "loss: 0.398751  [12832/42552]\n",
      "loss: 0.213232  [16032/42552]\n",
      "loss: 0.273198  [19232/42552]\n",
      "loss: 0.290733  [22432/42552]\n",
      "loss: 0.409178  [25632/42552]\n",
      "loss: 0.399918  [28832/42552]\n",
      "loss: 0.229662  [32032/42552]\n",
      "loss: 0.176297  [35232/42552]\n",
      "loss: 0.218293  [38432/42552]\n",
      "loss: 0.299758  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.260309 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.279588  [   32/42552]\n",
      "loss: 0.394636  [ 3232/42552]\n",
      "loss: 0.395133  [ 6432/42552]\n",
      "loss: 0.259492  [ 9632/42552]\n",
      "loss: 0.183384  [12832/42552]\n",
      "loss: 0.305458  [16032/42552]\n",
      "loss: 0.301549  [19232/42552]\n",
      "loss: 0.231415  [22432/42552]\n",
      "loss: 0.228555  [25632/42552]\n",
      "loss: 0.325867  [28832/42552]\n",
      "loss: 0.296266  [32032/42552]\n",
      "loss: 0.235673  [35232/42552]\n",
      "loss: 0.237541  [38432/42552]\n",
      "loss: 0.461110  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.245945 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.173969  [   32/42552]\n",
      "loss: 0.419990  [ 3232/42552]\n",
      "loss: 0.176080  [ 6432/42552]\n",
      "loss: 0.316192  [ 9632/42552]\n",
      "loss: 0.220051  [12832/42552]\n",
      "loss: 0.249669  [16032/42552]\n",
      "loss: 0.308811  [19232/42552]\n",
      "loss: 0.429053  [22432/42552]\n",
      "loss: 0.256177  [25632/42552]\n",
      "loss: 0.220040  [28832/42552]\n",
      "loss: 0.305645  [32032/42552]\n",
      "loss: 0.260538  [35232/42552]\n",
      "loss: 0.176028  [38432/42552]\n",
      "loss: 0.194163  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.247644 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.188421  [   32/42552]\n",
      "loss: 0.247394  [ 3232/42552]\n",
      "loss: 0.278516  [ 6432/42552]\n",
      "loss: 0.325678  [ 9632/42552]\n",
      "loss: 0.234934  [12832/42552]\n",
      "loss: 0.285308  [16032/42552]\n",
      "loss: 0.202804  [19232/42552]\n",
      "loss: 0.216361  [22432/42552]\n",
      "loss: 0.161714  [25632/42552]\n",
      "loss: 0.254965  [28832/42552]\n",
      "loss: 0.236897  [32032/42552]\n",
      "loss: 0.264097  [35232/42552]\n",
      "loss: 0.285762  [38432/42552]\n",
      "loss: 0.357824  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.254083 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.329546  [   32/42552]\n",
      "loss: 0.291127  [ 3232/42552]\n",
      "loss: 0.163410  [ 6432/42552]\n",
      "loss: 0.190561  [ 9632/42552]\n",
      "loss: 0.347219  [12832/42552]\n",
      "loss: 0.157379  [16032/42552]\n",
      "loss: 0.201036  [19232/42552]\n",
      "loss: 0.286238  [22432/42552]\n",
      "loss: 0.260888  [25632/42552]\n",
      "loss: 0.328935  [28832/42552]\n",
      "loss: 0.234649  [32032/42552]\n",
      "loss: 0.548479  [35232/42552]\n",
      "loss: 0.193516  [38432/42552]\n",
      "loss: 0.195754  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.249387 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.252116  [   32/42552]\n",
      "loss: 0.235161  [ 3232/42552]\n",
      "loss: 0.363587  [ 6432/42552]\n",
      "loss: 0.275753  [ 9632/42552]\n",
      "loss: 0.200061  [12832/42552]\n",
      "loss: 0.196002  [16032/42552]\n",
      "loss: 0.364401  [19232/42552]\n",
      "loss: 0.150210  [22432/42552]\n",
      "loss: 0.306559  [25632/42552]\n",
      "loss: 0.228721  [28832/42552]\n",
      "loss: 0.324220  [32032/42552]\n",
      "loss: 0.157964  [35232/42552]\n",
      "loss: 0.266793  [38432/42552]\n",
      "loss: 0.371088  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.248143 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.378874  [   32/42552]\n",
      "loss: 0.175414  [ 3232/42552]\n",
      "loss: 0.289525  [ 6432/42552]\n",
      "loss: 0.319749  [ 9632/42552]\n",
      "loss: 0.238065  [12832/42552]\n",
      "loss: 0.266559  [16032/42552]\n",
      "loss: 0.336656  [19232/42552]\n",
      "loss: 0.296871  [22432/42552]\n",
      "loss: 0.197012  [25632/42552]\n",
      "loss: 0.450715  [28832/42552]\n",
      "loss: 0.251679  [32032/42552]\n",
      "loss: 0.276115  [35232/42552]\n",
      "loss: 0.291549  [38432/42552]\n",
      "loss: 0.312502  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.261706 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.305588  [   32/42552]\n",
      "loss: 0.251078  [ 3232/42552]\n",
      "loss: 0.177942  [ 6432/42552]\n",
      "loss: 0.366372  [ 9632/42552]\n",
      "loss: 0.251033  [12832/42552]\n",
      "loss: 0.181512  [16032/42552]\n",
      "loss: 0.467999  [19232/42552]\n",
      "loss: 0.225295  [22432/42552]\n",
      "loss: 0.256199  [25632/42552]\n",
      "loss: 0.291471  [28832/42552]\n",
      "loss: 0.273510  [32032/42552]\n",
      "loss: 0.236371  [35232/42552]\n",
      "loss: 0.290444  [38432/42552]\n",
      "loss: 0.249401  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.245866 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.286707  [   32/42552]\n",
      "loss: 0.335151  [ 3232/42552]\n",
      "loss: 0.254945  [ 6432/42552]\n",
      "loss: 0.244853  [ 9632/42552]\n",
      "loss: 0.267204  [12832/42552]\n",
      "loss: 0.212296  [16032/42552]\n",
      "loss: 0.171465  [19232/42552]\n",
      "loss: 0.167829  [22432/42552]\n",
      "loss: 0.182893  [25632/42552]\n",
      "loss: 0.200236  [28832/42552]\n",
      "loss: 0.269847  [32032/42552]\n",
      "loss: 0.340554  [35232/42552]\n",
      "loss: 0.186185  [38432/42552]\n",
      "loss: 0.270949  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.247880 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.185807  [   32/42552]\n",
      "loss: 0.336205  [ 3232/42552]\n",
      "loss: 0.290027  [ 6432/42552]\n",
      "loss: 0.296814  [ 9632/42552]\n",
      "loss: 0.387108  [12832/42552]\n",
      "loss: 0.176765  [16032/42552]\n",
      "loss: 0.265511  [19232/42552]\n",
      "loss: 0.174126  [22432/42552]\n",
      "loss: 0.255767  [25632/42552]\n",
      "loss: 0.388060  [28832/42552]\n",
      "loss: 0.276375  [32032/42552]\n",
      "loss: 0.624712  [35232/42552]\n",
      "loss: 0.336296  [38432/42552]\n",
      "loss: 0.357991  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.249728 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.180411  [   32/42552]\n",
      "loss: 0.260168  [ 3232/42552]\n",
      "loss: 0.265666  [ 6432/42552]\n",
      "loss: 0.402619  [ 9632/42552]\n",
      "loss: 0.203940  [12832/42552]\n",
      "loss: 0.144114  [16032/42552]\n",
      "loss: 0.241287  [19232/42552]\n",
      "loss: 0.274540  [22432/42552]\n",
      "loss: 0.285118  [25632/42552]\n",
      "loss: 0.326010  [28832/42552]\n",
      "loss: 0.370496  [32032/42552]\n",
      "loss: 0.300786  [35232/42552]\n",
      "loss: 0.322464  [38432/42552]\n",
      "loss: 0.286802  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.243578 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.352473  [   32/42552]\n",
      "loss: 0.337404  [ 3232/42552]\n",
      "loss: 0.183281  [ 6432/42552]\n",
      "loss: 0.449675  [ 9632/42552]\n",
      "loss: 0.221764  [12832/42552]\n",
      "loss: 0.726695  [16032/42552]\n",
      "loss: 0.309186  [19232/42552]\n",
      "loss: 0.174279  [22432/42552]\n",
      "loss: 0.296832  [25632/42552]\n",
      "loss: 0.237423  [28832/42552]\n",
      "loss: 0.251814  [32032/42552]\n",
      "loss: 0.329558  [35232/42552]\n",
      "loss: 0.379752  [38432/42552]\n",
      "loss: 0.282121  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.238965 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.223820  [   32/42552]\n",
      "loss: 0.315306  [ 3232/42552]\n",
      "loss: 0.189485  [ 6432/42552]\n",
      "loss: 0.432345  [ 9632/42552]\n",
      "loss: 0.262206  [12832/42552]\n",
      "loss: 0.364522  [16032/42552]\n",
      "loss: 0.299373  [19232/42552]\n",
      "loss: 0.443885  [22432/42552]\n",
      "loss: 0.211780  [25632/42552]\n",
      "loss: 0.352948  [28832/42552]\n",
      "loss: 0.329874  [32032/42552]\n",
      "loss: 0.311255  [35232/42552]\n",
      "loss: 0.171873  [38432/42552]\n",
      "loss: 0.298860  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.244173 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.278685  [   32/42552]\n",
      "loss: 0.373151  [ 3232/42552]\n",
      "loss: 0.338300  [ 6432/42552]\n",
      "loss: 0.349568  [ 9632/42552]\n",
      "loss: 0.214632  [12832/42552]\n",
      "loss: 0.364896  [16032/42552]\n",
      "loss: 0.278574  [19232/42552]\n",
      "loss: 0.400722  [22432/42552]\n",
      "loss: 0.417535  [25632/42552]\n",
      "loss: 0.416863  [28832/42552]\n",
      "loss: 0.354454  [32032/42552]\n",
      "loss: 0.264867  [35232/42552]\n",
      "loss: 0.299376  [38432/42552]\n",
      "loss: 0.210754  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.244215 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.260252  [   32/42552]\n",
      "loss: 0.171166  [ 3232/42552]\n",
      "loss: 0.316095  [ 6432/42552]\n",
      "loss: 0.277989  [ 9632/42552]\n",
      "loss: 0.194941  [12832/42552]\n",
      "loss: 0.245078  [16032/42552]\n",
      "loss: 0.346349  [19232/42552]\n",
      "loss: 0.192343  [22432/42552]\n",
      "loss: 0.293942  [25632/42552]\n",
      "loss: 0.238296  [28832/42552]\n",
      "loss: 0.312463  [32032/42552]\n",
      "loss: 0.235385  [35232/42552]\n",
      "loss: 0.237158  [38432/42552]\n",
      "loss: 0.313898  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.246114 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.197479  [   32/42552]\n",
      "loss: 0.250333  [ 3232/42552]\n",
      "loss: 0.286547  [ 6432/42552]\n",
      "loss: 0.193123  [ 9632/42552]\n",
      "loss: 0.241088  [12832/42552]\n",
      "loss: 0.313592  [16032/42552]\n",
      "loss: 0.369220  [19232/42552]\n",
      "loss: 0.341696  [22432/42552]\n",
      "loss: 0.424069  [25632/42552]\n",
      "loss: 0.227345  [28832/42552]\n",
      "loss: 0.136995  [32032/42552]\n",
      "loss: 0.259588  [35232/42552]\n",
      "loss: 0.238464  [38432/42552]\n",
      "loss: 0.298505  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.246373 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.196595  [   32/42552]\n",
      "loss: 0.348950  [ 3232/42552]\n",
      "loss: 0.405579  [ 6432/42552]\n",
      "loss: 0.410635  [ 9632/42552]\n",
      "loss: 0.190659  [12832/42552]\n",
      "loss: 0.240024  [16032/42552]\n",
      "loss: 0.329294  [19232/42552]\n",
      "loss: 0.437233  [22432/42552]\n",
      "loss: 0.161645  [25632/42552]\n",
      "loss: 0.304014  [28832/42552]\n",
      "loss: 0.195810  [32032/42552]\n",
      "loss: 0.442785  [35232/42552]\n",
      "loss: 0.269162  [38432/42552]\n",
      "loss: 0.356319  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.249834 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.280156  [   32/42552]\n",
      "loss: 0.278609  [ 3232/42552]\n",
      "loss: 0.246793  [ 6432/42552]\n",
      "loss: 0.222633  [ 9632/42552]\n",
      "loss: 0.352493  [12832/42552]\n",
      "loss: 0.172031  [16032/42552]\n",
      "loss: 0.201810  [19232/42552]\n",
      "loss: 0.260278  [22432/42552]\n",
      "loss: 0.318632  [25632/42552]\n",
      "loss: 0.417303  [28832/42552]\n",
      "loss: 0.312398  [32032/42552]\n",
      "loss: 0.243370  [35232/42552]\n",
      "loss: 0.353964  [38432/42552]\n",
      "loss: 0.297579  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.248066 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.204990  [   32/42552]\n",
      "loss: 0.171076  [ 3232/42552]\n",
      "loss: 0.352316  [ 6432/42552]\n",
      "loss: 0.227652  [ 9632/42552]\n",
      "loss: 0.230125  [12832/42552]\n",
      "loss: 0.421480  [16032/42552]\n",
      "loss: 0.255814  [19232/42552]\n",
      "loss: 0.158663  [22432/42552]\n",
      "loss: 0.316928  [25632/42552]\n",
      "loss: 0.292729  [28832/42552]\n",
      "loss: 0.276794  [32032/42552]\n",
      "loss: 0.296089  [35232/42552]\n",
      "loss: 0.283478  [38432/42552]\n",
      "loss: 0.374376  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.258558 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.272824  [   32/42552]\n",
      "loss: 0.277840  [ 3232/42552]\n",
      "loss: 0.389980  [ 6432/42552]\n",
      "loss: 0.431909  [ 9632/42552]\n",
      "loss: 0.244140  [12832/42552]\n",
      "loss: 0.327912  [16032/42552]\n",
      "loss: 0.325032  [19232/42552]\n",
      "loss: 0.386305  [22432/42552]\n",
      "loss: 0.331889  [25632/42552]\n",
      "loss: 0.245984  [28832/42552]\n",
      "loss: 0.221798  [32032/42552]\n",
      "loss: 0.372733  [35232/42552]\n",
      "loss: 0.353420  [38432/42552]\n",
      "loss: 0.135462  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.248836 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.392180  [   32/42552]\n",
      "loss: 0.258129  [ 3232/42552]\n",
      "loss: 0.252879  [ 6432/42552]\n",
      "loss: 0.435329  [ 9632/42552]\n",
      "loss: 0.342488  [12832/42552]\n",
      "loss: 0.207917  [16032/42552]\n",
      "loss: 0.239931  [19232/42552]\n",
      "loss: 0.330194  [22432/42552]\n",
      "loss: 0.178310  [25632/42552]\n",
      "loss: 0.347305  [28832/42552]\n",
      "loss: 0.232924  [32032/42552]\n",
      "loss: 0.328434  [35232/42552]\n",
      "loss: 0.291020  [38432/42552]\n",
      "loss: 0.407357  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.249761 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.316047  [   32/42552]\n",
      "loss: 0.276779  [ 3232/42552]\n",
      "loss: 0.244443  [ 6432/42552]\n",
      "loss: 0.255402  [ 9632/42552]\n",
      "loss: 0.286132  [12832/42552]\n",
      "loss: 0.240131  [16032/42552]\n",
      "loss: 0.359415  [19232/42552]\n",
      "loss: 0.379528  [22432/42552]\n",
      "loss: 0.281206  [25632/42552]\n",
      "loss: 0.245124  [28832/42552]\n",
      "loss: 0.383260  [32032/42552]\n",
      "loss: 0.361342  [35232/42552]\n",
      "loss: 0.232906  [38432/42552]\n",
      "loss: 0.245219  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.256022 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.285224  [   32/42552]\n",
      "loss: 0.419645  [ 3232/42552]\n",
      "loss: 0.374744  [ 6432/42552]\n",
      "loss: 0.404837  [ 9632/42552]\n",
      "loss: 0.255619  [12832/42552]\n",
      "loss: 0.339040  [16032/42552]\n",
      "loss: 0.239200  [19232/42552]\n",
      "loss: 0.255387  [22432/42552]\n",
      "loss: 0.213995  [25632/42552]\n",
      "loss: 0.413291  [28832/42552]\n",
      "loss: 0.229529  [32032/42552]\n",
      "loss: 0.339982  [35232/42552]\n",
      "loss: 0.250400  [38432/42552]\n",
      "loss: 0.297450  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.252595 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.377231  [   32/42552]\n",
      "loss: 0.403656  [ 3232/42552]\n",
      "loss: 0.369413  [ 6432/42552]\n",
      "loss: 0.413319  [ 9632/42552]\n",
      "loss: 0.266468  [12832/42552]\n",
      "loss: 0.516858  [16032/42552]\n",
      "loss: 0.299360  [19232/42552]\n",
      "loss: 0.315062  [22432/42552]\n",
      "loss: 0.318685  [25632/42552]\n",
      "loss: 0.226705  [28832/42552]\n",
      "loss: 0.205053  [32032/42552]\n",
      "loss: 0.329781  [35232/42552]\n",
      "loss: 0.207738  [38432/42552]\n",
      "loss: 0.304454  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.248762 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.243437  [   32/42552]\n",
      "loss: 0.239103  [ 3232/42552]\n",
      "loss: 0.431880  [ 6432/42552]\n",
      "loss: 0.372958  [ 9632/42552]\n",
      "loss: 0.406894  [12832/42552]\n",
      "loss: 0.232591  [16032/42552]\n",
      "loss: 0.215212  [19232/42552]\n",
      "loss: 0.226652  [22432/42552]\n",
      "loss: 0.399872  [25632/42552]\n",
      "loss: 0.281316  [28832/42552]\n",
      "loss: 0.179945  [32032/42552]\n",
      "loss: 0.327171  [35232/42552]\n",
      "loss: 0.302450  [38432/42552]\n",
      "loss: 0.257324  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.249439 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.234546  [   32/42552]\n",
      "loss: 0.184580  [ 3232/42552]\n",
      "loss: 0.261590  [ 6432/42552]\n",
      "loss: 0.277883  [ 9632/42552]\n",
      "loss: 0.281354  [12832/42552]\n",
      "loss: 0.208759  [16032/42552]\n",
      "loss: 0.239361  [19232/42552]\n",
      "loss: 0.266222  [22432/42552]\n",
      "loss: 0.359165  [25632/42552]\n",
      "loss: 0.453315  [28832/42552]\n",
      "loss: 0.218527  [32032/42552]\n",
      "loss: 0.391136  [35232/42552]\n",
      "loss: 0.320775  [38432/42552]\n",
      "loss: 0.378519  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.245673 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.251230  [   32/42552]\n",
      "loss: 0.313446  [ 3232/42552]\n",
      "loss: 0.302418  [ 6432/42552]\n",
      "loss: 0.360486  [ 9632/42552]\n",
      "loss: 0.369057  [12832/42552]\n",
      "loss: 0.315470  [16032/42552]\n",
      "loss: 0.310979  [19232/42552]\n",
      "loss: 0.308315  [22432/42552]\n",
      "loss: 0.211696  [25632/42552]\n",
      "loss: 0.382254  [28832/42552]\n",
      "loss: 0.329196  [32032/42552]\n",
      "loss: 0.236009  [35232/42552]\n",
      "loss: 0.306203  [38432/42552]\n",
      "loss: 0.227380  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.245040 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.357266  [   32/42552]\n",
      "loss: 0.245563  [ 3232/42552]\n",
      "loss: 0.224290  [ 6432/42552]\n",
      "loss: 0.192229  [ 9632/42552]\n",
      "loss: 0.320907  [12832/42552]\n",
      "loss: 0.242449  [16032/42552]\n",
      "loss: 0.429293  [19232/42552]\n",
      "loss: 0.167404  [22432/42552]\n",
      "loss: 0.233259  [25632/42552]\n",
      "loss: 0.281365  [28832/42552]\n",
      "loss: 0.270844  [32032/42552]\n",
      "loss: 0.227948  [35232/42552]\n",
      "loss: 0.383909  [38432/42552]\n",
      "loss: 0.379516  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.241520 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.306245  [   32/42552]\n",
      "loss: 0.313154  [ 3232/42552]\n",
      "loss: 0.189487  [ 6432/42552]\n",
      "loss: 0.285139  [ 9632/42552]\n",
      "loss: 0.358308  [12832/42552]\n",
      "loss: 0.304349  [16032/42552]\n",
      "loss: 0.195723  [19232/42552]\n",
      "loss: 0.223269  [22432/42552]\n",
      "loss: 0.327846  [25632/42552]\n",
      "loss: 0.203182  [28832/42552]\n",
      "loss: 0.291341  [32032/42552]\n",
      "loss: 0.288980  [35232/42552]\n",
      "loss: 0.375864  [38432/42552]\n",
      "loss: 0.270231  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.243219 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.189002  [   32/42552]\n",
      "loss: 0.256607  [ 3232/42552]\n",
      "loss: 0.308607  [ 6432/42552]\n",
      "loss: 0.226298  [ 9632/42552]\n",
      "loss: 0.230505  [12832/42552]\n",
      "loss: 0.302792  [16032/42552]\n",
      "loss: 0.232843  [19232/42552]\n",
      "loss: 0.306787  [22432/42552]\n",
      "loss: 0.209651  [25632/42552]\n",
      "loss: 0.332324  [28832/42552]\n",
      "loss: 0.435913  [32032/42552]\n",
      "loss: 0.261488  [35232/42552]\n",
      "loss: 0.343644  [38432/42552]\n",
      "loss: 0.364030  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.244727 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.191492  [   32/42552]\n",
      "loss: 0.226822  [ 3232/42552]\n",
      "loss: 0.168128  [ 6432/42552]\n",
      "loss: 0.255206  [ 9632/42552]\n",
      "loss: 0.253070  [12832/42552]\n",
      "loss: 0.367599  [16032/42552]\n",
      "loss: 0.221050  [19232/42552]\n",
      "loss: 0.318214  [22432/42552]\n",
      "loss: 0.374487  [25632/42552]\n",
      "loss: 0.208577  [28832/42552]\n",
      "loss: 0.263306  [32032/42552]\n",
      "loss: 0.195994  [35232/42552]\n",
      "loss: 0.216838  [38432/42552]\n",
      "loss: 0.248247  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.259137 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.304596  [   32/42552]\n",
      "loss: 0.225618  [ 3232/42552]\n",
      "loss: 0.307746  [ 6432/42552]\n",
      "loss: 0.327629  [ 9632/42552]\n",
      "loss: 0.322414  [12832/42552]\n",
      "loss: 0.329236  [16032/42552]\n",
      "loss: 0.263614  [19232/42552]\n",
      "loss: 0.335008  [22432/42552]\n",
      "loss: 0.331377  [25632/42552]\n",
      "loss: 0.248093  [28832/42552]\n",
      "loss: 0.262580  [32032/42552]\n",
      "loss: 0.211454  [35232/42552]\n",
      "loss: 0.198874  [38432/42552]\n",
      "loss: 0.228713  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.244001 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.367660  [   32/42552]\n",
      "loss: 0.221221  [ 3232/42552]\n",
      "loss: 0.170947  [ 6432/42552]\n",
      "loss: 0.372717  [ 9632/42552]\n",
      "loss: 0.208834  [12832/42552]\n",
      "loss: 0.292782  [16032/42552]\n",
      "loss: 0.275236  [19232/42552]\n",
      "loss: 0.341880  [22432/42552]\n",
      "loss: 0.433732  [25632/42552]\n",
      "loss: 0.303339  [28832/42552]\n",
      "loss: 0.207151  [32032/42552]\n",
      "loss: 0.348688  [35232/42552]\n",
      "loss: 0.276538  [38432/42552]\n",
      "loss: 0.232424  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.244302 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.253726  [   32/42552]\n",
      "loss: 0.246019  [ 3232/42552]\n",
      "loss: 0.240758  [ 6432/42552]\n",
      "loss: 0.411471  [ 9632/42552]\n",
      "loss: 0.275460  [12832/42552]\n",
      "loss: 0.313635  [16032/42552]\n",
      "loss: 0.246680  [19232/42552]\n",
      "loss: 0.160486  [22432/42552]\n",
      "loss: 0.263282  [25632/42552]\n",
      "loss: 0.292603  [28832/42552]\n",
      "loss: 0.173174  [32032/42552]\n",
      "loss: 0.258158  [35232/42552]\n",
      "loss: 0.282437  [38432/42552]\n",
      "loss: 0.276682  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.246923 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.298731  [   32/42552]\n",
      "loss: 0.234490  [ 3232/42552]\n",
      "loss: 0.340411  [ 6432/42552]\n",
      "loss: 0.269304  [ 9632/42552]\n",
      "loss: 0.199465  [12832/42552]\n",
      "loss: 0.207517  [16032/42552]\n",
      "loss: 0.221353  [19232/42552]\n",
      "loss: 0.280042  [22432/42552]\n",
      "loss: 0.354698  [25632/42552]\n",
      "loss: 0.276802  [28832/42552]\n",
      "loss: 0.306943  [32032/42552]\n",
      "loss: 0.198715  [35232/42552]\n",
      "loss: 0.314372  [38432/42552]\n",
      "loss: 0.244716  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.245257 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.239294  [   32/42552]\n",
      "loss: 0.273372  [ 3232/42552]\n",
      "loss: 0.270238  [ 6432/42552]\n",
      "loss: 0.290742  [ 9632/42552]\n",
      "loss: 0.192197  [12832/42552]\n",
      "loss: 0.342408  [16032/42552]\n",
      "loss: 0.347491  [19232/42552]\n",
      "loss: 0.349556  [22432/42552]\n",
      "loss: 0.299388  [25632/42552]\n",
      "loss: 0.227812  [28832/42552]\n",
      "loss: 0.245780  [32032/42552]\n",
      "loss: 0.318998  [35232/42552]\n",
      "loss: 0.237869  [38432/42552]\n",
      "loss: 0.352730  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.243622 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.309000  [   32/42552]\n",
      "loss: 0.474923  [ 3232/42552]\n",
      "loss: 0.234473  [ 6432/42552]\n",
      "loss: 0.208791  [ 9632/42552]\n",
      "loss: 0.235122  [12832/42552]\n",
      "loss: 0.233357  [16032/42552]\n",
      "loss: 0.312613  [19232/42552]\n",
      "loss: 0.261413  [22432/42552]\n",
      "loss: 0.249024  [25632/42552]\n",
      "loss: 0.200185  [28832/42552]\n",
      "loss: 0.290914  [32032/42552]\n",
      "loss: 0.248565  [35232/42552]\n",
      "loss: 0.263322  [38432/42552]\n",
      "loss: 0.323460  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.249915 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.223785  [   32/42552]\n",
      "loss: 0.338930  [ 3232/42552]\n",
      "loss: 0.237453  [ 6432/42552]\n",
      "loss: 0.156939  [ 9632/42552]\n",
      "loss: 0.232735  [12832/42552]\n",
      "loss: 0.515136  [16032/42552]\n",
      "loss: 0.225359  [19232/42552]\n",
      "loss: 0.468227  [22432/42552]\n",
      "loss: 0.286178  [25632/42552]\n",
      "loss: 0.259509  [28832/42552]\n",
      "loss: 0.319449  [32032/42552]\n",
      "loss: 0.261421  [35232/42552]\n",
      "loss: 0.262063  [38432/42552]\n",
      "loss: 0.255567  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.245293 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.297888  [   32/42552]\n",
      "loss: 0.491535  [ 3232/42552]\n",
      "loss: 0.294877  [ 6432/42552]\n",
      "loss: 0.308792  [ 9632/42552]\n",
      "loss: 0.263117  [12832/42552]\n",
      "loss: 0.412875  [16032/42552]\n",
      "loss: 0.209214  [19232/42552]\n",
      "loss: 0.197257  [22432/42552]\n",
      "loss: 0.341494  [25632/42552]\n",
      "loss: 0.320691  [28832/42552]\n",
      "loss: 0.201549  [32032/42552]\n",
      "loss: 0.321187  [35232/42552]\n",
      "loss: 0.209388  [38432/42552]\n",
      "loss: 0.305384  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.249873 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.606427  [   32/42552]\n",
      "loss: 0.437839  [ 3232/42552]\n",
      "loss: 0.397767  [ 6432/42552]\n",
      "loss: 0.277199  [ 9632/42552]\n",
      "loss: 0.432927  [12832/42552]\n",
      "loss: 0.346175  [16032/42552]\n",
      "loss: 0.401567  [19232/42552]\n",
      "loss: 0.245538  [22432/42552]\n",
      "loss: 0.354764  [25632/42552]\n",
      "loss: 0.231854  [28832/42552]\n",
      "loss: 0.213013  [32032/42552]\n",
      "loss: 0.243117  [35232/42552]\n",
      "loss: 0.374027  [38432/42552]\n",
      "loss: 0.261285  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.247321 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.392505  [   32/42552]\n",
      "loss: 0.232404  [ 3232/42552]\n",
      "loss: 0.293406  [ 6432/42552]\n",
      "loss: 0.259699  [ 9632/42552]\n",
      "loss: 0.485817  [12832/42552]\n",
      "loss: 0.256742  [16032/42552]\n",
      "loss: 0.418319  [19232/42552]\n",
      "loss: 0.355065  [22432/42552]\n",
      "loss: 0.225812  [25632/42552]\n",
      "loss: 0.441348  [28832/42552]\n",
      "loss: 0.253445  [32032/42552]\n",
      "loss: 0.316176  [35232/42552]\n",
      "loss: 0.178613  [38432/42552]\n",
      "loss: 0.303277  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.243188 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.311819  [   32/42552]\n",
      "loss: 0.261770  [ 3232/42552]\n",
      "loss: 0.204603  [ 6432/42552]\n",
      "loss: 0.181198  [ 9632/42552]\n",
      "loss: 0.438294  [12832/42552]\n",
      "loss: 0.359011  [16032/42552]\n",
      "loss: 0.327562  [19232/42552]\n",
      "loss: 0.213411  [22432/42552]\n",
      "loss: 0.315596  [25632/42552]\n",
      "loss: 0.556973  [28832/42552]\n",
      "loss: 0.413838  [32032/42552]\n",
      "loss: 0.313130  [35232/42552]\n",
      "loss: 0.364691  [38432/42552]\n",
      "loss: 0.179126  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.244977 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.240155  [   32/42552]\n",
      "loss: 0.206555  [ 3232/42552]\n",
      "loss: 0.267571  [ 6432/42552]\n",
      "loss: 0.200069  [ 9632/42552]\n",
      "loss: 0.369650  [12832/42552]\n",
      "loss: 0.281506  [16032/42552]\n",
      "loss: 0.227653  [19232/42552]\n",
      "loss: 0.272127  [22432/42552]\n",
      "loss: 0.228601  [25632/42552]\n",
      "loss: 0.287761  [28832/42552]\n",
      "loss: 0.321855  [32032/42552]\n",
      "loss: 0.265476  [35232/42552]\n",
      "loss: 0.236636  [38432/42552]\n",
      "loss: 0.253116  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.250125 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.336501  [   32/42552]\n",
      "loss: 0.241984  [ 3232/42552]\n",
      "loss: 0.312398  [ 6432/42552]\n",
      "loss: 0.293902  [ 9632/42552]\n",
      "loss: 0.156610  [12832/42552]\n",
      "loss: 0.203439  [16032/42552]\n",
      "loss: 0.229029  [19232/42552]\n",
      "loss: 0.300487  [22432/42552]\n",
      "loss: 0.376036  [25632/42552]\n",
      "loss: 0.324358  [28832/42552]\n",
      "loss: 0.413390  [32032/42552]\n",
      "loss: 0.242274  [35232/42552]\n",
      "loss: 0.193802  [38432/42552]\n",
      "loss: 0.171927  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.246441 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.376626  [   32/42552]\n",
      "loss: 0.219984  [ 3232/42552]\n",
      "loss: 0.275033  [ 6432/42552]\n",
      "loss: 0.237503  [ 9632/42552]\n",
      "loss: 0.247398  [12832/42552]\n",
      "loss: 0.288211  [16032/42552]\n",
      "loss: 0.257158  [19232/42552]\n",
      "loss: 0.146666  [22432/42552]\n",
      "loss: 0.241995  [25632/42552]\n",
      "loss: 0.333358  [28832/42552]\n",
      "loss: 0.177722  [32032/42552]\n",
      "loss: 0.264053  [35232/42552]\n",
      "loss: 0.343257  [38432/42552]\n",
      "loss: 0.270912  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.241126 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.214353  [   32/42552]\n",
      "loss: 0.245198  [ 3232/42552]\n",
      "loss: 0.211947  [ 6432/42552]\n",
      "loss: 0.286049  [ 9632/42552]\n",
      "loss: 0.255524  [12832/42552]\n",
      "loss: 0.184535  [16032/42552]\n",
      "loss: 0.374247  [19232/42552]\n",
      "loss: 0.275164  [22432/42552]\n",
      "loss: 0.150689  [25632/42552]\n",
      "loss: 0.210942  [28832/42552]\n",
      "loss: 0.432726  [32032/42552]\n",
      "loss: 0.457677  [35232/42552]\n",
      "loss: 0.266531  [38432/42552]\n",
      "loss: 0.282270  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.243051 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.406527  [   32/42552]\n",
      "loss: 0.276250  [ 3232/42552]\n",
      "loss: 0.210889  [ 6432/42552]\n",
      "loss: 0.382621  [ 9632/42552]\n",
      "loss: 0.302789  [12832/42552]\n",
      "loss: 0.316705  [16032/42552]\n",
      "loss: 0.214754  [19232/42552]\n",
      "loss: 0.278695  [22432/42552]\n",
      "loss: 0.267222  [25632/42552]\n",
      "loss: 0.194004  [28832/42552]\n",
      "loss: 0.342311  [32032/42552]\n",
      "loss: 0.439548  [35232/42552]\n",
      "loss: 0.441013  [38432/42552]\n",
      "loss: 0.296266  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.247308 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.286799  [   32/42552]\n",
      "loss: 0.312900  [ 3232/42552]\n",
      "loss: 0.144377  [ 6432/42552]\n",
      "loss: 0.340669  [ 9632/42552]\n",
      "loss: 0.227083  [12832/42552]\n",
      "loss: 0.182598  [16032/42552]\n",
      "loss: 0.262946  [19232/42552]\n",
      "loss: 0.287994  [22432/42552]\n",
      "loss: 0.318198  [25632/42552]\n",
      "loss: 0.249757  [28832/42552]\n",
      "loss: 0.308282  [32032/42552]\n",
      "loss: 0.364918  [35232/42552]\n",
      "loss: 0.316855  [38432/42552]\n",
      "loss: 0.387779  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.243464 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.338330  [   32/42552]\n",
      "loss: 0.436743  [ 3232/42552]\n",
      "loss: 0.227106  [ 6432/42552]\n",
      "loss: 0.328131  [ 9632/42552]\n",
      "loss: 0.213469  [12832/42552]\n",
      "loss: 0.365575  [16032/42552]\n",
      "loss: 0.282396  [19232/42552]\n",
      "loss: 0.384059  [22432/42552]\n",
      "loss: 0.292628  [25632/42552]\n",
      "loss: 0.290077  [28832/42552]\n",
      "loss: 0.239150  [32032/42552]\n",
      "loss: 0.293678  [35232/42552]\n",
      "loss: 0.175777  [38432/42552]\n",
      "loss: 0.421148  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.243745 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.334928  [   32/42552]\n",
      "loss: 0.462375  [ 3232/42552]\n",
      "loss: 0.285415  [ 6432/42552]\n",
      "loss: 0.300322  [ 9632/42552]\n",
      "loss: 0.202148  [12832/42552]\n",
      "loss: 0.126714  [16032/42552]\n",
      "loss: 0.251285  [19232/42552]\n",
      "loss: 0.357928  [22432/42552]\n",
      "loss: 0.320540  [25632/42552]\n",
      "loss: 0.242243  [28832/42552]\n",
      "loss: 0.290238  [32032/42552]\n",
      "loss: 0.238480  [35232/42552]\n",
      "loss: 0.233852  [38432/42552]\n",
      "loss: 0.280800  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.251760 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.258885  [   32/42552]\n",
      "loss: 0.382091  [ 3232/42552]\n",
      "loss: 0.290861  [ 6432/42552]\n",
      "loss: 0.331207  [ 9632/42552]\n",
      "loss: 0.229245  [12832/42552]\n",
      "loss: 0.245155  [16032/42552]\n",
      "loss: 0.318275  [19232/42552]\n",
      "loss: 0.312296  [22432/42552]\n",
      "loss: 0.123236  [25632/42552]\n",
      "loss: 0.237397  [28832/42552]\n",
      "loss: 0.407809  [32032/42552]\n",
      "loss: 0.222701  [35232/42552]\n",
      "loss: 0.253935  [38432/42552]\n",
      "loss: 0.230185  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.244286 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.303875  [   32/42552]\n",
      "loss: 0.249916  [ 3232/42552]\n",
      "loss: 0.294611  [ 6432/42552]\n",
      "loss: 0.298504  [ 9632/42552]\n",
      "loss: 0.311961  [12832/42552]\n",
      "loss: 0.285493  [16032/42552]\n",
      "loss: 0.262909  [19232/42552]\n",
      "loss: 0.242255  [22432/42552]\n",
      "loss: 0.164626  [25632/42552]\n",
      "loss: 0.599887  [28832/42552]\n",
      "loss: 0.371707  [32032/42552]\n",
      "loss: 0.230157  [35232/42552]\n",
      "loss: 0.247527  [38432/42552]\n",
      "loss: 0.245545  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.243339 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.275071  [   32/42552]\n",
      "loss: 0.323339  [ 3232/42552]\n",
      "loss: 0.220334  [ 6432/42552]\n",
      "loss: 0.211132  [ 9632/42552]\n",
      "loss: 0.343179  [12832/42552]\n",
      "loss: 0.205177  [16032/42552]\n",
      "loss: 0.263977  [19232/42552]\n",
      "loss: 0.228737  [22432/42552]\n",
      "loss: 0.229021  [25632/42552]\n",
      "loss: 0.315732  [28832/42552]\n",
      "loss: 0.323327  [32032/42552]\n",
      "loss: 0.388485  [35232/42552]\n",
      "loss: 0.365138  [38432/42552]\n",
      "loss: 0.360138  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.244125 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.235736  [   32/42552]\n",
      "loss: 0.206888  [ 3232/42552]\n",
      "loss: 0.238707  [ 6432/42552]\n",
      "loss: 0.319729  [ 9632/42552]\n",
      "loss: 0.187434  [12832/42552]\n",
      "loss: 0.198598  [16032/42552]\n",
      "loss: 0.226198  [19232/42552]\n",
      "loss: 0.158058  [22432/42552]\n",
      "loss: 0.299881  [25632/42552]\n",
      "loss: 0.219807  [28832/42552]\n",
      "loss: 0.336798  [32032/42552]\n",
      "loss: 0.183184  [35232/42552]\n",
      "loss: 0.177907  [38432/42552]\n",
      "loss: 0.247009  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.254885 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.283591  [   32/42552]\n",
      "loss: 0.214717  [ 3232/42552]\n",
      "loss: 0.294306  [ 6432/42552]\n",
      "loss: 0.401416  [ 9632/42552]\n",
      "loss: 0.233730  [12832/42552]\n",
      "loss: 0.308826  [16032/42552]\n",
      "loss: 0.287470  [19232/42552]\n",
      "loss: 0.302263  [22432/42552]\n",
      "loss: 0.209374  [25632/42552]\n",
      "loss: 0.220680  [28832/42552]\n",
      "loss: 0.282911  [32032/42552]\n",
      "loss: 0.477122  [35232/42552]\n",
      "loss: 0.311519  [38432/42552]\n",
      "loss: 0.362337  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.243786 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.319868  [   32/42552]\n",
      "loss: 0.308540  [ 3232/42552]\n",
      "loss: 0.197529  [ 6432/42552]\n",
      "loss: 0.218760  [ 9632/42552]\n",
      "loss: 0.221748  [12832/42552]\n",
      "loss: 0.317260  [16032/42552]\n",
      "loss: 0.218835  [19232/42552]\n",
      "loss: 0.378711  [22432/42552]\n",
      "loss: 0.466428  [25632/42552]\n",
      "loss: 0.271125  [28832/42552]\n",
      "loss: 0.225820  [32032/42552]\n",
      "loss: 0.419620  [35232/42552]\n",
      "loss: 0.274576  [38432/42552]\n",
      "loss: 0.425386  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.249163 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.459080  [   32/42552]\n",
      "loss: 0.274447  [ 3232/42552]\n",
      "loss: 0.291883  [ 6432/42552]\n",
      "loss: 0.231650  [ 9632/42552]\n",
      "loss: 0.260690  [12832/42552]\n",
      "loss: 0.520146  [16032/42552]\n",
      "loss: 0.365678  [19232/42552]\n",
      "loss: 0.444897  [22432/42552]\n",
      "loss: 0.340416  [25632/42552]\n",
      "loss: 0.231700  [28832/42552]\n",
      "loss: 0.176795  [32032/42552]\n",
      "loss: 0.340684  [35232/42552]\n",
      "loss: 0.414072  [38432/42552]\n",
      "loss: 0.219415  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.238296 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.181005  [   32/42552]\n",
      "loss: 0.451748  [ 3232/42552]\n",
      "loss: 0.279770  [ 6432/42552]\n",
      "loss: 0.419146  [ 9632/42552]\n",
      "loss: 0.360103  [12832/42552]\n",
      "loss: 0.284075  [16032/42552]\n",
      "loss: 0.350791  [19232/42552]\n",
      "loss: 0.246227  [22432/42552]\n",
      "loss: 0.268836  [25632/42552]\n",
      "loss: 0.299657  [28832/42552]\n",
      "loss: 0.346708  [32032/42552]\n",
      "loss: 0.177162  [35232/42552]\n",
      "loss: 0.226408  [38432/42552]\n",
      "loss: 0.296247  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.247430 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.253870  [   32/42552]\n",
      "loss: 0.285577  [ 3232/42552]\n",
      "loss: 0.260425  [ 6432/42552]\n",
      "loss: 0.369179  [ 9632/42552]\n",
      "loss: 0.135224  [12832/42552]\n",
      "loss: 0.200958  [16032/42552]\n",
      "loss: 0.286655  [19232/42552]\n",
      "loss: 0.262347  [22432/42552]\n",
      "loss: 0.277432  [25632/42552]\n",
      "loss: 0.333197  [28832/42552]\n",
      "loss: 0.245348  [32032/42552]\n",
      "loss: 0.208646  [35232/42552]\n",
      "loss: 0.406184  [38432/42552]\n",
      "loss: 0.249082  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.240421 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.249596  [   32/42552]\n",
      "loss: 0.451957  [ 3232/42552]\n",
      "loss: 0.207402  [ 6432/42552]\n",
      "loss: 0.288121  [ 9632/42552]\n",
      "loss: 0.304934  [12832/42552]\n",
      "loss: 0.339414  [16032/42552]\n",
      "loss: 0.307793  [19232/42552]\n",
      "loss: 0.391326  [22432/42552]\n",
      "loss: 0.329666  [25632/42552]\n",
      "loss: 0.415476  [28832/42552]\n",
      "loss: 0.368201  [32032/42552]\n",
      "loss: 0.178559  [35232/42552]\n",
      "loss: 0.258704  [38432/42552]\n",
      "loss: 0.272743  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.248222 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.361516  [   32/42552]\n",
      "loss: 0.267186  [ 3232/42552]\n",
      "loss: 0.191893  [ 6432/42552]\n",
      "loss: 0.266451  [ 9632/42552]\n",
      "loss: 0.279781  [12832/42552]\n",
      "loss: 0.539285  [16032/42552]\n",
      "loss: 0.102843  [19232/42552]\n",
      "loss: 0.329361  [22432/42552]\n",
      "loss: 0.382701  [25632/42552]\n",
      "loss: 0.295426  [28832/42552]\n",
      "loss: 0.204891  [32032/42552]\n",
      "loss: 0.221320  [35232/42552]\n",
      "loss: 0.285610  [38432/42552]\n",
      "loss: 0.168530  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.249491 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.332340  [   32/42552]\n",
      "loss: 0.333501  [ 3232/42552]\n",
      "loss: 0.263679  [ 6432/42552]\n",
      "loss: 0.174298  [ 9632/42552]\n",
      "loss: 0.248018  [12832/42552]\n",
      "loss: 0.222428  [16032/42552]\n",
      "loss: 0.379213  [19232/42552]\n",
      "loss: 0.215413  [22432/42552]\n",
      "loss: 0.267445  [25632/42552]\n",
      "loss: 0.565586  [28832/42552]\n",
      "loss: 0.310812  [32032/42552]\n",
      "loss: 0.250298  [35232/42552]\n",
      "loss: 0.297992  [38432/42552]\n",
      "loss: 0.217073  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.239519 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.348819  [   32/42552]\n",
      "loss: 0.284553  [ 3232/42552]\n",
      "loss: 0.306630  [ 6432/42552]\n",
      "loss: 0.300136  [ 9632/42552]\n",
      "loss: 0.246193  [12832/42552]\n",
      "loss: 0.402225  [16032/42552]\n",
      "loss: 0.229087  [19232/42552]\n",
      "loss: 0.392010  [22432/42552]\n",
      "loss: 0.409505  [25632/42552]\n",
      "loss: 0.240249  [28832/42552]\n",
      "loss: 0.257363  [32032/42552]\n",
      "loss: 0.202852  [35232/42552]\n",
      "loss: 0.257532  [38432/42552]\n",
      "loss: 0.328588  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.246118 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.252880  [   32/42552]\n",
      "loss: 0.208118  [ 3232/42552]\n",
      "loss: 0.347095  [ 6432/42552]\n",
      "loss: 0.255070  [ 9632/42552]\n",
      "loss: 0.269927  [12832/42552]\n",
      "loss: 0.235394  [16032/42552]\n",
      "loss: 0.260363  [19232/42552]\n",
      "loss: 0.276588  [22432/42552]\n",
      "loss: 0.357740  [25632/42552]\n",
      "loss: 0.367713  [28832/42552]\n",
      "loss: 0.255345  [32032/42552]\n",
      "loss: 0.488775  [35232/42552]\n",
      "loss: 0.195380  [38432/42552]\n",
      "loss: 0.295002  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.250980 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.247658  [   32/42552]\n",
      "loss: 0.184373  [ 3232/42552]\n",
      "loss: 0.226098  [ 6432/42552]\n",
      "loss: 0.271011  [ 9632/42552]\n",
      "loss: 0.284437  [12832/42552]\n",
      "loss: 0.289345  [16032/42552]\n",
      "loss: 0.506781  [19232/42552]\n",
      "loss: 0.257247  [22432/42552]\n",
      "loss: 0.249652  [25632/42552]\n",
      "loss: 0.296670  [28832/42552]\n",
      "loss: 0.322021  [32032/42552]\n",
      "loss: 0.526317  [35232/42552]\n",
      "loss: 0.414551  [38432/42552]\n",
      "loss: 0.246395  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.242039 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.234399  [   32/42552]\n",
      "loss: 0.329602  [ 3232/42552]\n",
      "loss: 0.169423  [ 6432/42552]\n",
      "loss: 0.417253  [ 9632/42552]\n",
      "loss: 0.450876  [12832/42552]\n",
      "loss: 0.278279  [16032/42552]\n",
      "loss: 0.320893  [19232/42552]\n",
      "loss: 0.246497  [22432/42552]\n",
      "loss: 0.283112  [25632/42552]\n",
      "loss: 0.184467  [28832/42552]\n",
      "loss: 0.195313  [32032/42552]\n",
      "loss: 0.347870  [35232/42552]\n",
      "loss: 0.227981  [38432/42552]\n",
      "loss: 0.205356  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.247786 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.209334  [   32/42552]\n",
      "loss: 0.187187  [ 3232/42552]\n",
      "loss: 0.287427  [ 6432/42552]\n",
      "loss: 0.222086  [ 9632/42552]\n",
      "loss: 0.253357  [12832/42552]\n",
      "loss: 0.332924  [16032/42552]\n",
      "loss: 0.356176  [19232/42552]\n",
      "loss: 0.247296  [22432/42552]\n",
      "loss: 0.365879  [25632/42552]\n",
      "loss: 0.210279  [28832/42552]\n",
      "loss: 0.372259  [32032/42552]\n",
      "loss: 0.282199  [35232/42552]\n",
      "loss: 0.455347  [38432/42552]\n",
      "loss: 0.194938  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.257042 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.313380  [   32/42552]\n",
      "loss: 0.342472  [ 3232/42552]\n",
      "loss: 0.284594  [ 6432/42552]\n",
      "loss: 0.216311  [ 9632/42552]\n",
      "loss: 0.232275  [12832/42552]\n",
      "loss: 0.246340  [16032/42552]\n",
      "loss: 0.216381  [19232/42552]\n",
      "loss: 0.269350  [22432/42552]\n",
      "loss: 0.268405  [25632/42552]\n",
      "loss: 0.161182  [28832/42552]\n",
      "loss: 0.285857  [32032/42552]\n",
      "loss: 0.274850  [35232/42552]\n",
      "loss: 0.185109  [38432/42552]\n",
      "loss: 0.339071  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.250053 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.281869  [   32/42552]\n",
      "loss: 0.258318  [ 3232/42552]\n",
      "loss: 0.199839  [ 6432/42552]\n",
      "loss: 0.366594  [ 9632/42552]\n",
      "loss: 0.317748  [12832/42552]\n",
      "loss: 0.273439  [16032/42552]\n",
      "loss: 0.277827  [19232/42552]\n",
      "loss: 0.306954  [22432/42552]\n",
      "loss: 0.131654  [25632/42552]\n",
      "loss: 0.204320  [28832/42552]\n",
      "loss: 0.250718  [32032/42552]\n",
      "loss: 0.377483  [35232/42552]\n",
      "loss: 0.400153  [38432/42552]\n",
      "loss: 0.321826  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.258738 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.202331  [   32/42552]\n",
      "loss: 0.223372  [ 3232/42552]\n",
      "loss: 0.221704  [ 6432/42552]\n",
      "loss: 0.215920  [ 9632/42552]\n",
      "loss: 0.198898  [12832/42552]\n",
      "loss: 0.211006  [16032/42552]\n",
      "loss: 0.234920  [19232/42552]\n",
      "loss: 0.248572  [22432/42552]\n",
      "loss: 0.268073  [25632/42552]\n",
      "loss: 0.426524  [28832/42552]\n",
      "loss: 0.249904  [32032/42552]\n",
      "loss: 0.364100  [35232/42552]\n",
      "loss: 0.316327  [38432/42552]\n",
      "loss: 0.451420  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.237470 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.374847  [   32/42552]\n",
      "loss: 0.242364  [ 3232/42552]\n",
      "loss: 0.260074  [ 6432/42552]\n",
      "loss: 0.382872  [ 9632/42552]\n",
      "loss: 0.138046  [12832/42552]\n",
      "loss: 0.188622  [16032/42552]\n",
      "loss: 0.273256  [19232/42552]\n",
      "loss: 0.216697  [22432/42552]\n",
      "loss: 0.306902  [25632/42552]\n",
      "loss: 0.257213  [28832/42552]\n",
      "loss: 0.245359  [32032/42552]\n",
      "loss: 0.166357  [35232/42552]\n",
      "loss: 0.285876  [38432/42552]\n",
      "loss: 0.253129  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.240733 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.223565  [   32/42552]\n",
      "loss: 0.303279  [ 3232/42552]\n",
      "loss: 0.275475  [ 6432/42552]\n",
      "loss: 0.299918  [ 9632/42552]\n",
      "loss: 0.245627  [12832/42552]\n",
      "loss: 0.314079  [16032/42552]\n",
      "loss: 0.211582  [19232/42552]\n",
      "loss: 0.325644  [22432/42552]\n",
      "loss: 0.169429  [25632/42552]\n",
      "loss: 0.256799  [28832/42552]\n",
      "loss: 0.365135  [32032/42552]\n",
      "loss: 0.318195  [35232/42552]\n",
      "loss: 0.251898  [38432/42552]\n",
      "loss: 0.332842  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.240049 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.293990  [   32/42552]\n",
      "loss: 0.321935  [ 3232/42552]\n",
      "loss: 0.178280  [ 6432/42552]\n",
      "loss: 0.316895  [ 9632/42552]\n",
      "loss: 0.312664  [12832/42552]\n",
      "loss: 0.241375  [16032/42552]\n",
      "loss: 0.499155  [19232/42552]\n",
      "loss: 0.265926  [22432/42552]\n",
      "loss: 0.377480  [25632/42552]\n",
      "loss: 0.191520  [28832/42552]\n",
      "loss: 0.265842  [32032/42552]\n",
      "loss: 0.317289  [35232/42552]\n",
      "loss: 0.427155  [38432/42552]\n",
      "loss: 0.242945  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.240445 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.301129  [   32/42552]\n",
      "loss: 0.252553  [ 3232/42552]\n",
      "loss: 0.235982  [ 6432/42552]\n",
      "loss: 0.303621  [ 9632/42552]\n",
      "loss: 0.296762  [12832/42552]\n",
      "loss: 0.315664  [16032/42552]\n",
      "loss: 0.241326  [19232/42552]\n",
      "loss: 0.144415  [22432/42552]\n",
      "loss: 0.371076  [25632/42552]\n",
      "loss: 0.388649  [28832/42552]\n",
      "loss: 0.305164  [32032/42552]\n",
      "loss: 0.331450  [35232/42552]\n",
      "loss: 0.146575  [38432/42552]\n",
      "loss: 0.165342  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.244788 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.230149  [   32/42552]\n",
      "loss: 0.266193  [ 3232/42552]\n",
      "loss: 0.190857  [ 6432/42552]\n",
      "loss: 0.248989  [ 9632/42552]\n",
      "loss: 0.199279  [12832/42552]\n",
      "loss: 0.285457  [16032/42552]\n",
      "loss: 0.266178  [19232/42552]\n",
      "loss: 0.338366  [22432/42552]\n",
      "loss: 0.292011  [25632/42552]\n",
      "loss: 0.239140  [28832/42552]\n",
      "loss: 0.321676  [32032/42552]\n",
      "loss: 0.268751  [35232/42552]\n",
      "loss: 0.331505  [38432/42552]\n",
      "loss: 0.286417  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.239384 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.214906  [   32/42552]\n",
      "loss: 0.130470  [ 3232/42552]\n",
      "loss: 0.211018  [ 6432/42552]\n",
      "loss: 0.314621  [ 9632/42552]\n",
      "loss: 0.307030  [12832/42552]\n",
      "loss: 0.217629  [16032/42552]\n",
      "loss: 0.358247  [19232/42552]\n",
      "loss: 0.282066  [22432/42552]\n",
      "loss: 0.239235  [25632/42552]\n",
      "loss: 0.366750  [28832/42552]\n",
      "loss: 0.320794  [32032/42552]\n",
      "loss: 0.224385  [35232/42552]\n",
      "loss: 0.300112  [38432/42552]\n",
      "loss: 0.294880  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.238639 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.178428  [   32/42552]\n",
      "loss: 0.210777  [ 3232/42552]\n",
      "loss: 0.117197  [ 6432/42552]\n",
      "loss: 0.284081  [ 9632/42552]\n",
      "loss: 0.272279  [12832/42552]\n",
      "loss: 0.328197  [16032/42552]\n",
      "loss: 0.453218  [19232/42552]\n",
      "loss: 0.172473  [22432/42552]\n",
      "loss: 0.274077  [25632/42552]\n",
      "loss: 0.231185  [28832/42552]\n",
      "loss: 0.194175  [32032/42552]\n",
      "loss: 0.226395  [35232/42552]\n",
      "loss: 0.266926  [38432/42552]\n",
      "loss: 0.265091  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.240217 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.219185  [   32/42552]\n",
      "loss: 0.148471  [ 3232/42552]\n",
      "loss: 0.253068  [ 6432/42552]\n",
      "loss: 0.309537  [ 9632/42552]\n",
      "loss: 0.236512  [12832/42552]\n",
      "loss: 0.218890  [16032/42552]\n",
      "loss: 0.305690  [19232/42552]\n",
      "loss: 0.231437  [22432/42552]\n",
      "loss: 0.396087  [25632/42552]\n",
      "loss: 0.292003  [28832/42552]\n",
      "loss: 0.218065  [32032/42552]\n",
      "loss: 0.221994  [35232/42552]\n",
      "loss: 0.221164  [38432/42552]\n",
      "loss: 0.228759  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.248033 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.203562  [   32/42552]\n",
      "loss: 0.349603  [ 3232/42552]\n",
      "loss: 0.287037  [ 6432/42552]\n",
      "loss: 0.380197  [ 9632/42552]\n",
      "loss: 0.220403  [12832/42552]\n",
      "loss: 0.328137  [16032/42552]\n",
      "loss: 0.398904  [19232/42552]\n",
      "loss: 0.256934  [22432/42552]\n",
      "loss: 0.225411  [25632/42552]\n",
      "loss: 0.271506  [28832/42552]\n",
      "loss: 0.270965  [32032/42552]\n",
      "loss: 0.284428  [35232/42552]\n",
      "loss: 0.241933  [38432/42552]\n",
      "loss: 0.243996  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.240676 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.364075  [   32/42552]\n",
      "loss: 0.193070  [ 3232/42552]\n",
      "loss: 0.256811  [ 6432/42552]\n",
      "loss: 0.253314  [ 9632/42552]\n",
      "loss: 0.278162  [12832/42552]\n",
      "loss: 0.365510  [16032/42552]\n",
      "loss: 0.215273  [19232/42552]\n",
      "loss: 0.417808  [22432/42552]\n",
      "loss: 0.357972  [25632/42552]\n",
      "loss: 0.373266  [28832/42552]\n",
      "loss: 0.362255  [32032/42552]\n",
      "loss: 0.415211  [35232/42552]\n",
      "loss: 0.228722  [38432/42552]\n",
      "loss: 0.304566  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.238106 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.389813  [   32/42552]\n",
      "loss: 0.278563  [ 3232/42552]\n",
      "loss: 0.200754  [ 6432/42552]\n",
      "loss: 0.193036  [ 9632/42552]\n",
      "loss: 0.261068  [12832/42552]\n",
      "loss: 0.324268  [16032/42552]\n",
      "loss: 0.235768  [19232/42552]\n",
      "loss: 0.183511  [22432/42552]\n",
      "loss: 0.283745  [25632/42552]\n",
      "loss: 0.451562  [28832/42552]\n",
      "loss: 0.329777  [32032/42552]\n",
      "loss: 0.277217  [35232/42552]\n",
      "loss: 0.279453  [38432/42552]\n",
      "loss: 0.284179  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.240995 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.190464  [   32/42552]\n",
      "loss: 0.161084  [ 3232/42552]\n",
      "loss: 0.268379  [ 6432/42552]\n",
      "loss: 0.169853  [ 9632/42552]\n",
      "loss: 0.288646  [12832/42552]\n",
      "loss: 0.264152  [16032/42552]\n",
      "loss: 0.423453  [19232/42552]\n",
      "loss: 0.271877  [22432/42552]\n",
      "loss: 0.314580  [25632/42552]\n",
      "loss: 0.285132  [28832/42552]\n",
      "loss: 0.171780  [32032/42552]\n",
      "loss: 0.273563  [35232/42552]\n",
      "loss: 0.298481  [38432/42552]\n",
      "loss: 0.268819  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.236548 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.342274  [   32/42552]\n",
      "loss: 0.399805  [ 3232/42552]\n",
      "loss: 0.151823  [ 6432/42552]\n",
      "loss: 0.304958  [ 9632/42552]\n",
      "loss: 0.189224  [12832/42552]\n",
      "loss: 0.321130  [16032/42552]\n",
      "loss: 0.151452  [19232/42552]\n",
      "loss: 0.270831  [22432/42552]\n",
      "loss: 0.302665  [25632/42552]\n",
      "loss: 0.103671  [28832/42552]\n",
      "loss: 0.271992  [32032/42552]\n",
      "loss: 0.207360  [35232/42552]\n",
      "loss: 0.253544  [38432/42552]\n",
      "loss: 0.406501  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.240975 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.205298  [   32/42552]\n",
      "loss: 0.237009  [ 3232/42552]\n",
      "loss: 0.419772  [ 6432/42552]\n",
      "loss: 0.405675  [ 9632/42552]\n",
      "loss: 0.311008  [12832/42552]\n",
      "loss: 0.223096  [16032/42552]\n",
      "loss: 0.322255  [19232/42552]\n",
      "loss: 0.282578  [22432/42552]\n",
      "loss: 0.175413  [25632/42552]\n",
      "loss: 0.182462  [28832/42552]\n",
      "loss: 0.364515  [32032/42552]\n",
      "loss: 0.366785  [35232/42552]\n",
      "loss: 0.315826  [38432/42552]\n",
      "loss: 0.342666  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.237802 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.195538  [   32/42552]\n",
      "loss: 0.295589  [ 3232/42552]\n",
      "loss: 0.202640  [ 6432/42552]\n",
      "loss: 0.173481  [ 9632/42552]\n",
      "loss: 0.433654  [12832/42552]\n",
      "loss: 0.382378  [16032/42552]\n",
      "loss: 0.391820  [19232/42552]\n",
      "loss: 0.176392  [22432/42552]\n",
      "loss: 0.304865  [25632/42552]\n",
      "loss: 0.286927  [28832/42552]\n",
      "loss: 0.158697  [32032/42552]\n",
      "loss: 0.253377  [35232/42552]\n",
      "loss: 0.455862  [38432/42552]\n",
      "loss: 0.145702  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.239705 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.410558  [   32/42552]\n",
      "loss: 0.346101  [ 3232/42552]\n",
      "loss: 0.203775  [ 6432/42552]\n",
      "loss: 0.491928  [ 9632/42552]\n",
      "loss: 0.384455  [12832/42552]\n",
      "loss: 0.345248  [16032/42552]\n",
      "loss: 0.310457  [19232/42552]\n",
      "loss: 0.401883  [22432/42552]\n",
      "loss: 0.345374  [25632/42552]\n",
      "loss: 0.201274  [28832/42552]\n",
      "loss: 0.342410  [32032/42552]\n",
      "loss: 0.148604  [35232/42552]\n",
      "loss: 0.277473  [38432/42552]\n",
      "loss: 0.224238  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.239357 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.363499  [   32/42552]\n",
      "loss: 0.210373  [ 3232/42552]\n",
      "loss: 0.227764  [ 6432/42552]\n",
      "loss: 0.231620  [ 9632/42552]\n",
      "loss: 0.281015  [12832/42552]\n",
      "loss: 0.418517  [16032/42552]\n",
      "loss: 0.221429  [19232/42552]\n",
      "loss: 0.372274  [22432/42552]\n",
      "loss: 0.492732  [25632/42552]\n",
      "loss: 0.209801  [28832/42552]\n",
      "loss: 0.129396  [32032/42552]\n",
      "loss: 0.388136  [35232/42552]\n",
      "loss: 0.258861  [38432/42552]\n",
      "loss: 0.186291  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.268164 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.184246  [   32/42552]\n",
      "loss: 0.203827  [ 3232/42552]\n",
      "loss: 0.317708  [ 6432/42552]\n",
      "loss: 0.315457  [ 9632/42552]\n",
      "loss: 0.373117  [12832/42552]\n",
      "loss: 0.516889  [16032/42552]\n",
      "loss: 0.286595  [19232/42552]\n",
      "loss: 0.286866  [22432/42552]\n",
      "loss: 0.168430  [25632/42552]\n",
      "loss: 0.236896  [28832/42552]\n",
      "loss: 0.282959  [32032/42552]\n",
      "loss: 0.356171  [35232/42552]\n",
      "loss: 0.374090  [38432/42552]\n",
      "loss: 0.185996  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.235298 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.366668  [   32/42552]\n",
      "loss: 0.267289  [ 3232/42552]\n",
      "loss: 0.564119  [ 6432/42552]\n",
      "loss: 0.399206  [ 9632/42552]\n",
      "loss: 0.323938  [12832/42552]\n",
      "loss: 0.187086  [16032/42552]\n",
      "loss: 0.410628  [19232/42552]\n",
      "loss: 0.347770  [22432/42552]\n",
      "loss: 0.224554  [25632/42552]\n",
      "loss: 0.219504  [28832/42552]\n",
      "loss: 0.387078  [32032/42552]\n",
      "loss: 0.396413  [35232/42552]\n",
      "loss: 0.421936  [38432/42552]\n",
      "loss: 0.271417  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.241642 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.382105  [   32/42552]\n",
      "loss: 0.334987  [ 3232/42552]\n",
      "loss: 0.301888  [ 6432/42552]\n",
      "loss: 0.258625  [ 9632/42552]\n",
      "loss: 0.225107  [12832/42552]\n",
      "loss: 0.269638  [16032/42552]\n",
      "loss: 0.250737  [19232/42552]\n",
      "loss: 0.223361  [22432/42552]\n",
      "loss: 0.382393  [25632/42552]\n",
      "loss: 0.270502  [28832/42552]\n",
      "loss: 0.218624  [32032/42552]\n",
      "loss: 0.306022  [35232/42552]\n",
      "loss: 0.345717  [38432/42552]\n",
      "loss: 0.334840  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.242284 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.376297  [   32/42552]\n",
      "loss: 0.305672  [ 3232/42552]\n",
      "loss: 0.231316  [ 6432/42552]\n",
      "loss: 0.319974  [ 9632/42552]\n",
      "loss: 0.309314  [12832/42552]\n",
      "loss: 0.315204  [16032/42552]\n",
      "loss: 0.217258  [19232/42552]\n",
      "loss: 0.204528  [22432/42552]\n",
      "loss: 0.389407  [25632/42552]\n",
      "loss: 0.206036  [28832/42552]\n",
      "loss: 0.243220  [32032/42552]\n",
      "loss: 0.242976  [35232/42552]\n",
      "loss: 0.366061  [38432/42552]\n",
      "loss: 0.149677  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.237569 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.198979  [   32/42552]\n",
      "loss: 0.474090  [ 3232/42552]\n",
      "loss: 0.207748  [ 6432/42552]\n",
      "loss: 0.256989  [ 9632/42552]\n",
      "loss: 0.222190  [12832/42552]\n",
      "loss: 0.262710  [16032/42552]\n",
      "loss: 0.213452  [19232/42552]\n",
      "loss: 0.224605  [22432/42552]\n",
      "loss: 0.297663  [25632/42552]\n",
      "loss: 0.135897  [28832/42552]\n",
      "loss: 0.236064  [32032/42552]\n",
      "loss: 0.156305  [35232/42552]\n",
      "loss: 0.148853  [38432/42552]\n",
      "loss: 0.285728  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.236712 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.153365  [   32/42552]\n",
      "loss: 0.213897  [ 3232/42552]\n",
      "loss: 0.325587  [ 6432/42552]\n",
      "loss: 0.215454  [ 9632/42552]\n",
      "loss: 0.405946  [12832/42552]\n",
      "loss: 0.411449  [16032/42552]\n",
      "loss: 0.264115  [19232/42552]\n",
      "loss: 0.216067  [22432/42552]\n",
      "loss: 0.311688  [25632/42552]\n",
      "loss: 0.264095  [28832/42552]\n",
      "loss: 0.349737  [32032/42552]\n",
      "loss: 0.271398  [35232/42552]\n",
      "loss: 0.269355  [38432/42552]\n",
      "loss: 0.190240  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.238529 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.344045  [   32/42552]\n",
      "loss: 0.380385  [ 3232/42552]\n",
      "loss: 0.429650  [ 6432/42552]\n",
      "loss: 0.224024  [ 9632/42552]\n",
      "loss: 0.240761  [12832/42552]\n",
      "loss: 0.200815  [16032/42552]\n",
      "loss: 0.315930  [19232/42552]\n",
      "loss: 0.319368  [22432/42552]\n",
      "loss: 0.337572  [25632/42552]\n",
      "loss: 0.172318  [28832/42552]\n",
      "loss: 0.143724  [32032/42552]\n",
      "loss: 0.230208  [35232/42552]\n",
      "loss: 0.240556  [38432/42552]\n",
      "loss: 0.167784  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.237977 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.286346  [   32/42552]\n",
      "loss: 0.292794  [ 3232/42552]\n",
      "loss: 0.347527  [ 6432/42552]\n",
      "loss: 0.297716  [ 9632/42552]\n",
      "loss: 0.232543  [12832/42552]\n",
      "loss: 0.258419  [16032/42552]\n",
      "loss: 0.329492  [19232/42552]\n",
      "loss: 0.263478  [22432/42552]\n",
      "loss: 0.328835  [25632/42552]\n",
      "loss: 0.347683  [28832/42552]\n",
      "loss: 0.290192  [32032/42552]\n",
      "loss: 0.312620  [35232/42552]\n",
      "loss: 0.259866  [38432/42552]\n",
      "loss: 0.479479  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.239876 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.333263  [   32/42552]\n",
      "loss: 0.270866  [ 3232/42552]\n",
      "loss: 0.273641  [ 6432/42552]\n",
      "loss: 0.244918  [ 9632/42552]\n",
      "loss: 0.330484  [12832/42552]\n",
      "loss: 0.321984  [16032/42552]\n",
      "loss: 0.192436  [19232/42552]\n",
      "loss: 0.571712  [22432/42552]\n",
      "loss: 0.307000  [25632/42552]\n",
      "loss: 0.396185  [28832/42552]\n",
      "loss: 0.124832  [32032/42552]\n",
      "loss: 0.222113  [35232/42552]\n",
      "loss: 0.413601  [38432/42552]\n",
      "loss: 0.437522  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.234673 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.284490  [   32/42552]\n",
      "loss: 0.311002  [ 3232/42552]\n",
      "loss: 0.289426  [ 6432/42552]\n",
      "loss: 0.403469  [ 9632/42552]\n",
      "loss: 0.140581  [12832/42552]\n",
      "loss: 0.382457  [16032/42552]\n",
      "loss: 0.459704  [19232/42552]\n",
      "loss: 0.284007  [22432/42552]\n",
      "loss: 0.260407  [25632/42552]\n",
      "loss: 0.254844  [28832/42552]\n",
      "loss: 0.136804  [32032/42552]\n",
      "loss: 0.280279  [35232/42552]\n",
      "loss: 0.224558  [38432/42552]\n",
      "loss: 0.348886  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.258973 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.171508  [   32/42552]\n",
      "loss: 0.250957  [ 3232/42552]\n",
      "loss: 0.283472  [ 6432/42552]\n",
      "loss: 0.219860  [ 9632/42552]\n",
      "loss: 0.230320  [12832/42552]\n",
      "loss: 0.230005  [16032/42552]\n",
      "loss: 0.414110  [19232/42552]\n",
      "loss: 0.356838  [22432/42552]\n",
      "loss: 0.353642  [25632/42552]\n",
      "loss: 0.413443  [28832/42552]\n",
      "loss: 0.278751  [32032/42552]\n",
      "loss: 0.513871  [35232/42552]\n",
      "loss: 0.208154  [38432/42552]\n",
      "loss: 0.415499  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.241306 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.298819  [   32/42552]\n",
      "loss: 0.136935  [ 3232/42552]\n",
      "loss: 0.340325  [ 6432/42552]\n",
      "loss: 0.206179  [ 9632/42552]\n",
      "loss: 0.234589  [12832/42552]\n",
      "loss: 0.189132  [16032/42552]\n",
      "loss: 0.232297  [19232/42552]\n",
      "loss: 0.273639  [22432/42552]\n",
      "loss: 0.326742  [25632/42552]\n",
      "loss: 0.320951  [28832/42552]\n",
      "loss: 0.290573  [32032/42552]\n",
      "loss: 0.178928  [35232/42552]\n",
      "loss: 0.251653  [38432/42552]\n",
      "loss: 0.353923  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.237656 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.401967  [   32/42552]\n",
      "loss: 0.249138  [ 3232/42552]\n",
      "loss: 0.280914  [ 6432/42552]\n",
      "loss: 0.261412  [ 9632/42552]\n",
      "loss: 0.270020  [12832/42552]\n",
      "loss: 0.356420  [16032/42552]\n",
      "loss: 0.228927  [19232/42552]\n",
      "loss: 0.362267  [22432/42552]\n",
      "loss: 0.264769  [25632/42552]\n",
      "loss: 0.265786  [28832/42552]\n",
      "loss: 0.359228  [32032/42552]\n",
      "loss: 0.216426  [35232/42552]\n",
      "loss: 0.252263  [38432/42552]\n",
      "loss: 0.176145  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.239965 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.255025  [   32/42552]\n",
      "loss: 0.472262  [ 3232/42552]\n",
      "loss: 0.312780  [ 6432/42552]\n",
      "loss: 0.181214  [ 9632/42552]\n",
      "loss: 0.280529  [12832/42552]\n",
      "loss: 0.214464  [16032/42552]\n",
      "loss: 0.310037  [19232/42552]\n",
      "loss: 0.236925  [22432/42552]\n",
      "loss: 0.390491  [25632/42552]\n",
      "loss: 0.425569  [28832/42552]\n",
      "loss: 0.203751  [32032/42552]\n",
      "loss: 0.284374  [35232/42552]\n",
      "loss: 0.242088  [38432/42552]\n",
      "loss: 0.284247  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.233870 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.207366  [   32/42552]\n",
      "loss: 0.187086  [ 3232/42552]\n",
      "loss: 0.286780  [ 6432/42552]\n",
      "loss: 0.344256  [ 9632/42552]\n",
      "loss: 0.319553  [12832/42552]\n",
      "loss: 0.193941  [16032/42552]\n",
      "loss: 0.200125  [19232/42552]\n",
      "loss: 0.500395  [22432/42552]\n",
      "loss: 0.254496  [25632/42552]\n",
      "loss: 0.265577  [28832/42552]\n",
      "loss: 0.396925  [32032/42552]\n",
      "loss: 0.376005  [35232/42552]\n",
      "loss: 0.162188  [38432/42552]\n",
      "loss: 0.413595  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.237805 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.273275  [   32/42552]\n",
      "loss: 0.212158  [ 3232/42552]\n",
      "loss: 0.203999  [ 6432/42552]\n",
      "loss: 0.259340  [ 9632/42552]\n",
      "loss: 0.107183  [12832/42552]\n",
      "loss: 0.226167  [16032/42552]\n",
      "loss: 0.273527  [19232/42552]\n",
      "loss: 0.246411  [22432/42552]\n",
      "loss: 0.192906  [25632/42552]\n",
      "loss: 0.231899  [28832/42552]\n",
      "loss: 0.258794  [32032/42552]\n",
      "loss: 0.247381  [35232/42552]\n",
      "loss: 0.236233  [38432/42552]\n",
      "loss: 0.185690  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.238876 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.187397  [   32/42552]\n",
      "loss: 0.150581  [ 3232/42552]\n",
      "loss: 0.212325  [ 6432/42552]\n",
      "loss: 0.345356  [ 9632/42552]\n",
      "loss: 0.188271  [12832/42552]\n",
      "loss: 0.250237  [16032/42552]\n",
      "loss: 0.186550  [19232/42552]\n",
      "loss: 0.369494  [22432/42552]\n",
      "loss: 0.183414  [25632/42552]\n",
      "loss: 0.266733  [28832/42552]\n",
      "loss: 0.198355  [32032/42552]\n",
      "loss: 0.234430  [35232/42552]\n",
      "loss: 0.339938  [38432/42552]\n",
      "loss: 0.346590  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.234397 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.313862  [   32/42552]\n",
      "loss: 0.242964  [ 3232/42552]\n",
      "loss: 0.376523  [ 6432/42552]\n",
      "loss: 0.173054  [ 9632/42552]\n",
      "loss: 0.237154  [12832/42552]\n",
      "loss: 0.204142  [16032/42552]\n",
      "loss: 0.309233  [19232/42552]\n",
      "loss: 0.200238  [22432/42552]\n",
      "loss: 0.352626  [25632/42552]\n",
      "loss: 0.169331  [28832/42552]\n",
      "loss: 0.304562  [32032/42552]\n",
      "loss: 0.326612  [35232/42552]\n",
      "loss: 0.221076  [38432/42552]\n",
      "loss: 0.285639  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.239064 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.173084  [   32/42552]\n",
      "loss: 0.275824  [ 3232/42552]\n",
      "loss: 0.225974  [ 6432/42552]\n",
      "loss: 0.307028  [ 9632/42552]\n",
      "loss: 0.282846  [12832/42552]\n",
      "loss: 0.173359  [16032/42552]\n",
      "loss: 0.353732  [19232/42552]\n",
      "loss: 0.301524  [22432/42552]\n",
      "loss: 0.355265  [25632/42552]\n",
      "loss: 0.368384  [28832/42552]\n",
      "loss: 0.344467  [32032/42552]\n",
      "loss: 0.254109  [35232/42552]\n",
      "loss: 0.302078  [38432/42552]\n",
      "loss: 0.309839  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.241912 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.230064  [   32/42552]\n",
      "loss: 0.221051  [ 3232/42552]\n",
      "loss: 0.288566  [ 6432/42552]\n",
      "loss: 0.299007  [ 9632/42552]\n",
      "loss: 0.170022  [12832/42552]\n",
      "loss: 0.299270  [16032/42552]\n",
      "loss: 0.222210  [19232/42552]\n",
      "loss: 0.251347  [22432/42552]\n",
      "loss: 0.179043  [25632/42552]\n",
      "loss: 0.267059  [28832/42552]\n",
      "loss: 0.366504  [32032/42552]\n",
      "loss: 0.161368  [35232/42552]\n",
      "loss: 0.220694  [38432/42552]\n",
      "loss: 0.206835  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.235116 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.298512  [   32/42552]\n",
      "loss: 0.267612  [ 3232/42552]\n",
      "loss: 0.214996  [ 6432/42552]\n",
      "loss: 0.228896  [ 9632/42552]\n",
      "loss: 0.175546  [12832/42552]\n",
      "loss: 0.415503  [16032/42552]\n",
      "loss: 0.292641  [19232/42552]\n",
      "loss: 0.270550  [22432/42552]\n",
      "loss: 0.277010  [25632/42552]\n",
      "loss: 0.394701  [28832/42552]\n",
      "loss: 0.291500  [32032/42552]\n",
      "loss: 0.194143  [35232/42552]\n",
      "loss: 0.289866  [38432/42552]\n",
      "loss: 0.235335  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.232620 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.234145  [   32/42552]\n",
      "loss: 0.264545  [ 3232/42552]\n",
      "loss: 0.294013  [ 6432/42552]\n",
      "loss: 0.206998  [ 9632/42552]\n",
      "loss: 0.263299  [12832/42552]\n",
      "loss: 0.285931  [16032/42552]\n",
      "loss: 0.199472  [19232/42552]\n",
      "loss: 0.334139  [22432/42552]\n",
      "loss: 0.265644  [25632/42552]\n",
      "loss: 0.278014  [28832/42552]\n",
      "loss: 0.192987  [32032/42552]\n",
      "loss: 0.151220  [35232/42552]\n",
      "loss: 0.326512  [38432/42552]\n",
      "loss: 0.208618  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.235274 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.285185  [   32/42552]\n",
      "loss: 0.247131  [ 3232/42552]\n",
      "loss: 0.456562  [ 6432/42552]\n",
      "loss: 0.259085  [ 9632/42552]\n",
      "loss: 0.372039  [12832/42552]\n",
      "loss: 0.182058  [16032/42552]\n",
      "loss: 0.477301  [19232/42552]\n",
      "loss: 0.322495  [22432/42552]\n",
      "loss: 0.273478  [25632/42552]\n",
      "loss: 0.374363  [28832/42552]\n",
      "loss: 0.353801  [32032/42552]\n",
      "loss: 0.275894  [35232/42552]\n",
      "loss: 0.318430  [38432/42552]\n",
      "loss: 0.313146  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.245761 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.258713  [   32/42552]\n",
      "loss: 0.220520  [ 3232/42552]\n",
      "loss: 0.247175  [ 6432/42552]\n",
      "loss: 0.307977  [ 9632/42552]\n",
      "loss: 0.196298  [12832/42552]\n",
      "loss: 0.269347  [16032/42552]\n",
      "loss: 0.370108  [19232/42552]\n",
      "loss: 0.328064  [22432/42552]\n",
      "loss: 0.261984  [25632/42552]\n",
      "loss: 0.245371  [28832/42552]\n",
      "loss: 0.412603  [32032/42552]\n",
      "loss: 0.229026  [35232/42552]\n",
      "loss: 0.194975  [38432/42552]\n",
      "loss: 0.299434  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.245757 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.327502  [   32/42552]\n",
      "loss: 0.145017  [ 3232/42552]\n",
      "loss: 0.190725  [ 6432/42552]\n",
      "loss: 0.274421  [ 9632/42552]\n",
      "loss: 0.324324  [12832/42552]\n",
      "loss: 0.393954  [16032/42552]\n",
      "loss: 0.387224  [19232/42552]\n",
      "loss: 0.332163  [22432/42552]\n",
      "loss: 0.383088  [25632/42552]\n",
      "loss: 0.235616  [28832/42552]\n",
      "loss: 0.322747  [32032/42552]\n",
      "loss: 0.282382  [35232/42552]\n",
      "loss: 0.340693  [38432/42552]\n",
      "loss: 0.362584  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.240785 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.306118  [   32/42552]\n",
      "loss: 0.246524  [ 3232/42552]\n",
      "loss: 0.302936  [ 6432/42552]\n",
      "loss: 0.305209  [ 9632/42552]\n",
      "loss: 0.358186  [12832/42552]\n",
      "loss: 0.241391  [16032/42552]\n",
      "loss: 0.301655  [19232/42552]\n",
      "loss: 0.212100  [22432/42552]\n",
      "loss: 0.230122  [25632/42552]\n",
      "loss: 0.319829  [28832/42552]\n",
      "loss: 0.278536  [32032/42552]\n",
      "loss: 0.307904  [35232/42552]\n",
      "loss: 0.200969  [38432/42552]\n",
      "loss: 0.220033  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.248156 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.335327  [   32/42552]\n",
      "loss: 0.159134  [ 3232/42552]\n",
      "loss: 0.259863  [ 6432/42552]\n",
      "loss: 0.222049  [ 9632/42552]\n",
      "loss: 0.178834  [12832/42552]\n",
      "loss: 0.212290  [16032/42552]\n",
      "loss: 0.292883  [19232/42552]\n",
      "loss: 0.243963  [22432/42552]\n",
      "loss: 0.277368  [25632/42552]\n",
      "loss: 0.306135  [28832/42552]\n",
      "loss: 0.152384  [32032/42552]\n",
      "loss: 0.459519  [35232/42552]\n",
      "loss: 0.195641  [38432/42552]\n",
      "loss: 0.252437  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.232742 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.191334  [   32/42552]\n",
      "loss: 0.140469  [ 3232/42552]\n",
      "loss: 0.189769  [ 6432/42552]\n",
      "loss: 0.180727  [ 9632/42552]\n",
      "loss: 0.203421  [12832/42552]\n",
      "loss: 0.259967  [16032/42552]\n",
      "loss: 0.472841  [19232/42552]\n",
      "loss: 0.243306  [22432/42552]\n",
      "loss: 0.373391  [25632/42552]\n",
      "loss: 0.318307  [28832/42552]\n",
      "loss: 0.313299  [32032/42552]\n",
      "loss: 0.188897  [35232/42552]\n",
      "loss: 0.390750  [38432/42552]\n",
      "loss: 0.277325  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.241169 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.208769  [   32/42552]\n",
      "loss: 0.280542  [ 3232/42552]\n",
      "loss: 0.266001  [ 6432/42552]\n",
      "loss: 0.220327  [ 9632/42552]\n",
      "loss: 0.279936  [12832/42552]\n",
      "loss: 0.221017  [16032/42552]\n",
      "loss: 0.286023  [19232/42552]\n",
      "loss: 0.210957  [22432/42552]\n",
      "loss: 0.236678  [25632/42552]\n",
      "loss: 0.219127  [28832/42552]\n",
      "loss: 0.284156  [32032/42552]\n",
      "loss: 0.260401  [35232/42552]\n",
      "loss: 0.151338  [38432/42552]\n",
      "loss: 0.288091  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.238148 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.359498  [   32/42552]\n",
      "loss: 0.399695  [ 3232/42552]\n",
      "loss: 0.235610  [ 6432/42552]\n",
      "loss: 0.428059  [ 9632/42552]\n",
      "loss: 0.143016  [12832/42552]\n",
      "loss: 0.321474  [16032/42552]\n",
      "loss: 0.250069  [19232/42552]\n",
      "loss: 0.176516  [22432/42552]\n",
      "loss: 0.137179  [25632/42552]\n",
      "loss: 0.267547  [28832/42552]\n",
      "loss: 0.241221  [32032/42552]\n",
      "loss: 0.345578  [35232/42552]\n",
      "loss: 0.249007  [38432/42552]\n",
      "loss: 0.477243  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.242002 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.369290  [   32/42552]\n",
      "loss: 0.224409  [ 3232/42552]\n",
      "loss: 0.292639  [ 6432/42552]\n",
      "loss: 0.507964  [ 9632/42552]\n",
      "loss: 0.334014  [12832/42552]\n",
      "loss: 0.199242  [16032/42552]\n",
      "loss: 0.259259  [19232/42552]\n",
      "loss: 0.172764  [22432/42552]\n",
      "loss: 0.270590  [25632/42552]\n",
      "loss: 0.175223  [28832/42552]\n",
      "loss: 0.272258  [32032/42552]\n",
      "loss: 0.272752  [35232/42552]\n",
      "loss: 0.289776  [38432/42552]\n",
      "loss: 0.267743  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.236309 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.385002  [   32/42552]\n",
      "loss: 0.540090  [ 3232/42552]\n",
      "loss: 0.253964  [ 6432/42552]\n",
      "loss: 0.225233  [ 9632/42552]\n",
      "loss: 0.198225  [12832/42552]\n",
      "loss: 0.185689  [16032/42552]\n",
      "loss: 0.255599  [19232/42552]\n",
      "loss: 0.324647  [22432/42552]\n",
      "loss: 0.360768  [25632/42552]\n",
      "loss: 0.328607  [28832/42552]\n",
      "loss: 0.178242  [32032/42552]\n",
      "loss: 0.325032  [35232/42552]\n",
      "loss: 0.287308  [38432/42552]\n",
      "loss: 0.235434  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.247801 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.219816  [   32/42552]\n",
      "loss: 0.409741  [ 3232/42552]\n",
      "loss: 0.236180  [ 6432/42552]\n",
      "loss: 0.262367  [ 9632/42552]\n",
      "loss: 0.205168  [12832/42552]\n",
      "loss: 0.320910  [16032/42552]\n",
      "loss: 0.448454  [19232/42552]\n",
      "loss: 0.298633  [22432/42552]\n",
      "loss: 0.168686  [25632/42552]\n",
      "loss: 0.199571  [28832/42552]\n",
      "loss: 0.292042  [32032/42552]\n",
      "loss: 0.317310  [35232/42552]\n",
      "loss: 0.295033  [38432/42552]\n",
      "loss: 0.320812  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.245726 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.460848  [   32/42552]\n",
      "loss: 0.443918  [ 3232/42552]\n",
      "loss: 0.214111  [ 6432/42552]\n",
      "loss: 0.364917  [ 9632/42552]\n",
      "loss: 0.504112  [12832/42552]\n",
      "loss: 0.266046  [16032/42552]\n",
      "loss: 0.400347  [19232/42552]\n",
      "loss: 0.235340  [22432/42552]\n",
      "loss: 0.197153  [25632/42552]\n",
      "loss: 0.294644  [28832/42552]\n",
      "loss: 0.243723  [32032/42552]\n",
      "loss: 0.195385  [35232/42552]\n",
      "loss: 0.252803  [38432/42552]\n",
      "loss: 0.284554  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.236777 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.220308  [   32/42552]\n",
      "loss: 0.402485  [ 3232/42552]\n",
      "loss: 0.217623  [ 6432/42552]\n",
      "loss: 0.168909  [ 9632/42552]\n",
      "loss: 0.252090  [12832/42552]\n",
      "loss: 0.371071  [16032/42552]\n",
      "loss: 0.176317  [19232/42552]\n",
      "loss: 0.336149  [22432/42552]\n",
      "loss: 0.313841  [25632/42552]\n",
      "loss: 0.332438  [28832/42552]\n",
      "loss: 0.204536  [32032/42552]\n",
      "loss: 0.283816  [35232/42552]\n",
      "loss: 0.280352  [38432/42552]\n",
      "loss: 0.267509  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.239396 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.281253  [   32/42552]\n",
      "loss: 0.314945  [ 3232/42552]\n",
      "loss: 0.268344  [ 6432/42552]\n",
      "loss: 0.276979  [ 9632/42552]\n",
      "loss: 0.311878  [12832/42552]\n",
      "loss: 0.363687  [16032/42552]\n",
      "loss: 0.214232  [19232/42552]\n",
      "loss: 0.147266  [22432/42552]\n",
      "loss: 0.272314  [25632/42552]\n",
      "loss: 0.254641  [28832/42552]\n",
      "loss: 0.280387  [32032/42552]\n",
      "loss: 0.238647  [35232/42552]\n",
      "loss: 0.350866  [38432/42552]\n",
      "loss: 0.337130  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.238217 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.381665  [   32/42552]\n",
      "loss: 0.203821  [ 3232/42552]\n",
      "loss: 0.245213  [ 6432/42552]\n",
      "loss: 0.375635  [ 9632/42552]\n",
      "loss: 0.272531  [12832/42552]\n",
      "loss: 0.330994  [16032/42552]\n",
      "loss: 0.187995  [19232/42552]\n",
      "loss: 0.191850  [22432/42552]\n",
      "loss: 0.344692  [25632/42552]\n",
      "loss: 0.330407  [28832/42552]\n",
      "loss: 0.231270  [32032/42552]\n",
      "loss: 0.219040  [35232/42552]\n",
      "loss: 0.360790  [38432/42552]\n",
      "loss: 0.323742  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.240755 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.227285  [   32/42552]\n",
      "loss: 0.228049  [ 3232/42552]\n",
      "loss: 0.252445  [ 6432/42552]\n",
      "loss: 0.147629  [ 9632/42552]\n",
      "loss: 0.269344  [12832/42552]\n",
      "loss: 0.197643  [16032/42552]\n",
      "loss: 0.316093  [19232/42552]\n",
      "loss: 0.279219  [22432/42552]\n",
      "loss: 0.382844  [25632/42552]\n",
      "loss: 0.372061  [28832/42552]\n",
      "loss: 0.289662  [32032/42552]\n",
      "loss: 0.304423  [35232/42552]\n",
      "loss: 0.112162  [38432/42552]\n",
      "loss: 0.296443  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.239353 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.307636  [   32/42552]\n",
      "loss: 0.327327  [ 3232/42552]\n",
      "loss: 0.275705  [ 6432/42552]\n",
      "loss: 0.273330  [ 9632/42552]\n",
      "loss: 0.192609  [12832/42552]\n",
      "loss: 0.266468  [16032/42552]\n",
      "loss: 0.277920  [19232/42552]\n",
      "loss: 0.239600  [22432/42552]\n",
      "loss: 0.212267  [25632/42552]\n",
      "loss: 0.296166  [28832/42552]\n",
      "loss: 0.210326  [32032/42552]\n",
      "loss: 0.245152  [35232/42552]\n",
      "loss: 0.446470  [38432/42552]\n",
      "loss: 0.269373  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.232398 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.316094  [   32/42552]\n",
      "loss: 0.262025  [ 3232/42552]\n",
      "loss: 0.211723  [ 6432/42552]\n",
      "loss: 0.265237  [ 9632/42552]\n",
      "loss: 0.443840  [12832/42552]\n",
      "loss: 0.227263  [16032/42552]\n",
      "loss: 0.196382  [19232/42552]\n",
      "loss: 0.362382  [22432/42552]\n",
      "loss: 0.270581  [25632/42552]\n",
      "loss: 0.161668  [28832/42552]\n",
      "loss: 0.598051  [32032/42552]\n",
      "loss: 0.394472  [35232/42552]\n",
      "loss: 0.301547  [38432/42552]\n",
      "loss: 0.264205  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.233839 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.345594  [   32/42552]\n",
      "loss: 0.447025  [ 3232/42552]\n",
      "loss: 0.424306  [ 6432/42552]\n",
      "loss: 0.354077  [ 9632/42552]\n",
      "loss: 0.229601  [12832/42552]\n",
      "loss: 0.189402  [16032/42552]\n",
      "loss: 0.291228  [19232/42552]\n",
      "loss: 0.220687  [22432/42552]\n",
      "loss: 0.214693  [25632/42552]\n",
      "loss: 0.293443  [28832/42552]\n",
      "loss: 0.276187  [32032/42552]\n",
      "loss: 0.320630  [35232/42552]\n",
      "loss: 0.262893  [38432/42552]\n",
      "loss: 0.195968  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.234543 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.188503  [   32/42552]\n",
      "loss: 0.284356  [ 3232/42552]\n",
      "loss: 0.301172  [ 6432/42552]\n",
      "loss: 0.357342  [ 9632/42552]\n",
      "loss: 0.279567  [12832/42552]\n",
      "loss: 0.235403  [16032/42552]\n",
      "loss: 0.165746  [19232/42552]\n",
      "loss: 0.336479  [22432/42552]\n",
      "loss: 0.253087  [25632/42552]\n",
      "loss: 0.219417  [28832/42552]\n",
      "loss: 0.142025  [32032/42552]\n",
      "loss: 0.332791  [35232/42552]\n",
      "loss: 0.224003  [38432/42552]\n",
      "loss: 0.274350  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.237384 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.391529  [   32/42552]\n",
      "loss: 0.357591  [ 3232/42552]\n",
      "loss: 0.250196  [ 6432/42552]\n",
      "loss: 0.401117  [ 9632/42552]\n",
      "loss: 0.402901  [12832/42552]\n",
      "loss: 0.360203  [16032/42552]\n",
      "loss: 0.284053  [19232/42552]\n",
      "loss: 0.431187  [22432/42552]\n",
      "loss: 0.226717  [25632/42552]\n",
      "loss: 0.286303  [28832/42552]\n",
      "loss: 0.300035  [32032/42552]\n",
      "loss: 0.307307  [35232/42552]\n",
      "loss: 0.300047  [38432/42552]\n",
      "loss: 0.291125  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.233818 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.255563  [   32/42552]\n",
      "loss: 0.295814  [ 3232/42552]\n",
      "loss: 0.204476  [ 6432/42552]\n",
      "loss: 0.177672  [ 9632/42552]\n",
      "loss: 0.241289  [12832/42552]\n",
      "loss: 0.262869  [16032/42552]\n",
      "loss: 0.219635  [19232/42552]\n",
      "loss: 0.305420  [22432/42552]\n",
      "loss: 0.568265  [25632/42552]\n",
      "loss: 0.179739  [28832/42552]\n",
      "loss: 0.166055  [32032/42552]\n",
      "loss: 0.301241  [35232/42552]\n",
      "loss: 0.279995  [38432/42552]\n",
      "loss: 0.239202  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.232944 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.394936  [   32/42552]\n",
      "loss: 0.208755  [ 3232/42552]\n",
      "loss: 0.217043  [ 6432/42552]\n",
      "loss: 0.454891  [ 9632/42552]\n",
      "loss: 0.260015  [12832/42552]\n",
      "loss: 0.253637  [16032/42552]\n",
      "loss: 0.391992  [19232/42552]\n",
      "loss: 0.366963  [22432/42552]\n",
      "loss: 0.319648  [25632/42552]\n",
      "loss: 0.315648  [28832/42552]\n",
      "loss: 0.273090  [32032/42552]\n",
      "loss: 0.210991  [35232/42552]\n",
      "loss: 0.164335  [38432/42552]\n",
      "loss: 0.231761  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.255867 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.359045  [   32/42552]\n",
      "loss: 0.213583  [ 3232/42552]\n",
      "loss: 0.234777  [ 6432/42552]\n",
      "loss: 0.288950  [ 9632/42552]\n",
      "loss: 0.285816  [12832/42552]\n",
      "loss: 0.233708  [16032/42552]\n",
      "loss: 0.153754  [19232/42552]\n",
      "loss: 0.148930  [22432/42552]\n",
      "loss: 0.232996  [25632/42552]\n",
      "loss: 0.494445  [28832/42552]\n",
      "loss: 0.210243  [32032/42552]\n",
      "loss: 0.375809  [35232/42552]\n",
      "loss: 0.259092  [38432/42552]\n",
      "loss: 0.244433  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.238172 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.385045  [   32/42552]\n",
      "loss: 0.278057  [ 3232/42552]\n",
      "loss: 0.214550  [ 6432/42552]\n",
      "loss: 0.273484  [ 9632/42552]\n",
      "loss: 0.215612  [12832/42552]\n",
      "loss: 0.405885  [16032/42552]\n",
      "loss: 0.254135  [19232/42552]\n",
      "loss: 0.238458  [22432/42552]\n",
      "loss: 0.285667  [25632/42552]\n",
      "loss: 0.285707  [28832/42552]\n",
      "loss: 0.408127  [32032/42552]\n",
      "loss: 0.276740  [35232/42552]\n",
      "loss: 0.201145  [38432/42552]\n",
      "loss: 0.202815  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.248644 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.154713  [   32/42552]\n",
      "loss: 0.255815  [ 3232/42552]\n",
      "loss: 0.173763  [ 6432/42552]\n",
      "loss: 0.259513  [ 9632/42552]\n",
      "loss: 0.260217  [12832/42552]\n",
      "loss: 0.168469  [16032/42552]\n",
      "loss: 0.197019  [19232/42552]\n",
      "loss: 0.245458  [22432/42552]\n",
      "loss: 0.175005  [25632/42552]\n",
      "loss: 0.346181  [28832/42552]\n",
      "loss: 0.205291  [32032/42552]\n",
      "loss: 0.270586  [35232/42552]\n",
      "loss: 0.200463  [38432/42552]\n",
      "loss: 0.215527  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.233535 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.298905  [   32/42552]\n",
      "loss: 0.282305  [ 3232/42552]\n",
      "loss: 0.247218  [ 6432/42552]\n",
      "loss: 0.256993  [ 9632/42552]\n",
      "loss: 0.158973  [12832/42552]\n",
      "loss: 0.329379  [16032/42552]\n",
      "loss: 0.200755  [19232/42552]\n",
      "loss: 0.256848  [22432/42552]\n",
      "loss: 0.271947  [25632/42552]\n",
      "loss: 0.182487  [28832/42552]\n",
      "loss: 0.237654  [32032/42552]\n",
      "loss: 0.176429  [35232/42552]\n",
      "loss: 0.290839  [38432/42552]\n",
      "loss: 0.292666  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.232970 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.280522  [   32/42552]\n",
      "loss: 0.268718  [ 3232/42552]\n",
      "loss: 0.334850  [ 6432/42552]\n",
      "loss: 0.186133  [ 9632/42552]\n",
      "loss: 0.194000  [12832/42552]\n",
      "loss: 0.178851  [16032/42552]\n",
      "loss: 0.263216  [19232/42552]\n",
      "loss: 0.486066  [22432/42552]\n",
      "loss: 0.460051  [25632/42552]\n",
      "loss: 0.399218  [28832/42552]\n",
      "loss: 0.349629  [32032/42552]\n",
      "loss: 0.269003  [35232/42552]\n",
      "loss: 0.397971  [38432/42552]\n",
      "loss: 0.319504  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.239951 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.227718  [   32/42552]\n",
      "loss: 0.355533  [ 3232/42552]\n",
      "loss: 0.193708  [ 6432/42552]\n",
      "loss: 0.231084  [ 9632/42552]\n",
      "loss: 0.163418  [12832/42552]\n",
      "loss: 0.275361  [16032/42552]\n",
      "loss: 0.276238  [19232/42552]\n",
      "loss: 0.251081  [22432/42552]\n",
      "loss: 0.195560  [25632/42552]\n",
      "loss: 0.306625  [28832/42552]\n",
      "loss: 0.187704  [32032/42552]\n",
      "loss: 0.334984  [35232/42552]\n",
      "loss: 0.326096  [38432/42552]\n",
      "loss: 0.162275  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.242708 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.202392  [   32/42552]\n",
      "loss: 0.301092  [ 3232/42552]\n",
      "loss: 0.197767  [ 6432/42552]\n",
      "loss: 0.229898  [ 9632/42552]\n",
      "loss: 0.284618  [12832/42552]\n",
      "loss: 0.345243  [16032/42552]\n",
      "loss: 0.214991  [19232/42552]\n",
      "loss: 0.236386  [22432/42552]\n",
      "loss: 0.213609  [25632/42552]\n",
      "loss: 0.188287  [28832/42552]\n",
      "loss: 0.253402  [32032/42552]\n",
      "loss: 0.347469  [35232/42552]\n",
      "loss: 0.300575  [38432/42552]\n",
      "loss: 0.238086  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.233743 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.166399  [   32/42552]\n",
      "loss: 0.213495  [ 3232/42552]\n",
      "loss: 0.243286  [ 6432/42552]\n",
      "loss: 0.257512  [ 9632/42552]\n",
      "loss: 0.530152  [12832/42552]\n",
      "loss: 0.353663  [16032/42552]\n",
      "loss: 0.425207  [19232/42552]\n",
      "loss: 0.240891  [22432/42552]\n",
      "loss: 0.304594  [25632/42552]\n",
      "loss: 0.466151  [28832/42552]\n",
      "loss: 0.248176  [32032/42552]\n",
      "loss: 0.242416  [35232/42552]\n",
      "loss: 0.352657  [38432/42552]\n",
      "loss: 0.213607  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.235317 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.227401  [   32/42552]\n",
      "loss: 0.248331  [ 3232/42552]\n",
      "loss: 0.192010  [ 6432/42552]\n",
      "loss: 0.251407  [ 9632/42552]\n",
      "loss: 0.419623  [12832/42552]\n",
      "loss: 0.230946  [16032/42552]\n",
      "loss: 0.199283  [19232/42552]\n",
      "loss: 0.234266  [22432/42552]\n",
      "loss: 0.381628  [25632/42552]\n",
      "loss: 0.305200  [28832/42552]\n",
      "loss: 0.375595  [32032/42552]\n",
      "loss: 0.318588  [35232/42552]\n",
      "loss: 0.160011  [38432/42552]\n",
      "loss: 0.308737  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.236755 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.287384  [   32/42552]\n",
      "loss: 0.404434  [ 3232/42552]\n",
      "loss: 0.222197  [ 6432/42552]\n",
      "loss: 0.109330  [ 9632/42552]\n",
      "loss: 0.341465  [12832/42552]\n",
      "loss: 0.216883  [16032/42552]\n",
      "loss: 0.246182  [19232/42552]\n",
      "loss: 0.255410  [22432/42552]\n",
      "loss: 0.442239  [25632/42552]\n",
      "loss: 0.413979  [28832/42552]\n",
      "loss: 0.197896  [32032/42552]\n",
      "loss: 0.252272  [35232/42552]\n",
      "loss: 0.207370  [38432/42552]\n",
      "loss: 0.403911  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.237098 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.203578  [   32/42552]\n",
      "loss: 0.294413  [ 3232/42552]\n",
      "loss: 0.504549  [ 6432/42552]\n",
      "loss: 0.437490  [ 9632/42552]\n",
      "loss: 0.224098  [12832/42552]\n",
      "loss: 0.353851  [16032/42552]\n",
      "loss: 0.224760  [19232/42552]\n",
      "loss: 0.365054  [22432/42552]\n",
      "loss: 0.239652  [25632/42552]\n",
      "loss: 0.204091  [28832/42552]\n",
      "loss: 0.472487  [32032/42552]\n",
      "loss: 0.268218  [35232/42552]\n",
      "loss: 0.233629  [38432/42552]\n",
      "loss: 0.189745  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.236211 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.329112  [   32/42552]\n",
      "loss: 0.384314  [ 3232/42552]\n",
      "loss: 0.261321  [ 6432/42552]\n",
      "loss: 0.361436  [ 9632/42552]\n",
      "loss: 0.315434  [12832/42552]\n",
      "loss: 0.286436  [16032/42552]\n",
      "loss: 0.456700  [19232/42552]\n",
      "loss: 0.279244  [22432/42552]\n",
      "loss: 0.303777  [25632/42552]\n",
      "loss: 0.222153  [28832/42552]\n",
      "loss: 0.155256  [32032/42552]\n",
      "loss: 0.230338  [35232/42552]\n",
      "loss: 0.136225  [38432/42552]\n",
      "loss: 0.243808  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.235669 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.200621  [   32/42552]\n",
      "loss: 0.399713  [ 3232/42552]\n",
      "loss: 0.344056  [ 6432/42552]\n",
      "loss: 0.337668  [ 9632/42552]\n",
      "loss: 0.241758  [12832/42552]\n",
      "loss: 0.319073  [16032/42552]\n",
      "loss: 0.244160  [19232/42552]\n",
      "loss: 0.348419  [22432/42552]\n",
      "loss: 0.335542  [25632/42552]\n",
      "loss: 0.233814  [28832/42552]\n",
      "loss: 0.175166  [32032/42552]\n",
      "loss: 0.181377  [35232/42552]\n",
      "loss: 0.286514  [38432/42552]\n",
      "loss: 0.185930  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.238003 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.458568  [   32/42552]\n",
      "loss: 0.220341  [ 3232/42552]\n",
      "loss: 0.291865  [ 6432/42552]\n",
      "loss: 0.239382  [ 9632/42552]\n",
      "loss: 0.248838  [12832/42552]\n",
      "loss: 0.232201  [16032/42552]\n",
      "loss: 0.136966  [19232/42552]\n",
      "loss: 0.276156  [22432/42552]\n",
      "loss: 0.407516  [25632/42552]\n",
      "loss: 0.271468  [28832/42552]\n",
      "loss: 0.262162  [32032/42552]\n",
      "loss: 0.351428  [35232/42552]\n",
      "loss: 0.194913  [38432/42552]\n",
      "loss: 0.182657  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.231421 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.397914  [   32/42552]\n",
      "loss: 0.204676  [ 3232/42552]\n",
      "loss: 0.247692  [ 6432/42552]\n",
      "loss: 0.418301  [ 9632/42552]\n",
      "loss: 0.222014  [12832/42552]\n",
      "loss: 0.283657  [16032/42552]\n",
      "loss: 0.325486  [19232/42552]\n",
      "loss: 0.308038  [22432/42552]\n",
      "loss: 0.371166  [25632/42552]\n",
      "loss: 0.287516  [28832/42552]\n",
      "loss: 0.234554  [32032/42552]\n",
      "loss: 0.149705  [35232/42552]\n",
      "loss: 0.327898  [38432/42552]\n",
      "loss: 0.290330  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.232303 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.259000  [   32/42552]\n",
      "loss: 0.291571  [ 3232/42552]\n",
      "loss: 0.309113  [ 6432/42552]\n",
      "loss: 0.204325  [ 9632/42552]\n",
      "loss: 0.268162  [12832/42552]\n",
      "loss: 0.207208  [16032/42552]\n",
      "loss: 0.344708  [19232/42552]\n",
      "loss: 0.294068  [22432/42552]\n",
      "loss: 0.278009  [25632/42552]\n",
      "loss: 0.262401  [28832/42552]\n",
      "loss: 0.245009  [32032/42552]\n",
      "loss: 0.253510  [35232/42552]\n",
      "loss: 0.162394  [38432/42552]\n",
      "loss: 0.239754  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.237298 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.291381  [   32/42552]\n",
      "loss: 0.284198  [ 3232/42552]\n",
      "loss: 0.312648  [ 6432/42552]\n",
      "loss: 0.357771  [ 9632/42552]\n",
      "loss: 0.337957  [12832/42552]\n",
      "loss: 0.353351  [16032/42552]\n",
      "loss: 0.276738  [19232/42552]\n",
      "loss: 0.197429  [22432/42552]\n",
      "loss: 0.158066  [25632/42552]\n",
      "loss: 0.295246  [28832/42552]\n",
      "loss: 0.251546  [32032/42552]\n",
      "loss: 0.206048  [35232/42552]\n",
      "loss: 0.428442  [38432/42552]\n",
      "loss: 0.195333  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.236979 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.184198  [   32/42552]\n",
      "loss: 0.208597  [ 3232/42552]\n",
      "loss: 0.329346  [ 6432/42552]\n",
      "loss: 0.178333  [ 9632/42552]\n",
      "loss: 0.231149  [12832/42552]\n",
      "loss: 0.209132  [16032/42552]\n",
      "loss: 0.352828  [19232/42552]\n",
      "loss: 0.216765  [22432/42552]\n",
      "loss: 0.284262  [25632/42552]\n",
      "loss: 0.229494  [28832/42552]\n",
      "loss: 0.164449  [32032/42552]\n",
      "loss: 0.293195  [35232/42552]\n",
      "loss: 0.209147  [38432/42552]\n",
      "loss: 0.228855  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.235565 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.283521  [   32/42552]\n",
      "loss: 0.334282  [ 3232/42552]\n",
      "loss: 0.194866  [ 6432/42552]\n",
      "loss: 0.453583  [ 9632/42552]\n",
      "loss: 0.136815  [12832/42552]\n",
      "loss: 0.272217  [16032/42552]\n",
      "loss: 0.203469  [19232/42552]\n",
      "loss: 0.271174  [22432/42552]\n",
      "loss: 0.284210  [25632/42552]\n",
      "loss: 0.296297  [28832/42552]\n",
      "loss: 0.206191  [32032/42552]\n",
      "loss: 0.285395  [35232/42552]\n",
      "loss: 0.297290  [38432/42552]\n",
      "loss: 0.279379  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.244754 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.154383  [   32/42552]\n",
      "loss: 0.301686  [ 3232/42552]\n",
      "loss: 0.379078  [ 6432/42552]\n",
      "loss: 0.310150  [ 9632/42552]\n",
      "loss: 0.192833  [12832/42552]\n",
      "loss: 0.217663  [16032/42552]\n",
      "loss: 0.234883  [19232/42552]\n",
      "loss: 0.212392  [22432/42552]\n",
      "loss: 0.270320  [25632/42552]\n",
      "loss: 0.162258  [28832/42552]\n",
      "loss: 0.231857  [32032/42552]\n",
      "loss: 0.222722  [35232/42552]\n",
      "loss: 0.187889  [38432/42552]\n",
      "loss: 0.250225  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.233570 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.230753  [   32/42552]\n",
      "loss: 0.272537  [ 3232/42552]\n",
      "loss: 0.122161  [ 6432/42552]\n",
      "loss: 0.308169  [ 9632/42552]\n",
      "loss: 0.311207  [12832/42552]\n",
      "loss: 0.229890  [16032/42552]\n",
      "loss: 0.317384  [19232/42552]\n",
      "loss: 0.172753  [22432/42552]\n",
      "loss: 0.220331  [25632/42552]\n",
      "loss: 0.268443  [28832/42552]\n",
      "loss: 0.248480  [32032/42552]\n",
      "loss: 0.233091  [35232/42552]\n",
      "loss: 0.251117  [38432/42552]\n",
      "loss: 0.383920  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.231588 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.248937  [   32/42552]\n",
      "loss: 0.234097  [ 3232/42552]\n",
      "loss: 0.121022  [ 6432/42552]\n",
      "loss: 0.291594  [ 9632/42552]\n",
      "loss: 0.497292  [12832/42552]\n",
      "loss: 0.183594  [16032/42552]\n",
      "loss: 0.371541  [19232/42552]\n",
      "loss: 0.179550  [22432/42552]\n",
      "loss: 0.210811  [25632/42552]\n",
      "loss: 0.222514  [28832/42552]\n",
      "loss: 0.268260  [32032/42552]\n",
      "loss: 0.346946  [35232/42552]\n",
      "loss: 0.314656  [38432/42552]\n",
      "loss: 0.245443  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.235650 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.245113  [   32/42552]\n",
      "loss: 0.257736  [ 3232/42552]\n",
      "loss: 0.236288  [ 6432/42552]\n",
      "loss: 0.209047  [ 9632/42552]\n",
      "loss: 0.301251  [12832/42552]\n",
      "loss: 0.232090  [16032/42552]\n",
      "loss: 0.320201  [19232/42552]\n",
      "loss: 0.170844  [22432/42552]\n",
      "loss: 0.212286  [25632/42552]\n",
      "loss: 0.250752  [28832/42552]\n",
      "loss: 0.545923  [32032/42552]\n",
      "loss: 0.230801  [35232/42552]\n",
      "loss: 0.173204  [38432/42552]\n",
      "loss: 0.348844  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.233298 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.330820  [   32/42552]\n",
      "loss: 0.177104  [ 3232/42552]\n",
      "loss: 0.302203  [ 6432/42552]\n",
      "loss: 0.206244  [ 9632/42552]\n",
      "loss: 0.197064  [12832/42552]\n",
      "loss: 0.221610  [16032/42552]\n",
      "loss: 0.265159  [19232/42552]\n",
      "loss: 0.301379  [22432/42552]\n",
      "loss: 0.264113  [25632/42552]\n",
      "loss: 0.231253  [28832/42552]\n",
      "loss: 0.266234  [32032/42552]\n",
      "loss: 0.269485  [35232/42552]\n",
      "loss: 0.220654  [38432/42552]\n",
      "loss: 0.183072  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.236246 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.169032  [   32/42552]\n",
      "loss: 0.182456  [ 3232/42552]\n",
      "loss: 0.261425  [ 6432/42552]\n",
      "loss: 0.250993  [ 9632/42552]\n",
      "loss: 0.246527  [12832/42552]\n",
      "loss: 0.313763  [16032/42552]\n",
      "loss: 0.174670  [19232/42552]\n",
      "loss: 0.322663  [22432/42552]\n",
      "loss: 0.240440  [25632/42552]\n",
      "loss: 0.250787  [28832/42552]\n",
      "loss: 0.286101  [32032/42552]\n",
      "loss: 0.302756  [35232/42552]\n",
      "loss: 0.263416  [38432/42552]\n",
      "loss: 0.358902  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.228369 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.238842  [   32/42552]\n",
      "loss: 0.232432  [ 3232/42552]\n",
      "loss: 0.341059  [ 6432/42552]\n",
      "loss: 0.175113  [ 9632/42552]\n",
      "loss: 0.157944  [12832/42552]\n",
      "loss: 0.166025  [16032/42552]\n",
      "loss: 0.347274  [19232/42552]\n",
      "loss: 0.297217  [22432/42552]\n",
      "loss: 0.263929  [25632/42552]\n",
      "loss: 0.223659  [28832/42552]\n",
      "loss: 0.176364  [32032/42552]\n",
      "loss: 0.230312  [35232/42552]\n",
      "loss: 0.220364  [38432/42552]\n",
      "loss: 0.249194  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.235711 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.225134  [   32/42552]\n",
      "loss: 0.148352  [ 3232/42552]\n",
      "loss: 0.262196  [ 6432/42552]\n",
      "loss: 0.233285  [ 9632/42552]\n",
      "loss: 0.297400  [12832/42552]\n",
      "loss: 0.276108  [16032/42552]\n",
      "loss: 0.286150  [19232/42552]\n",
      "loss: 0.176310  [22432/42552]\n",
      "loss: 0.289755  [25632/42552]\n",
      "loss: 0.257293  [28832/42552]\n",
      "loss: 0.247169  [32032/42552]\n",
      "loss: 0.248601  [35232/42552]\n",
      "loss: 0.250975  [38432/42552]\n",
      "loss: 0.267524  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.239700 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.370581  [   32/42552]\n",
      "loss: 0.333725  [ 3232/42552]\n",
      "loss: 0.151964  [ 6432/42552]\n",
      "loss: 0.241064  [ 9632/42552]\n",
      "loss: 0.334442  [12832/42552]\n",
      "loss: 0.266602  [16032/42552]\n",
      "loss: 0.325741  [19232/42552]\n",
      "loss: 0.298722  [22432/42552]\n",
      "loss: 0.383248  [25632/42552]\n",
      "loss: 0.141742  [28832/42552]\n",
      "loss: 0.202021  [32032/42552]\n",
      "loss: 0.244593  [35232/42552]\n",
      "loss: 0.299633  [38432/42552]\n",
      "loss: 0.239765  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.232369 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.256309  [   32/42552]\n",
      "loss: 0.392051  [ 3232/42552]\n",
      "loss: 0.390684  [ 6432/42552]\n",
      "loss: 0.265804  [ 9632/42552]\n",
      "loss: 0.328902  [12832/42552]\n",
      "loss: 0.277385  [16032/42552]\n",
      "loss: 0.344707  [19232/42552]\n",
      "loss: 0.259083  [22432/42552]\n",
      "loss: 0.340638  [25632/42552]\n",
      "loss: 0.385056  [28832/42552]\n",
      "loss: 0.275284  [32032/42552]\n",
      "loss: 0.330867  [35232/42552]\n",
      "loss: 0.266714  [38432/42552]\n",
      "loss: 0.302260  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.237272 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.250406  [   32/42552]\n",
      "loss: 0.189605  [ 3232/42552]\n",
      "loss: 0.294484  [ 6432/42552]\n",
      "loss: 0.354755  [ 9632/42552]\n",
      "loss: 0.301929  [12832/42552]\n",
      "loss: 0.470625  [16032/42552]\n",
      "loss: 0.252693  [19232/42552]\n",
      "loss: 0.401624  [22432/42552]\n",
      "loss: 0.266648  [25632/42552]\n",
      "loss: 0.462653  [28832/42552]\n",
      "loss: 0.176067  [32032/42552]\n",
      "loss: 0.134400  [35232/42552]\n",
      "loss: 0.159784  [38432/42552]\n",
      "loss: 0.229634  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.227356 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.224266  [   32/42552]\n",
      "loss: 0.241417  [ 3232/42552]\n",
      "loss: 0.158042  [ 6432/42552]\n",
      "loss: 0.249699  [ 9632/42552]\n",
      "loss: 0.214285  [12832/42552]\n",
      "loss: 0.234064  [16032/42552]\n",
      "loss: 0.201412  [19232/42552]\n",
      "loss: 0.176033  [22432/42552]\n",
      "loss: 0.449439  [25632/42552]\n",
      "loss: 0.183213  [28832/42552]\n",
      "loss: 0.332149  [32032/42552]\n",
      "loss: 0.151451  [35232/42552]\n",
      "loss: 0.215176  [38432/42552]\n",
      "loss: 0.201118  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.229878 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.317450  [   32/42552]\n",
      "loss: 0.261161  [ 3232/42552]\n",
      "loss: 0.307434  [ 6432/42552]\n",
      "loss: 0.201443  [ 9632/42552]\n",
      "loss: 0.210218  [12832/42552]\n",
      "loss: 0.294308  [16032/42552]\n",
      "loss: 0.485308  [19232/42552]\n",
      "loss: 0.356243  [22432/42552]\n",
      "loss: 0.165786  [25632/42552]\n",
      "loss: 0.229571  [28832/42552]\n",
      "loss: 0.206158  [32032/42552]\n",
      "loss: 0.212607  [35232/42552]\n",
      "loss: 0.287981  [38432/42552]\n",
      "loss: 0.339461  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.235784 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.309511  [   32/42552]\n",
      "loss: 0.176671  [ 3232/42552]\n",
      "loss: 0.171399  [ 6432/42552]\n",
      "loss: 0.323398  [ 9632/42552]\n",
      "loss: 0.253581  [12832/42552]\n",
      "loss: 0.167104  [16032/42552]\n",
      "loss: 0.331454  [19232/42552]\n",
      "loss: 0.300200  [22432/42552]\n",
      "loss: 0.291103  [25632/42552]\n",
      "loss: 0.266023  [28832/42552]\n",
      "loss: 0.226811  [32032/42552]\n",
      "loss: 0.237684  [35232/42552]\n",
      "loss: 0.293846  [38432/42552]\n",
      "loss: 0.298933  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.241076 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.445878  [   32/42552]\n",
      "loss: 0.370467  [ 3232/42552]\n",
      "loss: 0.233226  [ 6432/42552]\n",
      "loss: 0.311932  [ 9632/42552]\n",
      "loss: 0.316878  [12832/42552]\n",
      "loss: 0.300472  [16032/42552]\n",
      "loss: 0.235321  [19232/42552]\n",
      "loss: 0.252659  [22432/42552]\n",
      "loss: 0.232622  [25632/42552]\n",
      "loss: 0.272514  [28832/42552]\n",
      "loss: 0.288659  [32032/42552]\n",
      "loss: 0.252731  [35232/42552]\n",
      "loss: 0.428413  [38432/42552]\n",
      "loss: 0.213033  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.230913 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.173375  [   32/42552]\n",
      "loss: 0.185284  [ 3232/42552]\n",
      "loss: 0.215438  [ 6432/42552]\n",
      "loss: 0.491646  [ 9632/42552]\n",
      "loss: 0.112920  [12832/42552]\n",
      "loss: 0.213756  [16032/42552]\n",
      "loss: 0.158920  [19232/42552]\n",
      "loss: 0.214016  [22432/42552]\n",
      "loss: 0.263980  [25632/42552]\n",
      "loss: 0.279420  [28832/42552]\n",
      "loss: 0.531439  [32032/42552]\n",
      "loss: 0.411288  [35232/42552]\n",
      "loss: 0.191210  [38432/42552]\n",
      "loss: 0.248905  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.241874 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.198923  [   32/42552]\n",
      "loss: 0.368188  [ 3232/42552]\n",
      "loss: 0.279342  [ 6432/42552]\n",
      "loss: 0.307744  [ 9632/42552]\n",
      "loss: 0.243735  [12832/42552]\n",
      "loss: 0.379598  [16032/42552]\n",
      "loss: 0.262808  [19232/42552]\n",
      "loss: 0.145642  [22432/42552]\n",
      "loss: 0.455448  [25632/42552]\n",
      "loss: 0.353473  [28832/42552]\n",
      "loss: 0.395100  [32032/42552]\n",
      "loss: 0.134946  [35232/42552]\n",
      "loss: 0.231936  [38432/42552]\n",
      "loss: 0.306009  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.238425 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.274131  [   32/42552]\n",
      "loss: 0.227422  [ 3232/42552]\n",
      "loss: 0.252288  [ 6432/42552]\n",
      "loss: 0.362455  [ 9632/42552]\n",
      "loss: 0.382834  [12832/42552]\n",
      "loss: 0.213925  [16032/42552]\n",
      "loss: 0.491410  [19232/42552]\n",
      "loss: 0.500791  [22432/42552]\n",
      "loss: 0.361080  [25632/42552]\n",
      "loss: 0.294581  [28832/42552]\n",
      "loss: 0.234865  [32032/42552]\n",
      "loss: 0.443895  [35232/42552]\n",
      "loss: 0.149845  [38432/42552]\n",
      "loss: 0.323424  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.236749 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.275922  [   32/42552]\n",
      "loss: 0.246879  [ 3232/42552]\n",
      "loss: 0.180320  [ 6432/42552]\n",
      "loss: 0.333713  [ 9632/42552]\n",
      "loss: 0.212176  [12832/42552]\n",
      "loss: 0.367171  [16032/42552]\n",
      "loss: 0.364566  [19232/42552]\n",
      "loss: 0.253036  [22432/42552]\n",
      "loss: 0.313996  [25632/42552]\n",
      "loss: 0.355861  [28832/42552]\n",
      "loss: 0.250195  [32032/42552]\n",
      "loss: 0.324487  [35232/42552]\n",
      "loss: 0.238032  [38432/42552]\n",
      "loss: 0.279246  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.227373 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.365285  [   32/42552]\n",
      "loss: 0.222902  [ 3232/42552]\n",
      "loss: 0.428987  [ 6432/42552]\n",
      "loss: 0.329909  [ 9632/42552]\n",
      "loss: 0.181891  [12832/42552]\n",
      "loss: 0.157260  [16032/42552]\n",
      "loss: 0.153591  [19232/42552]\n",
      "loss: 0.219156  [22432/42552]\n",
      "loss: 0.393196  [25632/42552]\n",
      "loss: 0.272133  [28832/42552]\n",
      "loss: 0.142722  [32032/42552]\n",
      "loss: 0.229637  [35232/42552]\n",
      "loss: 0.237752  [38432/42552]\n",
      "loss: 0.157231  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.238696 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.243693  [   32/42552]\n",
      "loss: 0.198277  [ 3232/42552]\n",
      "loss: 0.092431  [ 6432/42552]\n",
      "loss: 0.275135  [ 9632/42552]\n",
      "loss: 0.199709  [12832/42552]\n",
      "loss: 0.212636  [16032/42552]\n",
      "loss: 0.317629  [19232/42552]\n",
      "loss: 0.187702  [22432/42552]\n",
      "loss: 0.395037  [25632/42552]\n",
      "loss: 0.356475  [28832/42552]\n",
      "loss: 0.225939  [32032/42552]\n",
      "loss: 0.232231  [35232/42552]\n",
      "loss: 0.262400  [38432/42552]\n",
      "loss: 0.205050  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.229255 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.357245  [   32/42552]\n",
      "loss: 0.295876  [ 3232/42552]\n",
      "loss: 0.325244  [ 6432/42552]\n",
      "loss: 0.219359  [ 9632/42552]\n",
      "loss: 0.215446  [12832/42552]\n",
      "loss: 0.315205  [16032/42552]\n",
      "loss: 0.194867  [19232/42552]\n",
      "loss: 0.262227  [22432/42552]\n",
      "loss: 0.297162  [25632/42552]\n",
      "loss: 0.258661  [28832/42552]\n",
      "loss: 0.229455  [32032/42552]\n",
      "loss: 0.258254  [35232/42552]\n",
      "loss: 0.436480  [38432/42552]\n",
      "loss: 0.248532  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.231616 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.409687  [   32/42552]\n",
      "loss: 0.375537  [ 3232/42552]\n",
      "loss: 0.213275  [ 6432/42552]\n",
      "loss: 0.227812  [ 9632/42552]\n",
      "loss: 0.190153  [12832/42552]\n",
      "loss: 0.247715  [16032/42552]\n",
      "loss: 0.213781  [19232/42552]\n",
      "loss: 0.330344  [22432/42552]\n",
      "loss: 0.301316  [25632/42552]\n",
      "loss: 0.301929  [28832/42552]\n",
      "loss: 0.268610  [32032/42552]\n",
      "loss: 0.158305  [35232/42552]\n",
      "loss: 0.249960  [38432/42552]\n",
      "loss: 0.379554  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.233317 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.233043  [   32/42552]\n",
      "loss: 0.179970  [ 3232/42552]\n",
      "loss: 0.307530  [ 6432/42552]\n",
      "loss: 0.254906  [ 9632/42552]\n",
      "loss: 0.278178  [12832/42552]\n",
      "loss: 0.251667  [16032/42552]\n",
      "loss: 0.276866  [19232/42552]\n",
      "loss: 0.383259  [22432/42552]\n",
      "loss: 0.197008  [25632/42552]\n",
      "loss: 0.231755  [28832/42552]\n",
      "loss: 0.268442  [32032/42552]\n",
      "loss: 0.329521  [35232/42552]\n",
      "loss: 0.337834  [38432/42552]\n",
      "loss: 0.236222  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.238328 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.324239  [   32/42552]\n",
      "loss: 0.347075  [ 3232/42552]\n",
      "loss: 0.176338  [ 6432/42552]\n",
      "loss: 0.280781  [ 9632/42552]\n",
      "loss: 0.199562  [12832/42552]\n",
      "loss: 0.236165  [16032/42552]\n",
      "loss: 0.322613  [19232/42552]\n",
      "loss: 0.252899  [22432/42552]\n",
      "loss: 0.242797  [25632/42552]\n",
      "loss: 0.286546  [28832/42552]\n",
      "loss: 0.364751  [32032/42552]\n",
      "loss: 0.138390  [35232/42552]\n",
      "loss: 0.164390  [38432/42552]\n",
      "loss: 0.428621  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.234981 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.206329  [   32/42552]\n",
      "loss: 0.195642  [ 3232/42552]\n",
      "loss: 0.225962  [ 6432/42552]\n",
      "loss: 0.238987  [ 9632/42552]\n",
      "loss: 0.343343  [12832/42552]\n",
      "loss: 0.264312  [16032/42552]\n",
      "loss: 0.163570  [19232/42552]\n",
      "loss: 0.206533  [22432/42552]\n",
      "loss: 0.290176  [25632/42552]\n",
      "loss: 0.259839  [28832/42552]\n",
      "loss: 0.219745  [32032/42552]\n",
      "loss: 0.485692  [35232/42552]\n",
      "loss: 0.160351  [38432/42552]\n",
      "loss: 0.307617  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.230313 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.289382  [   32/42552]\n",
      "loss: 0.301274  [ 3232/42552]\n",
      "loss: 0.341394  [ 6432/42552]\n",
      "loss: 0.373307  [ 9632/42552]\n",
      "loss: 0.382678  [12832/42552]\n",
      "loss: 0.311055  [16032/42552]\n",
      "loss: 0.158092  [19232/42552]\n",
      "loss: 0.410079  [22432/42552]\n",
      "loss: 0.307713  [25632/42552]\n",
      "loss: 0.368972  [28832/42552]\n",
      "loss: 0.271238  [32032/42552]\n",
      "loss: 0.164253  [35232/42552]\n",
      "loss: 0.277444  [38432/42552]\n",
      "loss: 0.400105  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.229290 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.260886  [   32/42552]\n",
      "loss: 0.362668  [ 3232/42552]\n",
      "loss: 0.296119  [ 6432/42552]\n",
      "loss: 0.255599  [ 9632/42552]\n",
      "loss: 0.271722  [12832/42552]\n",
      "loss: 0.318814  [16032/42552]\n",
      "loss: 0.412796  [19232/42552]\n",
      "loss: 0.312174  [22432/42552]\n",
      "loss: 0.249032  [25632/42552]\n",
      "loss: 0.252710  [28832/42552]\n",
      "loss: 0.230003  [32032/42552]\n",
      "loss: 0.224028  [35232/42552]\n",
      "loss: 0.295356  [38432/42552]\n",
      "loss: 0.317418  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.250884 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.278904  [   32/42552]\n",
      "loss: 0.313792  [ 3232/42552]\n",
      "loss: 0.165234  [ 6432/42552]\n",
      "loss: 0.306048  [ 9632/42552]\n",
      "loss: 0.215515  [12832/42552]\n",
      "loss: 0.214857  [16032/42552]\n",
      "loss: 0.365461  [19232/42552]\n",
      "loss: 0.286530  [22432/42552]\n",
      "loss: 0.202350  [25632/42552]\n",
      "loss: 0.292300  [28832/42552]\n",
      "loss: 0.176936  [32032/42552]\n",
      "loss: 0.327517  [35232/42552]\n",
      "loss: 0.247469  [38432/42552]\n",
      "loss: 0.265421  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.231889 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.237975  [   32/42552]\n",
      "loss: 0.311669  [ 3232/42552]\n",
      "loss: 0.188650  [ 6432/42552]\n",
      "loss: 0.223875  [ 9632/42552]\n",
      "loss: 0.217497  [12832/42552]\n",
      "loss: 0.289470  [16032/42552]\n",
      "loss: 0.165022  [19232/42552]\n",
      "loss: 0.255016  [22432/42552]\n",
      "loss: 0.195018  [25632/42552]\n",
      "loss: 0.280160  [28832/42552]\n",
      "loss: 0.223424  [32032/42552]\n",
      "loss: 0.285583  [35232/42552]\n",
      "loss: 0.262031  [38432/42552]\n",
      "loss: 0.239015  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.229326 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.191210  [   32/42552]\n",
      "loss: 0.374811  [ 3232/42552]\n",
      "loss: 0.382787  [ 6432/42552]\n",
      "loss: 0.459148  [ 9632/42552]\n",
      "loss: 0.237054  [12832/42552]\n",
      "loss: 0.313314  [16032/42552]\n",
      "loss: 0.269287  [19232/42552]\n",
      "loss: 0.186710  [22432/42552]\n",
      "loss: 0.225766  [25632/42552]\n",
      "loss: 0.195704  [28832/42552]\n",
      "loss: 0.245684  [32032/42552]\n",
      "loss: 0.331213  [35232/42552]\n",
      "loss: 0.452593  [38432/42552]\n",
      "loss: 0.258998  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.232847 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.327743  [   32/42552]\n",
      "loss: 0.226475  [ 3232/42552]\n",
      "loss: 0.364267  [ 6432/42552]\n",
      "loss: 0.280001  [ 9632/42552]\n",
      "loss: 0.174854  [12832/42552]\n",
      "loss: 0.187409  [16032/42552]\n",
      "loss: 0.219271  [19232/42552]\n",
      "loss: 0.281720  [22432/42552]\n",
      "loss: 0.431315  [25632/42552]\n",
      "loss: 0.302872  [28832/42552]\n",
      "loss: 0.380379  [32032/42552]\n",
      "loss: 0.290770  [35232/42552]\n",
      "loss: 0.170893  [38432/42552]\n",
      "loss: 0.180718  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.236662 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.277934  [   32/42552]\n",
      "loss: 0.334821  [ 3232/42552]\n",
      "loss: 0.284239  [ 6432/42552]\n",
      "loss: 0.202188  [ 9632/42552]\n",
      "loss: 0.164697  [12832/42552]\n",
      "loss: 0.184294  [16032/42552]\n",
      "loss: 0.207324  [19232/42552]\n",
      "loss: 0.276301  [22432/42552]\n",
      "loss: 0.177491  [25632/42552]\n",
      "loss: 0.411583  [28832/42552]\n",
      "loss: 0.233597  [32032/42552]\n",
      "loss: 0.338348  [35232/42552]\n",
      "loss: 0.229370  [38432/42552]\n",
      "loss: 0.272936  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.232125 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.246980  [   32/42552]\n",
      "loss: 0.221499  [ 3232/42552]\n",
      "loss: 0.154286  [ 6432/42552]\n",
      "loss: 0.449947  [ 9632/42552]\n",
      "loss: 0.345202  [12832/42552]\n",
      "loss: 0.215311  [16032/42552]\n",
      "loss: 0.223705  [19232/42552]\n",
      "loss: 0.323390  [22432/42552]\n",
      "loss: 0.200273  [25632/42552]\n",
      "loss: 0.319347  [28832/42552]\n",
      "loss: 0.252643  [32032/42552]\n",
      "loss: 0.261265  [35232/42552]\n",
      "loss: 0.273302  [38432/42552]\n",
      "loss: 0.243668  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.234150 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.230380  [   32/42552]\n",
      "loss: 0.232984  [ 3232/42552]\n",
      "loss: 0.181200  [ 6432/42552]\n",
      "loss: 0.206404  [ 9632/42552]\n",
      "loss: 0.347906  [12832/42552]\n",
      "loss: 0.388826  [16032/42552]\n",
      "loss: 0.284709  [19232/42552]\n",
      "loss: 0.148958  [22432/42552]\n",
      "loss: 0.234409  [25632/42552]\n",
      "loss: 0.187807  [28832/42552]\n",
      "loss: 0.240531  [32032/42552]\n",
      "loss: 0.204148  [35232/42552]\n",
      "loss: 0.203880  [38432/42552]\n",
      "loss: 0.199961  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.235305 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.183792  [   32/42552]\n",
      "loss: 0.274557  [ 3232/42552]\n",
      "loss: 0.327830  [ 6432/42552]\n",
      "loss: 0.248129  [ 9632/42552]\n",
      "loss: 0.206658  [12832/42552]\n",
      "loss: 0.339622  [16032/42552]\n",
      "loss: 0.198276  [19232/42552]\n",
      "loss: 0.209960  [22432/42552]\n",
      "loss: 0.383240  [25632/42552]\n",
      "loss: 0.214568  [28832/42552]\n",
      "loss: 0.234638  [32032/42552]\n",
      "loss: 0.214060  [35232/42552]\n",
      "loss: 0.142381  [38432/42552]\n",
      "loss: 0.217642  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.239325 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.328191  [   32/42552]\n",
      "loss: 0.216670  [ 3232/42552]\n",
      "loss: 0.352530  [ 6432/42552]\n",
      "loss: 0.333515  [ 9632/42552]\n",
      "loss: 0.159408  [12832/42552]\n",
      "loss: 0.264191  [16032/42552]\n",
      "loss: 0.220735  [19232/42552]\n",
      "loss: 0.306038  [22432/42552]\n",
      "loss: 0.378781  [25632/42552]\n",
      "loss: 0.217484  [28832/42552]\n",
      "loss: 0.191092  [32032/42552]\n",
      "loss: 0.234714  [35232/42552]\n",
      "loss: 0.334319  [38432/42552]\n",
      "loss: 0.168410  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.238152 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.442510  [   32/42552]\n",
      "loss: 0.294266  [ 3232/42552]\n",
      "loss: 0.250983  [ 6432/42552]\n",
      "loss: 0.339770  [ 9632/42552]\n",
      "loss: 0.157148  [12832/42552]\n",
      "loss: 0.230936  [16032/42552]\n",
      "loss: 0.345726  [19232/42552]\n",
      "loss: 0.195408  [22432/42552]\n",
      "loss: 0.250003  [25632/42552]\n",
      "loss: 0.259820  [28832/42552]\n",
      "loss: 0.266970  [32032/42552]\n",
      "loss: 0.179584  [35232/42552]\n",
      "loss: 0.250032  [38432/42552]\n",
      "loss: 0.250391  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.229567 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.238170  [   32/42552]\n",
      "loss: 0.290268  [ 3232/42552]\n",
      "loss: 0.251391  [ 6432/42552]\n",
      "loss: 0.219789  [ 9632/42552]\n",
      "loss: 0.274424  [12832/42552]\n",
      "loss: 0.221399  [16032/42552]\n",
      "loss: 0.387316  [19232/42552]\n",
      "loss: 0.408601  [22432/42552]\n",
      "loss: 0.230932  [25632/42552]\n",
      "loss: 0.235714  [28832/42552]\n",
      "loss: 0.360154  [32032/42552]\n",
      "loss: 0.340321  [35232/42552]\n",
      "loss: 0.233817  [38432/42552]\n",
      "loss: 0.154571  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.236699 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.202829  [   32/42552]\n",
      "loss: 0.260841  [ 3232/42552]\n",
      "loss: 0.184580  [ 6432/42552]\n",
      "loss: 0.202309  [ 9632/42552]\n",
      "loss: 0.200045  [12832/42552]\n",
      "loss: 0.241727  [16032/42552]\n",
      "loss: 0.288777  [19232/42552]\n",
      "loss: 0.200522  [22432/42552]\n",
      "loss: 0.299483  [25632/42552]\n",
      "loss: 0.321532  [28832/42552]\n",
      "loss: 0.358159  [32032/42552]\n",
      "loss: 0.212684  [35232/42552]\n",
      "loss: 0.261392  [38432/42552]\n",
      "loss: 0.183603  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.231001 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.248530  [   32/42552]\n",
      "loss: 0.210165  [ 3232/42552]\n",
      "loss: 0.299607  [ 6432/42552]\n",
      "loss: 0.227456  [ 9632/42552]\n",
      "loss: 0.219798  [12832/42552]\n",
      "loss: 0.218888  [16032/42552]\n",
      "loss: 0.311923  [19232/42552]\n",
      "loss: 0.407938  [22432/42552]\n",
      "loss: 0.326624  [25632/42552]\n",
      "loss: 0.254002  [28832/42552]\n",
      "loss: 0.362184  [32032/42552]\n",
      "loss: 0.359239  [35232/42552]\n",
      "loss: 0.281669  [38432/42552]\n",
      "loss: 0.262330  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.228038 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.236428  [   32/42552]\n",
      "loss: 0.247393  [ 3232/42552]\n",
      "loss: 0.246090  [ 6432/42552]\n",
      "loss: 0.387866  [ 9632/42552]\n",
      "loss: 0.262892  [12832/42552]\n",
      "loss: 0.211251  [16032/42552]\n",
      "loss: 0.314673  [19232/42552]\n",
      "loss: 0.255476  [22432/42552]\n",
      "loss: 0.337543  [25632/42552]\n",
      "loss: 0.190514  [28832/42552]\n",
      "loss: 0.309003  [32032/42552]\n",
      "loss: 0.375467  [35232/42552]\n",
      "loss: 0.283244  [38432/42552]\n",
      "loss: 0.190285  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.228778 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.217467  [   32/42552]\n",
      "loss: 0.279554  [ 3232/42552]\n",
      "loss: 0.340349  [ 6432/42552]\n",
      "loss: 0.318270  [ 9632/42552]\n",
      "loss: 0.409934  [12832/42552]\n",
      "loss: 0.319463  [16032/42552]\n",
      "loss: 0.231161  [19232/42552]\n",
      "loss: 0.333436  [22432/42552]\n",
      "loss: 0.358591  [25632/42552]\n",
      "loss: 0.287427  [28832/42552]\n",
      "loss: 0.261656  [32032/42552]\n",
      "loss: 0.224705  [35232/42552]\n",
      "loss: 0.256947  [38432/42552]\n",
      "loss: 0.238610  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.239593 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.413831  [   32/42552]\n",
      "loss: 0.246770  [ 3232/42552]\n",
      "loss: 0.225596  [ 6432/42552]\n",
      "loss: 0.187773  [ 9632/42552]\n",
      "loss: 0.389975  [12832/42552]\n",
      "loss: 0.152424  [16032/42552]\n",
      "loss: 0.217248  [19232/42552]\n",
      "loss: 0.235702  [22432/42552]\n",
      "loss: 0.183445  [25632/42552]\n",
      "loss: 0.171760  [28832/42552]\n",
      "loss: 0.152841  [32032/42552]\n",
      "loss: 0.245157  [35232/42552]\n",
      "loss: 0.343013  [38432/42552]\n",
      "loss: 0.278533  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.228134 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.220148  [   32/42552]\n",
      "loss: 0.255242  [ 3232/42552]\n",
      "loss: 0.178815  [ 6432/42552]\n",
      "loss: 0.266733  [ 9632/42552]\n",
      "loss: 0.280032  [12832/42552]\n",
      "loss: 0.325193  [16032/42552]\n",
      "loss: 0.136215  [19232/42552]\n",
      "loss: 0.269773  [22432/42552]\n",
      "loss: 0.194385  [25632/42552]\n",
      "loss: 0.281901  [28832/42552]\n",
      "loss: 0.247520  [32032/42552]\n",
      "loss: 0.156862  [35232/42552]\n",
      "loss: 0.148288  [38432/42552]\n",
      "loss: 0.316201  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.238375 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.275646  [   32/42552]\n",
      "loss: 0.228084  [ 3232/42552]\n",
      "loss: 0.227747  [ 6432/42552]\n",
      "loss: 0.639896  [ 9632/42552]\n",
      "loss: 0.314190  [12832/42552]\n",
      "loss: 0.275664  [16032/42552]\n",
      "loss: 0.258553  [19232/42552]\n",
      "loss: 0.287146  [22432/42552]\n",
      "loss: 0.242826  [25632/42552]\n",
      "loss: 0.306085  [28832/42552]\n",
      "loss: 0.220573  [32032/42552]\n",
      "loss: 0.318230  [35232/42552]\n",
      "loss: 0.177974  [38432/42552]\n",
      "loss: 0.174159  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.229384 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.548806  [   32/42552]\n",
      "loss: 0.454369  [ 3232/42552]\n",
      "loss: 0.252517  [ 6432/42552]\n",
      "loss: 0.238246  [ 9632/42552]\n",
      "loss: 0.315611  [12832/42552]\n",
      "loss: 0.267898  [16032/42552]\n",
      "loss: 0.329142  [19232/42552]\n",
      "loss: 0.178646  [22432/42552]\n",
      "loss: 0.291650  [25632/42552]\n",
      "loss: 0.240879  [28832/42552]\n",
      "loss: 0.281735  [32032/42552]\n",
      "loss: 0.236114  [35232/42552]\n",
      "loss: 0.306968  [38432/42552]\n",
      "loss: 0.199561  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.248539 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.325076  [   32/42552]\n",
      "loss: 0.300845  [ 3232/42552]\n",
      "loss: 0.246812  [ 6432/42552]\n",
      "loss: 0.246425  [ 9632/42552]\n",
      "loss: 0.136468  [12832/42552]\n",
      "loss: 0.258005  [16032/42552]\n",
      "loss: 0.241134  [19232/42552]\n",
      "loss: 0.256031  [22432/42552]\n",
      "loss: 0.322590  [25632/42552]\n",
      "loss: 0.227809  [28832/42552]\n",
      "loss: 0.311595  [32032/42552]\n",
      "loss: 0.211428  [35232/42552]\n",
      "loss: 0.243408  [38432/42552]\n",
      "loss: 0.208642  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.238564 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.246503  [   32/42552]\n",
      "loss: 0.312835  [ 3232/42552]\n",
      "loss: 0.299211  [ 6432/42552]\n",
      "loss: 0.615138  [ 9632/42552]\n",
      "loss: 0.265708  [12832/42552]\n",
      "loss: 0.174921  [16032/42552]\n",
      "loss: 0.339070  [19232/42552]\n",
      "loss: 0.249436  [22432/42552]\n",
      "loss: 0.227035  [25632/42552]\n",
      "loss: 0.173025  [28832/42552]\n",
      "loss: 0.232527  [32032/42552]\n",
      "loss: 0.369090  [35232/42552]\n",
      "loss: 0.151641  [38432/42552]\n",
      "loss: 0.271105  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.236823 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.315450  [   32/42552]\n",
      "loss: 0.226677  [ 3232/42552]\n",
      "loss: 0.242472  [ 6432/42552]\n",
      "loss: 0.335867  [ 9632/42552]\n",
      "loss: 0.319279  [12832/42552]\n",
      "loss: 0.281061  [16032/42552]\n",
      "loss: 0.219638  [19232/42552]\n",
      "loss: 0.254714  [22432/42552]\n",
      "loss: 0.210423  [25632/42552]\n",
      "loss: 0.177838  [28832/42552]\n",
      "loss: 0.179295  [32032/42552]\n",
      "loss: 0.356177  [35232/42552]\n",
      "loss: 0.261039  [38432/42552]\n",
      "loss: 0.210627  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.237825 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.308414  [   32/42552]\n",
      "loss: 0.366646  [ 3232/42552]\n",
      "loss: 0.324240  [ 6432/42552]\n",
      "loss: 0.228739  [ 9632/42552]\n",
      "loss: 0.307416  [12832/42552]\n",
      "loss: 0.415391  [16032/42552]\n",
      "loss: 0.241065  [19232/42552]\n",
      "loss: 0.274494  [22432/42552]\n",
      "loss: 0.269099  [25632/42552]\n",
      "loss: 0.318443  [28832/42552]\n",
      "loss: 0.168501  [32032/42552]\n",
      "loss: 0.256763  [35232/42552]\n",
      "loss: 0.223201  [38432/42552]\n",
      "loss: 0.459740  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.236598 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.388275  [   32/42552]\n",
      "loss: 0.221882  [ 3232/42552]\n",
      "loss: 0.264693  [ 6432/42552]\n",
      "loss: 0.393876  [ 9632/42552]\n",
      "loss: 0.186062  [12832/42552]\n",
      "loss: 0.346523  [16032/42552]\n",
      "loss: 0.426732  [19232/42552]\n",
      "loss: 0.274315  [22432/42552]\n",
      "loss: 0.373154  [25632/42552]\n",
      "loss: 0.194845  [28832/42552]\n",
      "loss: 0.247854  [32032/42552]\n",
      "loss: 0.263428  [35232/42552]\n",
      "loss: 0.394276  [38432/42552]\n",
      "loss: 0.155358  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.225562 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.253532  [   32/42552]\n",
      "loss: 0.430502  [ 3232/42552]\n",
      "loss: 0.446272  [ 6432/42552]\n",
      "loss: 0.256489  [ 9632/42552]\n",
      "loss: 0.242884  [12832/42552]\n",
      "loss: 0.209423  [16032/42552]\n",
      "loss: 0.328790  [19232/42552]\n",
      "loss: 0.312588  [22432/42552]\n",
      "loss: 0.352733  [25632/42552]\n",
      "loss: 0.249271  [28832/42552]\n",
      "loss: 0.155844  [32032/42552]\n",
      "loss: 0.170678  [35232/42552]\n",
      "loss: 0.228311  [38432/42552]\n",
      "loss: 0.350831  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.229189 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.278578  [   32/42552]\n",
      "loss: 0.199676  [ 3232/42552]\n",
      "loss: 0.175788  [ 6432/42552]\n",
      "loss: 0.281432  [ 9632/42552]\n",
      "loss: 0.244778  [12832/42552]\n",
      "loss: 0.369423  [16032/42552]\n",
      "loss: 0.361539  [19232/42552]\n",
      "loss: 0.315626  [22432/42552]\n",
      "loss: 0.377161  [25632/42552]\n",
      "loss: 0.176363  [28832/42552]\n",
      "loss: 0.329998  [32032/42552]\n",
      "loss: 0.253654  [35232/42552]\n",
      "loss: 0.314298  [38432/42552]\n",
      "loss: 0.341750  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.229415 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.374189  [   32/42552]\n",
      "loss: 0.471566  [ 3232/42552]\n",
      "loss: 0.202405  [ 6432/42552]\n",
      "loss: 0.193469  [ 9632/42552]\n",
      "loss: 0.203623  [12832/42552]\n",
      "loss: 0.269103  [16032/42552]\n",
      "loss: 0.285209  [19232/42552]\n",
      "loss: 0.276539  [22432/42552]\n",
      "loss: 0.212218  [25632/42552]\n",
      "loss: 0.440056  [28832/42552]\n",
      "loss: 0.428706  [32032/42552]\n",
      "loss: 0.188480  [35232/42552]\n",
      "loss: 0.307697  [38432/42552]\n",
      "loss: 0.253894  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.231056 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.290871  [   32/42552]\n",
      "loss: 0.250225  [ 3232/42552]\n",
      "loss: 0.406660  [ 6432/42552]\n",
      "loss: 0.281763  [ 9632/42552]\n",
      "loss: 0.249407  [12832/42552]\n",
      "loss: 0.194218  [16032/42552]\n",
      "loss: 0.218367  [19232/42552]\n",
      "loss: 0.229060  [22432/42552]\n",
      "loss: 0.230546  [25632/42552]\n",
      "loss: 0.447887  [28832/42552]\n",
      "loss: 0.383621  [32032/42552]\n",
      "loss: 0.269934  [35232/42552]\n",
      "loss: 0.305834  [38432/42552]\n",
      "loss: 0.393790  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.225164 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.256885  [   32/42552]\n",
      "loss: 0.180821  [ 3232/42552]\n",
      "loss: 0.215625  [ 6432/42552]\n",
      "loss: 0.373111  [ 9632/42552]\n",
      "loss: 0.247713  [12832/42552]\n",
      "loss: 0.211132  [16032/42552]\n",
      "loss: 0.224404  [19232/42552]\n",
      "loss: 0.252390  [22432/42552]\n",
      "loss: 0.112551  [25632/42552]\n",
      "loss: 0.301925  [28832/42552]\n",
      "loss: 0.258755  [32032/42552]\n",
      "loss: 0.214248  [35232/42552]\n",
      "loss: 0.137695  [38432/42552]\n",
      "loss: 0.322124  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.228225 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.464636  [   32/42552]\n",
      "loss: 0.249513  [ 3232/42552]\n",
      "loss: 0.273023  [ 6432/42552]\n",
      "loss: 0.209988  [ 9632/42552]\n",
      "loss: 0.226594  [12832/42552]\n",
      "loss: 0.223137  [16032/42552]\n",
      "loss: 0.178192  [19232/42552]\n",
      "loss: 0.326519  [22432/42552]\n",
      "loss: 0.284652  [25632/42552]\n",
      "loss: 0.258151  [28832/42552]\n",
      "loss: 0.223140  [32032/42552]\n",
      "loss: 0.327957  [35232/42552]\n",
      "loss: 0.219791  [38432/42552]\n",
      "loss: 0.204506  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.232002 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.350578  [   32/42552]\n",
      "loss: 0.256563  [ 3232/42552]\n",
      "loss: 0.357631  [ 6432/42552]\n",
      "loss: 0.223253  [ 9632/42552]\n",
      "loss: 0.295803  [12832/42552]\n",
      "loss: 0.304835  [16032/42552]\n",
      "loss: 0.262081  [19232/42552]\n",
      "loss: 0.205535  [22432/42552]\n",
      "loss: 0.327609  [25632/42552]\n",
      "loss: 0.345150  [28832/42552]\n",
      "loss: 0.348668  [32032/42552]\n",
      "loss: 0.383137  [35232/42552]\n",
      "loss: 0.296158  [38432/42552]\n",
      "loss: 0.392146  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.230752 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.194434  [   32/42552]\n",
      "loss: 0.331448  [ 3232/42552]\n",
      "loss: 0.347929  [ 6432/42552]\n",
      "loss: 0.240051  [ 9632/42552]\n",
      "loss: 0.238563  [12832/42552]\n",
      "loss: 0.274392  [16032/42552]\n",
      "loss: 0.263184  [19232/42552]\n",
      "loss: 0.236929  [22432/42552]\n",
      "loss: 0.395923  [25632/42552]\n",
      "loss: 0.264469  [28832/42552]\n",
      "loss: 0.266826  [32032/42552]\n",
      "loss: 0.413403  [35232/42552]\n",
      "loss: 0.219132  [38432/42552]\n",
      "loss: 0.363218  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.233126 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.253478  [   32/42552]\n",
      "loss: 0.336273  [ 3232/42552]\n",
      "loss: 0.295263  [ 6432/42552]\n",
      "loss: 0.293701  [ 9632/42552]\n",
      "loss: 0.211909  [12832/42552]\n",
      "loss: 0.298740  [16032/42552]\n",
      "loss: 0.412594  [19232/42552]\n",
      "loss: 0.338572  [22432/42552]\n",
      "loss: 0.162686  [25632/42552]\n",
      "loss: 0.263771  [28832/42552]\n",
      "loss: 0.178985  [32032/42552]\n",
      "loss: 0.338452  [35232/42552]\n",
      "loss: 0.255957  [38432/42552]\n",
      "loss: 0.220798  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.232419 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.194754  [   32/42552]\n",
      "loss: 0.296739  [ 3232/42552]\n",
      "loss: 0.251831  [ 6432/42552]\n",
      "loss: 0.211773  [ 9632/42552]\n",
      "loss: 0.271357  [12832/42552]\n",
      "loss: 0.280025  [16032/42552]\n",
      "loss: 0.376159  [19232/42552]\n",
      "loss: 0.242357  [22432/42552]\n",
      "loss: 0.309697  [25632/42552]\n",
      "loss: 0.188332  [28832/42552]\n",
      "loss: 0.212235  [32032/42552]\n",
      "loss: 0.235477  [35232/42552]\n",
      "loss: 0.171066  [38432/42552]\n",
      "loss: 0.244498  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.236500 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.263642  [   32/42552]\n",
      "loss: 0.238106  [ 3232/42552]\n",
      "loss: 0.179028  [ 6432/42552]\n",
      "loss: 0.349051  [ 9632/42552]\n",
      "loss: 0.398297  [12832/42552]\n",
      "loss: 0.257665  [16032/42552]\n",
      "loss: 0.216919  [19232/42552]\n",
      "loss: 0.282410  [22432/42552]\n",
      "loss: 0.188775  [25632/42552]\n",
      "loss: 0.305430  [28832/42552]\n",
      "loss: 0.175291  [32032/42552]\n",
      "loss: 0.284885  [35232/42552]\n",
      "loss: 0.383173  [38432/42552]\n",
      "loss: 0.260092  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.234031 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.270003  [   32/42552]\n",
      "loss: 0.352192  [ 3232/42552]\n",
      "loss: 0.182964  [ 6432/42552]\n",
      "loss: 0.194023  [ 9632/42552]\n",
      "loss: 0.330859  [12832/42552]\n",
      "loss: 0.314488  [16032/42552]\n",
      "loss: 0.167010  [19232/42552]\n",
      "loss: 0.293432  [22432/42552]\n",
      "loss: 0.349710  [25632/42552]\n",
      "loss: 0.207448  [28832/42552]\n",
      "loss: 0.355334  [32032/42552]\n",
      "loss: 0.351375  [35232/42552]\n",
      "loss: 0.256894  [38432/42552]\n",
      "loss: 0.214825  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.250087 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.397002  [   32/42552]\n",
      "loss: 0.170014  [ 3232/42552]\n",
      "loss: 0.279675  [ 6432/42552]\n",
      "loss: 0.251258  [ 9632/42552]\n",
      "loss: 0.250705  [12832/42552]\n",
      "loss: 0.294052  [16032/42552]\n",
      "loss: 0.316354  [19232/42552]\n",
      "loss: 0.224560  [22432/42552]\n",
      "loss: 0.172059  [25632/42552]\n",
      "loss: 0.137831  [28832/42552]\n",
      "loss: 0.219625  [32032/42552]\n",
      "loss: 0.189088  [35232/42552]\n",
      "loss: 0.338794  [38432/42552]\n",
      "loss: 0.210889  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.229703 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.287532  [   32/42552]\n",
      "loss: 0.176022  [ 3232/42552]\n",
      "loss: 0.331448  [ 6432/42552]\n",
      "loss: 0.195500  [ 9632/42552]\n",
      "loss: 0.185854  [12832/42552]\n",
      "loss: 0.261421  [16032/42552]\n",
      "loss: 0.308181  [19232/42552]\n",
      "loss: 0.317739  [22432/42552]\n",
      "loss: 0.190442  [25632/42552]\n",
      "loss: 0.223482  [28832/42552]\n",
      "loss: 0.336359  [32032/42552]\n",
      "loss: 0.274102  [35232/42552]\n",
      "loss: 0.444980  [38432/42552]\n",
      "loss: 0.203946  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.230932 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.210038  [   32/42552]\n",
      "loss: 0.249336  [ 3232/42552]\n",
      "loss: 0.217375  [ 6432/42552]\n",
      "loss: 0.206564  [ 9632/42552]\n",
      "loss: 0.252542  [12832/42552]\n",
      "loss: 0.300876  [16032/42552]\n",
      "loss: 0.206432  [19232/42552]\n",
      "loss: 0.373494  [22432/42552]\n",
      "loss: 0.261889  [25632/42552]\n",
      "loss: 0.339033  [28832/42552]\n",
      "loss: 0.331946  [32032/42552]\n",
      "loss: 0.155221  [35232/42552]\n",
      "loss: 0.368409  [38432/42552]\n",
      "loss: 0.326155  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.232426 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.419616  [   32/42552]\n",
      "loss: 0.233001  [ 3232/42552]\n",
      "loss: 0.291449  [ 6432/42552]\n",
      "loss: 0.194269  [ 9632/42552]\n",
      "loss: 0.233052  [12832/42552]\n",
      "loss: 0.188212  [16032/42552]\n",
      "loss: 0.242126  [19232/42552]\n",
      "loss: 0.327001  [22432/42552]\n",
      "loss: 0.143523  [25632/42552]\n",
      "loss: 0.395844  [28832/42552]\n",
      "loss: 0.312241  [32032/42552]\n",
      "loss: 0.389679  [35232/42552]\n",
      "loss: 0.382745  [38432/42552]\n",
      "loss: 0.320027  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.240625 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.318986  [   32/42552]\n",
      "loss: 0.187588  [ 3232/42552]\n",
      "loss: 0.192401  [ 6432/42552]\n",
      "loss: 0.185291  [ 9632/42552]\n",
      "loss: 0.466307  [12832/42552]\n",
      "loss: 0.208413  [16032/42552]\n",
      "loss: 0.280024  [19232/42552]\n",
      "loss: 0.270090  [22432/42552]\n",
      "loss: 0.221750  [25632/42552]\n",
      "loss: 0.207870  [28832/42552]\n",
      "loss: 0.327181  [32032/42552]\n",
      "loss: 0.320264  [35232/42552]\n",
      "loss: 0.231223  [38432/42552]\n",
      "loss: 0.535863  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.247769 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.283696  [   32/42552]\n",
      "loss: 0.278543  [ 3232/42552]\n",
      "loss: 0.134498  [ 6432/42552]\n",
      "loss: 0.212481  [ 9632/42552]\n",
      "loss: 0.259745  [12832/42552]\n",
      "loss: 0.209564  [16032/42552]\n",
      "loss: 0.229065  [19232/42552]\n",
      "loss: 0.221656  [22432/42552]\n",
      "loss: 0.235562  [25632/42552]\n",
      "loss: 0.259515  [28832/42552]\n",
      "loss: 0.349553  [32032/42552]\n",
      "loss: 0.339450  [35232/42552]\n",
      "loss: 0.207830  [38432/42552]\n",
      "loss: 0.274711  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.227821 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.214253  [   32/42552]\n",
      "loss: 0.266668  [ 3232/42552]\n",
      "loss: 0.226263  [ 6432/42552]\n",
      "loss: 0.242959  [ 9632/42552]\n",
      "loss: 0.343920  [12832/42552]\n",
      "loss: 0.133221  [16032/42552]\n",
      "loss: 0.345097  [19232/42552]\n",
      "loss: 0.222317  [22432/42552]\n",
      "loss: 0.231346  [25632/42552]\n",
      "loss: 0.254740  [28832/42552]\n",
      "loss: 0.289403  [32032/42552]\n",
      "loss: 0.220861  [35232/42552]\n",
      "loss: 0.342491  [38432/42552]\n",
      "loss: 0.250655  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.232468 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.318194  [   32/42552]\n",
      "loss: 0.274003  [ 3232/42552]\n",
      "loss: 0.331852  [ 6432/42552]\n",
      "loss: 0.333670  [ 9632/42552]\n",
      "loss: 0.168877  [12832/42552]\n",
      "loss: 0.272680  [16032/42552]\n",
      "loss: 0.270826  [19232/42552]\n",
      "loss: 0.383045  [22432/42552]\n",
      "loss: 0.177517  [25632/42552]\n",
      "loss: 0.151422  [28832/42552]\n",
      "loss: 0.210192  [32032/42552]\n",
      "loss: 0.202775  [35232/42552]\n",
      "loss: 0.325107  [38432/42552]\n",
      "loss: 0.404372  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.232420 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.260025  [   32/42552]\n",
      "loss: 0.384952  [ 3232/42552]\n",
      "loss: 0.248491  [ 6432/42552]\n",
      "loss: 0.339481  [ 9632/42552]\n",
      "loss: 0.221829  [12832/42552]\n",
      "loss: 0.269404  [16032/42552]\n",
      "loss: 0.229245  [19232/42552]\n",
      "loss: 0.382604  [22432/42552]\n",
      "loss: 0.262247  [25632/42552]\n",
      "loss: 0.199404  [28832/42552]\n",
      "loss: 0.331497  [32032/42552]\n",
      "loss: 0.228462  [35232/42552]\n",
      "loss: 0.151070  [38432/42552]\n",
      "loss: 0.205034  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.229206 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.299792  [   32/42552]\n",
      "loss: 0.279836  [ 3232/42552]\n",
      "loss: 0.254249  [ 6432/42552]\n",
      "loss: 0.258040  [ 9632/42552]\n",
      "loss: 0.201894  [12832/42552]\n",
      "loss: 0.242288  [16032/42552]\n",
      "loss: 0.323727  [19232/42552]\n",
      "loss: 0.209735  [22432/42552]\n",
      "loss: 0.138683  [25632/42552]\n",
      "loss: 0.496841  [28832/42552]\n",
      "loss: 0.273053  [32032/42552]\n",
      "loss: 0.289956  [35232/42552]\n",
      "loss: 0.268391  [38432/42552]\n",
      "loss: 0.283861  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.230677 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.145298  [   32/42552]\n",
      "loss: 0.198517  [ 3232/42552]\n",
      "loss: 0.220719  [ 6432/42552]\n",
      "loss: 0.408443  [ 9632/42552]\n",
      "loss: 0.315430  [12832/42552]\n",
      "loss: 0.333699  [16032/42552]\n",
      "loss: 0.168901  [19232/42552]\n",
      "loss: 0.273540  [22432/42552]\n",
      "loss: 0.261785  [25632/42552]\n",
      "loss: 0.162999  [28832/42552]\n",
      "loss: 0.199762  [32032/42552]\n",
      "loss: 0.276934  [35232/42552]\n",
      "loss: 0.227547  [38432/42552]\n",
      "loss: 0.246622  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.228866 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.229038  [   32/42552]\n",
      "loss: 0.295665  [ 3232/42552]\n",
      "loss: 0.206123  [ 6432/42552]\n",
      "loss: 0.265033  [ 9632/42552]\n",
      "loss: 0.452910  [12832/42552]\n",
      "loss: 0.343133  [16032/42552]\n",
      "loss: 0.285160  [19232/42552]\n",
      "loss: 0.381427  [22432/42552]\n",
      "loss: 0.301218  [25632/42552]\n",
      "loss: 0.351623  [28832/42552]\n",
      "loss: 0.239366  [32032/42552]\n",
      "loss: 0.131409  [35232/42552]\n",
      "loss: 0.248939  [38432/42552]\n",
      "loss: 0.517640  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.226000 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.309651  [   32/42552]\n",
      "loss: 0.352949  [ 3232/42552]\n",
      "loss: 0.256822  [ 6432/42552]\n",
      "loss: 0.234630  [ 9632/42552]\n",
      "loss: 0.136267  [12832/42552]\n",
      "loss: 0.200335  [16032/42552]\n",
      "loss: 0.531787  [19232/42552]\n",
      "loss: 0.168469  [22432/42552]\n",
      "loss: 0.252552  [25632/42552]\n",
      "loss: 0.356217  [28832/42552]\n",
      "loss: 0.305508  [32032/42552]\n",
      "loss: 0.206026  [35232/42552]\n",
      "loss: 0.246390  [38432/42552]\n",
      "loss: 0.170417  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.226804 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.245523  [   32/42552]\n",
      "loss: 0.213375  [ 3232/42552]\n",
      "loss: 0.351248  [ 6432/42552]\n",
      "loss: 0.204056  [ 9632/42552]\n",
      "loss: 0.254193  [12832/42552]\n",
      "loss: 0.203880  [16032/42552]\n",
      "loss: 0.341267  [19232/42552]\n",
      "loss: 0.222280  [22432/42552]\n",
      "loss: 0.164203  [25632/42552]\n",
      "loss: 0.220945  [28832/42552]\n",
      "loss: 0.454709  [32032/42552]\n",
      "loss: 0.208150  [35232/42552]\n",
      "loss: 0.278290  [38432/42552]\n",
      "loss: 0.195109  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.232679 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.352326  [   32/42552]\n",
      "loss: 0.229486  [ 3232/42552]\n",
      "loss: 0.186008  [ 6432/42552]\n",
      "loss: 0.246835  [ 9632/42552]\n",
      "loss: 0.261379  [12832/42552]\n",
      "loss: 0.151645  [16032/42552]\n",
      "loss: 0.259544  [19232/42552]\n",
      "loss: 0.244258  [22432/42552]\n",
      "loss: 0.344393  [25632/42552]\n",
      "loss: 0.240105  [28832/42552]\n",
      "loss: 0.451619  [32032/42552]\n",
      "loss: 0.173795  [35232/42552]\n",
      "loss: 0.332995  [38432/42552]\n",
      "loss: 0.173454  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.235653 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.265993  [   32/42552]\n",
      "loss: 0.144699  [ 3232/42552]\n",
      "loss: 0.277135  [ 6432/42552]\n",
      "loss: 0.233361  [ 9632/42552]\n",
      "loss: 0.373137  [12832/42552]\n",
      "loss: 0.377836  [16032/42552]\n",
      "loss: 0.233014  [19232/42552]\n",
      "loss: 0.251248  [22432/42552]\n",
      "loss: 0.287728  [25632/42552]\n",
      "loss: 0.223267  [28832/42552]\n",
      "loss: 0.249374  [32032/42552]\n",
      "loss: 0.210954  [35232/42552]\n",
      "loss: 0.166906  [38432/42552]\n",
      "loss: 0.417709  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.232559 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.340228  [   32/42552]\n",
      "loss: 0.273399  [ 3232/42552]\n",
      "loss: 0.468416  [ 6432/42552]\n",
      "loss: 0.437014  [ 9632/42552]\n",
      "loss: 0.234879  [12832/42552]\n",
      "loss: 0.295189  [16032/42552]\n",
      "loss: 0.208568  [19232/42552]\n",
      "loss: 0.232774  [22432/42552]\n",
      "loss: 0.281496  [25632/42552]\n",
      "loss: 0.265640  [28832/42552]\n",
      "loss: 0.288926  [32032/42552]\n",
      "loss: 0.174085  [35232/42552]\n",
      "loss: 0.210234  [38432/42552]\n",
      "loss: 0.339055  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.238803 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.268516  [   32/42552]\n",
      "loss: 0.280816  [ 3232/42552]\n",
      "loss: 0.189902  [ 6432/42552]\n",
      "loss: 0.314557  [ 9632/42552]\n",
      "loss: 0.219800  [12832/42552]\n",
      "loss: 0.235905  [16032/42552]\n",
      "loss: 0.280963  [19232/42552]\n",
      "loss: 0.239610  [22432/42552]\n",
      "loss: 0.311729  [25632/42552]\n",
      "loss: 0.285376  [28832/42552]\n",
      "loss: 0.242349  [32032/42552]\n",
      "loss: 0.318160  [35232/42552]\n",
      "loss: 0.241242  [38432/42552]\n",
      "loss: 0.264884  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.243332 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.340958  [   32/42552]\n",
      "loss: 0.301952  [ 3232/42552]\n",
      "loss: 0.284135  [ 6432/42552]\n",
      "loss: 0.270964  [ 9632/42552]\n",
      "loss: 0.400482  [12832/42552]\n",
      "loss: 0.227487  [16032/42552]\n",
      "loss: 0.155982  [19232/42552]\n",
      "loss: 0.355975  [22432/42552]\n",
      "loss: 0.234961  [25632/42552]\n",
      "loss: 0.212694  [28832/42552]\n",
      "loss: 0.410999  [32032/42552]\n",
      "loss: 0.421496  [35232/42552]\n",
      "loss: 0.163184  [38432/42552]\n",
      "loss: 0.297484  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.232341 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.118911  [   32/42552]\n",
      "loss: 0.168322  [ 3232/42552]\n",
      "loss: 0.193338  [ 6432/42552]\n",
      "loss: 0.222043  [ 9632/42552]\n",
      "loss: 0.449977  [12832/42552]\n",
      "loss: 0.232789  [16032/42552]\n",
      "loss: 0.369869  [19232/42552]\n",
      "loss: 0.305856  [22432/42552]\n",
      "loss: 0.236678  [25632/42552]\n",
      "loss: 0.228072  [28832/42552]\n",
      "loss: 0.344382  [32032/42552]\n",
      "loss: 0.231424  [35232/42552]\n",
      "loss: 0.193358  [38432/42552]\n",
      "loss: 0.318891  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.234033 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.201313  [   32/42552]\n",
      "loss: 0.233737  [ 3232/42552]\n",
      "loss: 0.200089  [ 6432/42552]\n",
      "loss: 0.271692  [ 9632/42552]\n",
      "loss: 0.099548  [12832/42552]\n",
      "loss: 0.191762  [16032/42552]\n",
      "loss: 0.279306  [19232/42552]\n",
      "loss: 0.238769  [22432/42552]\n",
      "loss: 0.314324  [25632/42552]\n",
      "loss: 0.246386  [28832/42552]\n",
      "loss: 0.247522  [32032/42552]\n",
      "loss: 0.247719  [35232/42552]\n",
      "loss: 0.281433  [38432/42552]\n",
      "loss: 0.320800  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.225264 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.261912  [   32/42552]\n",
      "loss: 0.133502  [ 3232/42552]\n",
      "loss: 0.352321  [ 6432/42552]\n",
      "loss: 0.282500  [ 9632/42552]\n",
      "loss: 0.166500  [12832/42552]\n",
      "loss: 0.250189  [16032/42552]\n",
      "loss: 0.176813  [19232/42552]\n",
      "loss: 0.287974  [22432/42552]\n",
      "loss: 0.210679  [25632/42552]\n",
      "loss: 0.176173  [28832/42552]\n",
      "loss: 0.279881  [32032/42552]\n",
      "loss: 0.240553  [35232/42552]\n",
      "loss: 0.363682  [38432/42552]\n",
      "loss: 0.295063  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.228014 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.196954  [   32/42552]\n",
      "loss: 0.385828  [ 3232/42552]\n",
      "loss: 0.142473  [ 6432/42552]\n",
      "loss: 0.244660  [ 9632/42552]\n",
      "loss: 0.344943  [12832/42552]\n",
      "loss: 0.196951  [16032/42552]\n",
      "loss: 0.299501  [19232/42552]\n",
      "loss: 0.362234  [22432/42552]\n",
      "loss: 0.155758  [25632/42552]\n",
      "loss: 0.251324  [28832/42552]\n",
      "loss: 0.237543  [32032/42552]\n",
      "loss: 0.274320  [35232/42552]\n",
      "loss: 0.182486  [38432/42552]\n",
      "loss: 0.362251  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.234921 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.299799  [   32/42552]\n",
      "loss: 0.265430  [ 3232/42552]\n",
      "loss: 0.189741  [ 6432/42552]\n",
      "loss: 0.307748  [ 9632/42552]\n",
      "loss: 0.227197  [12832/42552]\n",
      "loss: 0.264709  [16032/42552]\n",
      "loss: 0.529859  [19232/42552]\n",
      "loss: 0.386283  [22432/42552]\n",
      "loss: 0.360418  [25632/42552]\n",
      "loss: 0.184785  [28832/42552]\n",
      "loss: 0.313899  [32032/42552]\n",
      "loss: 0.314635  [35232/42552]\n",
      "loss: 0.275740  [38432/42552]\n",
      "loss: 0.239908  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.224649 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.256560  [   32/42552]\n",
      "loss: 0.217538  [ 3232/42552]\n",
      "loss: 0.173963  [ 6432/42552]\n",
      "loss: 0.321182  [ 9632/42552]\n",
      "loss: 0.182994  [12832/42552]\n",
      "loss: 0.265076  [16032/42552]\n",
      "loss: 0.273202  [19232/42552]\n",
      "loss: 0.332301  [22432/42552]\n",
      "loss: 0.393884  [25632/42552]\n",
      "loss: 0.123084  [28832/42552]\n",
      "loss: 0.356125  [32032/42552]\n",
      "loss: 0.290714  [35232/42552]\n",
      "loss: 0.331824  [38432/42552]\n",
      "loss: 0.489711  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.225677 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.451220  [   32/42552]\n",
      "loss: 0.366125  [ 3232/42552]\n",
      "loss: 0.241603  [ 6432/42552]\n",
      "loss: 0.161904  [ 9632/42552]\n",
      "loss: 0.223247  [12832/42552]\n",
      "loss: 0.266667  [16032/42552]\n",
      "loss: 0.192393  [19232/42552]\n",
      "loss: 0.241045  [22432/42552]\n",
      "loss: 0.209806  [25632/42552]\n",
      "loss: 0.277249  [28832/42552]\n",
      "loss: 0.332710  [32032/42552]\n",
      "loss: 0.230707  [35232/42552]\n",
      "loss: 0.228407  [38432/42552]\n",
      "loss: 0.202565  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.228134 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.249484  [   32/42552]\n",
      "loss: 0.274814  [ 3232/42552]\n",
      "loss: 0.278934  [ 6432/42552]\n",
      "loss: 0.350510  [ 9632/42552]\n",
      "loss: 0.341737  [12832/42552]\n",
      "loss: 0.329571  [16032/42552]\n",
      "loss: 0.310348  [19232/42552]\n",
      "loss: 0.217845  [22432/42552]\n",
      "loss: 0.235039  [25632/42552]\n",
      "loss: 0.202999  [28832/42552]\n",
      "loss: 0.195066  [32032/42552]\n",
      "loss: 0.463817  [35232/42552]\n",
      "loss: 0.383834  [38432/42552]\n",
      "loss: 0.165083  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.226583 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.291921  [   32/42552]\n",
      "loss: 0.288343  [ 3232/42552]\n",
      "loss: 0.257906  [ 6432/42552]\n",
      "loss: 0.249719  [ 9632/42552]\n",
      "loss: 0.380040  [12832/42552]\n",
      "loss: 0.359634  [16032/42552]\n",
      "loss: 0.276460  [19232/42552]\n",
      "loss: 0.135753  [22432/42552]\n",
      "loss: 0.285838  [25632/42552]\n",
      "loss: 0.243172  [28832/42552]\n",
      "loss: 0.246345  [32032/42552]\n",
      "loss: 0.279129  [35232/42552]\n",
      "loss: 0.167532  [38432/42552]\n",
      "loss: 0.358501  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.228457 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.117282  [   32/42552]\n",
      "loss: 0.369647  [ 3232/42552]\n",
      "loss: 0.160483  [ 6432/42552]\n",
      "loss: 0.274935  [ 9632/42552]\n",
      "loss: 0.398649  [12832/42552]\n",
      "loss: 0.308713  [16032/42552]\n",
      "loss: 0.484923  [19232/42552]\n",
      "loss: 0.280428  [22432/42552]\n",
      "loss: 0.179319  [25632/42552]\n",
      "loss: 0.261301  [28832/42552]\n",
      "loss: 0.276070  [32032/42552]\n",
      "loss: 0.238786  [35232/42552]\n",
      "loss: 0.410762  [38432/42552]\n",
      "loss: 0.261206  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.230195 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.304242  [   32/42552]\n",
      "loss: 0.297108  [ 3232/42552]\n",
      "loss: 0.247183  [ 6432/42552]\n",
      "loss: 0.376004  [ 9632/42552]\n",
      "loss: 0.184701  [12832/42552]\n",
      "loss: 0.344505  [16032/42552]\n",
      "loss: 0.213176  [19232/42552]\n",
      "loss: 0.333524  [22432/42552]\n",
      "loss: 0.193706  [25632/42552]\n",
      "loss: 0.323709  [28832/42552]\n",
      "loss: 0.179826  [32032/42552]\n",
      "loss: 0.167513  [35232/42552]\n",
      "loss: 0.189763  [38432/42552]\n",
      "loss: 0.274734  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.232225 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.245542  [   32/42552]\n",
      "loss: 0.333467  [ 3232/42552]\n",
      "loss: 0.390784  [ 6432/42552]\n",
      "loss: 0.281116  [ 9632/42552]\n",
      "loss: 0.310418  [12832/42552]\n",
      "loss: 0.301077  [16032/42552]\n",
      "loss: 0.330984  [19232/42552]\n",
      "loss: 0.212644  [22432/42552]\n",
      "loss: 0.302923  [25632/42552]\n",
      "loss: 0.408151  [28832/42552]\n",
      "loss: 0.202528  [32032/42552]\n",
      "loss: 0.196815  [35232/42552]\n",
      "loss: 0.259412  [38432/42552]\n",
      "loss: 0.259846  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.230190 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.182272  [   32/42552]\n",
      "loss: 0.336548  [ 3232/42552]\n",
      "loss: 0.318230  [ 6432/42552]\n",
      "loss: 0.166141  [ 9632/42552]\n",
      "loss: 0.262174  [12832/42552]\n",
      "loss: 0.334290  [16032/42552]\n",
      "loss: 0.298037  [19232/42552]\n",
      "loss: 0.269009  [22432/42552]\n",
      "loss: 0.406548  [25632/42552]\n",
      "loss: 0.327746  [28832/42552]\n",
      "loss: 0.182543  [32032/42552]\n",
      "loss: 0.373296  [35232/42552]\n",
      "loss: 0.221030  [38432/42552]\n",
      "loss: 0.207739  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.226042 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.425606  [   32/42552]\n",
      "loss: 0.314777  [ 3232/42552]\n",
      "loss: 0.413647  [ 6432/42552]\n",
      "loss: 0.258918  [ 9632/42552]\n",
      "loss: 0.224519  [12832/42552]\n",
      "loss: 0.219335  [16032/42552]\n",
      "loss: 0.325704  [19232/42552]\n",
      "loss: 0.276862  [22432/42552]\n",
      "loss: 0.299789  [25632/42552]\n",
      "loss: 0.300123  [28832/42552]\n",
      "loss: 0.323563  [32032/42552]\n",
      "loss: 0.248226  [35232/42552]\n",
      "loss: 0.199703  [38432/42552]\n",
      "loss: 0.284857  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.227376 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.332357  [   32/42552]\n",
      "loss: 0.210672  [ 3232/42552]\n",
      "loss: 0.128083  [ 6432/42552]\n",
      "loss: 0.169932  [ 9632/42552]\n",
      "loss: 0.161068  [12832/42552]\n",
      "loss: 0.298653  [16032/42552]\n",
      "loss: 0.317462  [19232/42552]\n",
      "loss: 0.410106  [22432/42552]\n",
      "loss: 0.164092  [25632/42552]\n",
      "loss: 0.164132  [28832/42552]\n",
      "loss: 0.376664  [32032/42552]\n",
      "loss: 0.305375  [35232/42552]\n",
      "loss: 0.391816  [38432/42552]\n",
      "loss: 0.287075  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.225967 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.440466  [   32/42552]\n",
      "loss: 0.257608  [ 3232/42552]\n",
      "loss: 0.247573  [ 6432/42552]\n",
      "loss: 0.478388  [ 9632/42552]\n",
      "loss: 0.282503  [12832/42552]\n",
      "loss: 0.263987  [16032/42552]\n",
      "loss: 0.345275  [19232/42552]\n",
      "loss: 0.314481  [22432/42552]\n",
      "loss: 0.295302  [25632/42552]\n",
      "loss: 0.129046  [28832/42552]\n",
      "loss: 0.266502  [32032/42552]\n",
      "loss: 0.327032  [35232/42552]\n",
      "loss: 0.256420  [38432/42552]\n",
      "loss: 0.154034  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.228475 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.213004  [   32/42552]\n",
      "loss: 0.300925  [ 3232/42552]\n",
      "loss: 0.130071  [ 6432/42552]\n",
      "loss: 0.260299  [ 9632/42552]\n",
      "loss: 0.242888  [12832/42552]\n",
      "loss: 0.189231  [16032/42552]\n",
      "loss: 0.261737  [19232/42552]\n",
      "loss: 0.153124  [22432/42552]\n",
      "loss: 0.238543  [25632/42552]\n",
      "loss: 0.255510  [28832/42552]\n",
      "loss: 0.259816  [32032/42552]\n",
      "loss: 0.210601  [35232/42552]\n",
      "loss: 0.279161  [38432/42552]\n",
      "loss: 0.212781  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.226261 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.273859  [   32/42552]\n",
      "loss: 0.415960  [ 3232/42552]\n",
      "loss: 0.203251  [ 6432/42552]\n",
      "loss: 0.239958  [ 9632/42552]\n",
      "loss: 0.231851  [12832/42552]\n",
      "loss: 0.203256  [16032/42552]\n",
      "loss: 0.238025  [19232/42552]\n",
      "loss: 0.195795  [22432/42552]\n",
      "loss: 0.123415  [25632/42552]\n",
      "loss: 0.281273  [28832/42552]\n",
      "loss: 0.204661  [32032/42552]\n",
      "loss: 0.294609  [35232/42552]\n",
      "loss: 0.345450  [38432/42552]\n",
      "loss: 0.181167  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.234462 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.446354  [   32/42552]\n",
      "loss: 0.277097  [ 3232/42552]\n",
      "loss: 0.276858  [ 6432/42552]\n",
      "loss: 0.294115  [ 9632/42552]\n",
      "loss: 0.234716  [12832/42552]\n",
      "loss: 0.269640  [16032/42552]\n",
      "loss: 0.274928  [19232/42552]\n",
      "loss: 0.265820  [22432/42552]\n",
      "loss: 0.403710  [25632/42552]\n",
      "loss: 0.282631  [28832/42552]\n",
      "loss: 0.231575  [32032/42552]\n",
      "loss: 0.210675  [35232/42552]\n",
      "loss: 0.185732  [38432/42552]\n",
      "loss: 0.409425  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.229788 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.162222  [   32/42552]\n",
      "loss: 0.230225  [ 3232/42552]\n",
      "loss: 0.149433  [ 6432/42552]\n",
      "loss: 0.353578  [ 9632/42552]\n",
      "loss: 0.256791  [12832/42552]\n",
      "loss: 0.225218  [16032/42552]\n",
      "loss: 0.323338  [19232/42552]\n",
      "loss: 0.208968  [22432/42552]\n",
      "loss: 0.280827  [25632/42552]\n",
      "loss: 0.246084  [28832/42552]\n",
      "loss: 0.194918  [32032/42552]\n",
      "loss: 0.279085  [35232/42552]\n",
      "loss: 0.277808  [38432/42552]\n",
      "loss: 0.371439  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.225461 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.172721  [   32/42552]\n",
      "loss: 0.283258  [ 3232/42552]\n",
      "loss: 0.265979  [ 6432/42552]\n",
      "loss: 0.142562  [ 9632/42552]\n",
      "loss: 0.243409  [12832/42552]\n",
      "loss: 0.343131  [16032/42552]\n",
      "loss: 0.315678  [19232/42552]\n",
      "loss: 0.178667  [22432/42552]\n",
      "loss: 0.137941  [25632/42552]\n",
      "loss: 0.208532  [28832/42552]\n",
      "loss: 0.178874  [32032/42552]\n",
      "loss: 0.334267  [35232/42552]\n",
      "loss: 0.188700  [38432/42552]\n",
      "loss: 0.215889  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.231160 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.252947  [   32/42552]\n",
      "loss: 0.439147  [ 3232/42552]\n",
      "loss: 0.196419  [ 6432/42552]\n",
      "loss: 0.352321  [ 9632/42552]\n",
      "loss: 0.201597  [12832/42552]\n",
      "loss: 0.255393  [16032/42552]\n",
      "loss: 0.255797  [19232/42552]\n",
      "loss: 0.268558  [22432/42552]\n",
      "loss: 0.252615  [25632/42552]\n",
      "loss: 0.253121  [28832/42552]\n",
      "loss: 0.339918  [32032/42552]\n",
      "loss: 0.248122  [35232/42552]\n",
      "loss: 0.287284  [38432/42552]\n",
      "loss: 0.228476  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.235372 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.188039  [   32/42552]\n",
      "loss: 0.248907  [ 3232/42552]\n",
      "loss: 0.234277  [ 6432/42552]\n",
      "loss: 0.237819  [ 9632/42552]\n",
      "loss: 0.407124  [12832/42552]\n",
      "loss: 0.245947  [16032/42552]\n",
      "loss: 0.165106  [19232/42552]\n",
      "loss: 0.247682  [22432/42552]\n",
      "loss: 0.172133  [25632/42552]\n",
      "loss: 0.292285  [28832/42552]\n",
      "loss: 0.111749  [32032/42552]\n",
      "loss: 0.319969  [35232/42552]\n",
      "loss: 0.182970  [38432/42552]\n",
      "loss: 0.151071  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.229064 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.263448  [   32/42552]\n",
      "loss: 0.143313  [ 3232/42552]\n",
      "loss: 0.266708  [ 6432/42552]\n",
      "loss: 0.310653  [ 9632/42552]\n",
      "loss: 0.227752  [12832/42552]\n",
      "loss: 0.195883  [16032/42552]\n",
      "loss: 0.334148  [19232/42552]\n",
      "loss: 0.436451  [22432/42552]\n",
      "loss: 0.221401  [25632/42552]\n",
      "loss: 0.311976  [28832/42552]\n",
      "loss: 0.134050  [32032/42552]\n",
      "loss: 0.182635  [35232/42552]\n",
      "loss: 0.361580  [38432/42552]\n",
      "loss: 0.255546  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.226292 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.215202  [   32/42552]\n",
      "loss: 0.342418  [ 3232/42552]\n",
      "loss: 0.215005  [ 6432/42552]\n",
      "loss: 0.256094  [ 9632/42552]\n",
      "loss: 0.176503  [12832/42552]\n",
      "loss: 0.374669  [16032/42552]\n",
      "loss: 0.388916  [19232/42552]\n",
      "loss: 0.190009  [22432/42552]\n",
      "loss: 0.385971  [25632/42552]\n",
      "loss: 0.137505  [28832/42552]\n",
      "loss: 0.184004  [32032/42552]\n",
      "loss: 0.245278  [35232/42552]\n",
      "loss: 0.262734  [38432/42552]\n",
      "loss: 0.262790  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.234131 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.356998  [   32/42552]\n",
      "loss: 0.260131  [ 3232/42552]\n",
      "loss: 0.194203  [ 6432/42552]\n",
      "loss: 0.248175  [ 9632/42552]\n",
      "loss: 0.360644  [12832/42552]\n",
      "loss: 0.319824  [16032/42552]\n",
      "loss: 0.216836  [19232/42552]\n",
      "loss: 0.327025  [22432/42552]\n",
      "loss: 0.232415  [25632/42552]\n",
      "loss: 0.298899  [28832/42552]\n",
      "loss: 0.164545  [32032/42552]\n",
      "loss: 0.333294  [35232/42552]\n",
      "loss: 0.275694  [38432/42552]\n",
      "loss: 0.351914  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.244423 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.342936  [   32/42552]\n",
      "loss: 0.182979  [ 3232/42552]\n",
      "loss: 0.482901  [ 6432/42552]\n",
      "loss: 0.370025  [ 9632/42552]\n",
      "loss: 0.133529  [12832/42552]\n",
      "loss: 0.295007  [16032/42552]\n",
      "loss: 0.231145  [19232/42552]\n",
      "loss: 0.294396  [22432/42552]\n",
      "loss: 0.253365  [25632/42552]\n",
      "loss: 0.183839  [28832/42552]\n",
      "loss: 0.269667  [32032/42552]\n",
      "loss: 0.207925  [35232/42552]\n",
      "loss: 0.259623  [38432/42552]\n",
      "loss: 0.200811  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.226231 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.341572  [   32/42552]\n",
      "loss: 0.263313  [ 3232/42552]\n",
      "loss: 0.205375  [ 6432/42552]\n",
      "loss: 0.349795  [ 9632/42552]\n",
      "loss: 0.222263  [12832/42552]\n",
      "loss: 0.221696  [16032/42552]\n",
      "loss: 0.161125  [19232/42552]\n",
      "loss: 0.198328  [22432/42552]\n",
      "loss: 0.128985  [25632/42552]\n",
      "loss: 0.232134  [28832/42552]\n",
      "loss: 0.357462  [32032/42552]\n",
      "loss: 0.148153  [35232/42552]\n",
      "loss: 0.139883  [38432/42552]\n",
      "loss: 0.240047  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.232525 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.274088  [   32/42552]\n",
      "loss: 0.297240  [ 3232/42552]\n",
      "loss: 0.260169  [ 6432/42552]\n",
      "loss: 0.323194  [ 9632/42552]\n",
      "loss: 0.166533  [12832/42552]\n",
      "loss: 0.173454  [16032/42552]\n",
      "loss: 0.173978  [19232/42552]\n",
      "loss: 0.330665  [22432/42552]\n",
      "loss: 0.303886  [25632/42552]\n",
      "loss: 0.354393  [28832/42552]\n",
      "loss: 0.430460  [32032/42552]\n",
      "loss: 0.316633  [35232/42552]\n",
      "loss: 0.416511  [38432/42552]\n",
      "loss: 0.176103  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.236103 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.188713  [   32/42552]\n",
      "loss: 0.174275  [ 3232/42552]\n",
      "loss: 0.257448  [ 6432/42552]\n",
      "loss: 0.444939  [ 9632/42552]\n",
      "loss: 0.372257  [12832/42552]\n",
      "loss: 0.347607  [16032/42552]\n",
      "loss: 0.305408  [19232/42552]\n",
      "loss: 0.298031  [22432/42552]\n",
      "loss: 0.277161  [25632/42552]\n",
      "loss: 0.276808  [28832/42552]\n",
      "loss: 0.291490  [32032/42552]\n",
      "loss: 0.143663  [35232/42552]\n",
      "loss: 0.104616  [38432/42552]\n",
      "loss: 0.175496  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.228177 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.247572  [   32/42552]\n",
      "loss: 0.197552  [ 3232/42552]\n",
      "loss: 0.233358  [ 6432/42552]\n",
      "loss: 0.180469  [ 9632/42552]\n",
      "loss: 0.433913  [12832/42552]\n",
      "loss: 0.302396  [16032/42552]\n",
      "loss: 0.137870  [19232/42552]\n",
      "loss: 0.246494  [22432/42552]\n",
      "loss: 0.245385  [25632/42552]\n",
      "loss: 0.619615  [28832/42552]\n",
      "loss: 0.192584  [32032/42552]\n",
      "loss: 0.217858  [35232/42552]\n",
      "loss: 0.356148  [38432/42552]\n",
      "loss: 0.176075  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.224831 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.211294  [   32/42552]\n",
      "loss: 0.270189  [ 3232/42552]\n",
      "loss: 0.256010  [ 6432/42552]\n",
      "loss: 0.296019  [ 9632/42552]\n",
      "loss: 0.301323  [12832/42552]\n",
      "loss: 0.236833  [16032/42552]\n",
      "loss: 0.347471  [19232/42552]\n",
      "loss: 0.322512  [22432/42552]\n",
      "loss: 0.125716  [25632/42552]\n",
      "loss: 0.237728  [28832/42552]\n",
      "loss: 0.313580  [32032/42552]\n",
      "loss: 0.281121  [35232/42552]\n",
      "loss: 0.178206  [38432/42552]\n",
      "loss: 0.279018  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.225862 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.410746  [   32/42552]\n",
      "loss: 0.229847  [ 3232/42552]\n",
      "loss: 0.237023  [ 6432/42552]\n",
      "loss: 0.159371  [ 9632/42552]\n",
      "loss: 0.168002  [12832/42552]\n",
      "loss: 0.294186  [16032/42552]\n",
      "loss: 0.253222  [19232/42552]\n",
      "loss: 0.212198  [22432/42552]\n",
      "loss: 0.191708  [25632/42552]\n",
      "loss: 0.261142  [28832/42552]\n",
      "loss: 0.378211  [32032/42552]\n",
      "loss: 0.264861  [35232/42552]\n",
      "loss: 0.185252  [38432/42552]\n",
      "loss: 0.284846  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.224790 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.214190  [   32/42552]\n",
      "loss: 0.202984  [ 3232/42552]\n",
      "loss: 0.199397  [ 6432/42552]\n",
      "loss: 0.237253  [ 9632/42552]\n",
      "loss: 0.112336  [12832/42552]\n",
      "loss: 0.169357  [16032/42552]\n",
      "loss: 0.357530  [19232/42552]\n",
      "loss: 0.190186  [22432/42552]\n",
      "loss: 0.359269  [25632/42552]\n",
      "loss: 0.184351  [28832/42552]\n",
      "loss: 0.155456  [32032/42552]\n",
      "loss: 0.207453  [35232/42552]\n",
      "loss: 0.113283  [38432/42552]\n",
      "loss: 0.328027  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.224267 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.361822  [   32/42552]\n",
      "loss: 0.249158  [ 3232/42552]\n",
      "loss: 0.294054  [ 6432/42552]\n",
      "loss: 0.386217  [ 9632/42552]\n",
      "loss: 0.247859  [12832/42552]\n",
      "loss: 0.296466  [16032/42552]\n",
      "loss: 0.299855  [19232/42552]\n",
      "loss: 0.278666  [22432/42552]\n",
      "loss: 0.245396  [25632/42552]\n",
      "loss: 0.212693  [28832/42552]\n",
      "loss: 0.185114  [32032/42552]\n",
      "loss: 0.392962  [35232/42552]\n",
      "loss: 0.177770  [38432/42552]\n",
      "loss: 0.299377  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.235630 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.218919  [   32/42552]\n",
      "loss: 0.253680  [ 3232/42552]\n",
      "loss: 0.162305  [ 6432/42552]\n",
      "loss: 0.239283  [ 9632/42552]\n",
      "loss: 0.145076  [12832/42552]\n",
      "loss: 0.346772  [16032/42552]\n",
      "loss: 0.260600  [19232/42552]\n",
      "loss: 0.361414  [22432/42552]\n",
      "loss: 0.260984  [25632/42552]\n",
      "loss: 0.276382  [28832/42552]\n",
      "loss: 0.208088  [32032/42552]\n",
      "loss: 0.313055  [35232/42552]\n",
      "loss: 0.260838  [38432/42552]\n",
      "loss: 0.270398  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.226503 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.367628  [   32/42552]\n",
      "loss: 0.124833  [ 3232/42552]\n",
      "loss: 0.310679  [ 6432/42552]\n",
      "loss: 0.418000  [ 9632/42552]\n",
      "loss: 0.175428  [12832/42552]\n",
      "loss: 0.380811  [16032/42552]\n",
      "loss: 0.144712  [19232/42552]\n",
      "loss: 0.167247  [22432/42552]\n",
      "loss: 0.224597  [25632/42552]\n",
      "loss: 0.238531  [28832/42552]\n",
      "loss: 0.270966  [32032/42552]\n",
      "loss: 0.218087  [35232/42552]\n",
      "loss: 0.270155  [38432/42552]\n",
      "loss: 0.509722  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.229718 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.264770  [   32/42552]\n",
      "loss: 0.323509  [ 3232/42552]\n",
      "loss: 0.272168  [ 6432/42552]\n",
      "loss: 0.375631  [ 9632/42552]\n",
      "loss: 0.257614  [12832/42552]\n",
      "loss: 0.304236  [16032/42552]\n",
      "loss: 0.311902  [19232/42552]\n",
      "loss: 0.225535  [22432/42552]\n",
      "loss: 0.262682  [25632/42552]\n",
      "loss: 0.207554  [28832/42552]\n",
      "loss: 0.211819  [32032/42552]\n",
      "loss: 0.238475  [35232/42552]\n",
      "loss: 0.374038  [38432/42552]\n",
      "loss: 0.283880  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.232157 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.278136  [   32/42552]\n",
      "loss: 0.200507  [ 3232/42552]\n",
      "loss: 0.193598  [ 6432/42552]\n",
      "loss: 0.217764  [ 9632/42552]\n",
      "loss: 0.225587  [12832/42552]\n",
      "loss: 0.183980  [16032/42552]\n",
      "loss: 0.232944  [19232/42552]\n",
      "loss: 0.346545  [22432/42552]\n",
      "loss: 0.309129  [25632/42552]\n",
      "loss: 0.395672  [28832/42552]\n",
      "loss: 0.366630  [32032/42552]\n",
      "loss: 0.181959  [35232/42552]\n",
      "loss: 0.225516  [38432/42552]\n",
      "loss: 0.175287  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.229768 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.220626  [   32/42552]\n",
      "loss: 0.213168  [ 3232/42552]\n",
      "loss: 0.214782  [ 6432/42552]\n",
      "loss: 0.251589  [ 9632/42552]\n",
      "loss: 0.356851  [12832/42552]\n",
      "loss: 0.283593  [16032/42552]\n",
      "loss: 0.198277  [19232/42552]\n",
      "loss: 0.204927  [22432/42552]\n",
      "loss: 0.225355  [25632/42552]\n",
      "loss: 0.280367  [28832/42552]\n",
      "loss: 0.288736  [32032/42552]\n",
      "loss: 0.325236  [35232/42552]\n",
      "loss: 0.230812  [38432/42552]\n",
      "loss: 0.385288  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.226758 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.357141  [   32/42552]\n",
      "loss: 0.300270  [ 3232/42552]\n",
      "loss: 0.227963  [ 6432/42552]\n",
      "loss: 0.359210  [ 9632/42552]\n",
      "loss: 0.265123  [12832/42552]\n",
      "loss: 0.250506  [16032/42552]\n",
      "loss: 0.377418  [19232/42552]\n",
      "loss: 0.274964  [22432/42552]\n",
      "loss: 0.124259  [25632/42552]\n",
      "loss: 0.237739  [28832/42552]\n",
      "loss: 0.271844  [32032/42552]\n",
      "loss: 0.216151  [35232/42552]\n",
      "loss: 0.288874  [38432/42552]\n",
      "loss: 0.305878  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.228456 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.292303  [   32/42552]\n",
      "loss: 0.323000  [ 3232/42552]\n",
      "loss: 0.320975  [ 6432/42552]\n",
      "loss: 0.198335  [ 9632/42552]\n",
      "loss: 0.266303  [12832/42552]\n",
      "loss: 0.297987  [16032/42552]\n",
      "loss: 0.294999  [19232/42552]\n",
      "loss: 0.295631  [22432/42552]\n",
      "loss: 0.291586  [25632/42552]\n",
      "loss: 0.305543  [28832/42552]\n",
      "loss: 0.371006  [32032/42552]\n",
      "loss: 0.166006  [35232/42552]\n",
      "loss: 0.355963  [38432/42552]\n",
      "loss: 0.202824  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.231779 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.139592  [   32/42552]\n",
      "loss: 0.358049  [ 3232/42552]\n",
      "loss: 0.457265  [ 6432/42552]\n",
      "loss: 0.165439  [ 9632/42552]\n",
      "loss: 0.205849  [12832/42552]\n",
      "loss: 0.331330  [16032/42552]\n",
      "loss: 0.296877  [19232/42552]\n",
      "loss: 0.241686  [22432/42552]\n",
      "loss: 0.407112  [25632/42552]\n",
      "loss: 0.237848  [28832/42552]\n",
      "loss: 0.299824  [32032/42552]\n",
      "loss: 0.200773  [35232/42552]\n",
      "loss: 0.428343  [38432/42552]\n",
      "loss: 0.218077  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.224278 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.426803  [   32/42552]\n",
      "loss: 0.401974  [ 3232/42552]\n",
      "loss: 0.208233  [ 6432/42552]\n",
      "loss: 0.304015  [ 9632/42552]\n",
      "loss: 0.294221  [12832/42552]\n",
      "loss: 0.356938  [16032/42552]\n",
      "loss: 0.203216  [19232/42552]\n",
      "loss: 0.320524  [22432/42552]\n",
      "loss: 0.536349  [25632/42552]\n",
      "loss: 0.304506  [28832/42552]\n",
      "loss: 0.283302  [32032/42552]\n",
      "loss: 0.191209  [35232/42552]\n",
      "loss: 0.488473  [38432/42552]\n",
      "loss: 0.253424  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.230193 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.333273  [   32/42552]\n",
      "loss: 0.138794  [ 3232/42552]\n",
      "loss: 0.187746  [ 6432/42552]\n",
      "loss: 0.303839  [ 9632/42552]\n",
      "loss: 0.139376  [12832/42552]\n",
      "loss: 0.210107  [16032/42552]\n",
      "loss: 0.177745  [19232/42552]\n",
      "loss: 0.243553  [22432/42552]\n",
      "loss: 0.171803  [25632/42552]\n",
      "loss: 0.255866  [28832/42552]\n",
      "loss: 0.256568  [32032/42552]\n",
      "loss: 0.242868  [35232/42552]\n",
      "loss: 0.160089  [38432/42552]\n",
      "loss: 0.294878  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.223955 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.250522  [   32/42552]\n",
      "loss: 0.238006  [ 3232/42552]\n",
      "loss: 0.225697  [ 6432/42552]\n",
      "loss: 0.280782  [ 9632/42552]\n",
      "loss: 0.185387  [12832/42552]\n",
      "loss: 0.388562  [16032/42552]\n",
      "loss: 0.125886  [19232/42552]\n",
      "loss: 0.174096  [22432/42552]\n",
      "loss: 0.333119  [25632/42552]\n",
      "loss: 0.168578  [28832/42552]\n",
      "loss: 0.220120  [32032/42552]\n",
      "loss: 0.189515  [35232/42552]\n",
      "loss: 0.298140  [38432/42552]\n",
      "loss: 0.280980  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.225147 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.246238  [   32/42552]\n",
      "loss: 0.258273  [ 3232/42552]\n",
      "loss: 0.296869  [ 6432/42552]\n",
      "loss: 0.258493  [ 9632/42552]\n",
      "loss: 0.204145  [12832/42552]\n",
      "loss: 0.209573  [16032/42552]\n",
      "loss: 0.206524  [19232/42552]\n",
      "loss: 0.175733  [22432/42552]\n",
      "loss: 0.297576  [25632/42552]\n",
      "loss: 0.291323  [28832/42552]\n",
      "loss: 0.384591  [32032/42552]\n",
      "loss: 0.290143  [35232/42552]\n",
      "loss: 0.388454  [38432/42552]\n",
      "loss: 0.334793  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.232094 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.148651  [   32/42552]\n",
      "loss: 0.424301  [ 3232/42552]\n",
      "loss: 0.262549  [ 6432/42552]\n",
      "loss: 0.260045  [ 9632/42552]\n",
      "loss: 0.174028  [12832/42552]\n",
      "loss: 0.323947  [16032/42552]\n",
      "loss: 0.252191  [19232/42552]\n",
      "loss: 0.212971  [22432/42552]\n",
      "loss: 0.299059  [25632/42552]\n",
      "loss: 0.355141  [28832/42552]\n",
      "loss: 0.214438  [32032/42552]\n",
      "loss: 0.334120  [35232/42552]\n",
      "loss: 0.260626  [38432/42552]\n",
      "loss: 0.337261  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.225220 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.249754  [   32/42552]\n",
      "loss: 0.222220  [ 3232/42552]\n",
      "loss: 0.148984  [ 6432/42552]\n",
      "loss: 0.259280  [ 9632/42552]\n",
      "loss: 0.217761  [12832/42552]\n",
      "loss: 0.163389  [16032/42552]\n",
      "loss: 0.374403  [19232/42552]\n",
      "loss: 0.328966  [22432/42552]\n",
      "loss: 0.230247  [25632/42552]\n",
      "loss: 0.280036  [28832/42552]\n",
      "loss: 0.176312  [32032/42552]\n",
      "loss: 0.286946  [35232/42552]\n",
      "loss: 0.229646  [38432/42552]\n",
      "loss: 0.239566  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.223311 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.189648  [   32/42552]\n",
      "loss: 0.265148  [ 3232/42552]\n",
      "loss: 0.391964  [ 6432/42552]\n",
      "loss: 0.358357  [ 9632/42552]\n",
      "loss: 0.249266  [12832/42552]\n",
      "loss: 0.270382  [16032/42552]\n",
      "loss: 0.321131  [19232/42552]\n",
      "loss: 0.357569  [22432/42552]\n",
      "loss: 0.219071  [25632/42552]\n",
      "loss: 0.441714  [28832/42552]\n",
      "loss: 0.203285  [32032/42552]\n",
      "loss: 0.429894  [35232/42552]\n",
      "loss: 0.224696  [38432/42552]\n",
      "loss: 0.310193  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.227424 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.168880  [   32/42552]\n",
      "loss: 0.556724  [ 3232/42552]\n",
      "loss: 0.275005  [ 6432/42552]\n",
      "loss: 0.312787  [ 9632/42552]\n",
      "loss: 0.348928  [12832/42552]\n",
      "loss: 0.246779  [16032/42552]\n",
      "loss: 0.159606  [19232/42552]\n",
      "loss: 0.149465  [22432/42552]\n",
      "loss: 0.272824  [25632/42552]\n",
      "loss: 0.281182  [28832/42552]\n",
      "loss: 0.179048  [32032/42552]\n",
      "loss: 0.371667  [35232/42552]\n",
      "loss: 0.203506  [38432/42552]\n",
      "loss: 0.176799  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.242053 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.208011  [   32/42552]\n",
      "loss: 0.218769  [ 3232/42552]\n",
      "loss: 0.222025  [ 6432/42552]\n",
      "loss: 0.219767  [ 9632/42552]\n",
      "loss: 0.155833  [12832/42552]\n",
      "loss: 0.299012  [16032/42552]\n",
      "loss: 0.268946  [19232/42552]\n",
      "loss: 0.145188  [22432/42552]\n",
      "loss: 0.330466  [25632/42552]\n",
      "loss: 0.193266  [28832/42552]\n",
      "loss: 0.251535  [32032/42552]\n",
      "loss: 0.403702  [35232/42552]\n",
      "loss: 0.226479  [38432/42552]\n",
      "loss: 0.373200  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.224851 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.186807  [   32/42552]\n",
      "loss: 0.211761  [ 3232/42552]\n",
      "loss: 0.229443  [ 6432/42552]\n",
      "loss: 0.160098  [ 9632/42552]\n",
      "loss: 0.219857  [12832/42552]\n",
      "loss: 0.218902  [16032/42552]\n",
      "loss: 0.269546  [19232/42552]\n",
      "loss: 0.153750  [22432/42552]\n",
      "loss: 0.189029  [25632/42552]\n",
      "loss: 0.272900  [28832/42552]\n",
      "loss: 0.187406  [32032/42552]\n",
      "loss: 0.263431  [35232/42552]\n",
      "loss: 0.390830  [38432/42552]\n",
      "loss: 0.290376  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.226884 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.481944  [   32/42552]\n",
      "loss: 0.159722  [ 3232/42552]\n",
      "loss: 0.261238  [ 6432/42552]\n",
      "loss: 0.308113  [ 9632/42552]\n",
      "loss: 0.268340  [12832/42552]\n",
      "loss: 0.275767  [16032/42552]\n",
      "loss: 0.234559  [19232/42552]\n",
      "loss: 0.174900  [22432/42552]\n",
      "loss: 0.209578  [25632/42552]\n",
      "loss: 0.331943  [28832/42552]\n",
      "loss: 0.312744  [32032/42552]\n",
      "loss: 0.266543  [35232/42552]\n",
      "loss: 0.592680  [38432/42552]\n",
      "loss: 0.288379  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.228132 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.222295  [   32/42552]\n",
      "loss: 0.256215  [ 3232/42552]\n",
      "loss: 0.275035  [ 6432/42552]\n",
      "loss: 0.173423  [ 9632/42552]\n",
      "loss: 0.189244  [12832/42552]\n",
      "loss: 0.136516  [16032/42552]\n",
      "loss: 0.247707  [19232/42552]\n",
      "loss: 0.323279  [22432/42552]\n",
      "loss: 0.292530  [25632/42552]\n",
      "loss: 0.152853  [28832/42552]\n",
      "loss: 0.414029  [32032/42552]\n",
      "loss: 0.357827  [35232/42552]\n",
      "loss: 0.204501  [38432/42552]\n",
      "loss: 0.343750  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.228904 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.383575  [   32/42552]\n",
      "loss: 0.218852  [ 3232/42552]\n",
      "loss: 0.328875  [ 6432/42552]\n",
      "loss: 0.243881  [ 9632/42552]\n",
      "loss: 0.326880  [12832/42552]\n",
      "loss: 0.289728  [16032/42552]\n",
      "loss: 0.256649  [19232/42552]\n",
      "loss: 0.244467  [22432/42552]\n",
      "loss: 0.148715  [25632/42552]\n",
      "loss: 0.268847  [28832/42552]\n",
      "loss: 0.180450  [32032/42552]\n",
      "loss: 0.283916  [35232/42552]\n",
      "loss: 0.187184  [38432/42552]\n",
      "loss: 0.302216  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.227746 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model, train_losses, val_losses = train_early_stopping(\n",
    "    train_loader, val_loader, model, loss_fn, optimizer, epochs=500, early_stopping=True,\n",
    "    threshold=0.2,\n",
    "    counter=True, clipping=True)\n",
    "\n",
    "# plot training and validation losses\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '../models/display-box/display-box-18-rolling-means-noise-location.mdl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gwjrml9d76v_zwbyb2x6yc0r0000gn/T/ipykernel_23665/2522857457.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CounterCNN().to(device)\n",
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.334368 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.33436842371934444, 0.6468039003250271)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out of sample test\n",
    "model.eval()\n",
    "# test(test_loader, model, loss_fn)\n",
    "test_counter(full_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 1, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 0, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 0, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 1, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 0, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # Get a single example from the test dataset\n",
    "    example_data, example_label = test_dataset[i]\n",
    "\n",
    "    # Move the example data to the appropriate device\n",
    "    example_data = example_data.unsqueeze(0).to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get the model's prediction\n",
    "    with torch.no_grad():\n",
    "        example_data = example_data.to(device)\n",
    "        output = model(example_data)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "    # Print the predicted class and the actual label\n",
    "    print(f'Predicted class: {predicted_class}, Actual label: {example_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for X, _ in test_loader:\n",
    "        X = X.to(device)\n",
    "        output = model(X)\n",
    "        # predicted_classes = output.argmax(dim=1)\n",
    "        # predictions.extend(predicted_classes.cpu().numpy())\n",
    "        # predicted_zones = torch.round(torch.clamp(output, min=0, max=1))\n",
    "        predicted_zones = torch.round(torch.sigmoid(output))\n",
    "        predictions.extend(predicted_zones.cpu().numpy())\n",
    "\n",
    "# Convert predictions to a numpy array\n",
    "y_preds = np.array(predictions)\n",
    "print(y_preds)\n",
    "\n",
    "y_test = np.array([y for _, y in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# FULL DATASET\n",
    "# Create a DataLoader for the entire dataset\n",
    "full_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predictions\n",
    "predictions = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for X, _ in full_loader:\n",
    "        X = X.to(device)\n",
    "        output = model(X)\n",
    "        # predicted_classes = output.argmax(dim=1)\n",
    "        # predictions.extend(predicted_classes.cpu().numpy())\n",
    "        # predicted_zones = torch.round(torch.clamp(output, min=0, max=1))\n",
    "        predicted_zones = torch.round(torch.sigmoid(output))\n",
    "        predictions.extend(predicted_zones.cpu().numpy())\n",
    "\n",
    "# Convert outputs to a numpy array\n",
    "y_preds = np.array(predictions)\n",
    "print(y_preds)\n",
    "\n",
    "y_test = np.array([y for _, y in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqAAAAJTCAYAAADXOqRyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCzklEQVR4nO3deXxU1d3H8e9kmwSUhBCygEDYtYKgLAHZIRq1IqhFcAESRKtFBKNYqOwqARdAFIpSAyiKKBUUUCxEFi1hKQgqVSw7BhIStmiQbHOfP3gYHRMgCZkzJPN593Vfr+bcc8/9TfrA8+N3lrFZlmUJAAAAMMTH0wEAAADAu5CAAgAAwCgSUAAAABhFAgoAAACjSEABAABgFAkoAAAAjCIBBQAAgFEkoAAAADCKBBQAAABGkYACMC47O1vDhg1T/fr15e/vL5vNpu3bt7v1ndHR0YqOjnbrOyqz8ePHy2azae3atZ4OBUAlQAIKeIGtW7fqwQcfVOPGjVW1alUFBQWpYcOG6t+/v1atWmU8nqefflozZsxQs2bNNHLkSI0bN06RkZHG4/Ck6Oho2Ww22Ww2ffvtt8X2KSwsVO3atZ399u/fX+b3zZs3TzabTfPmzSvzGABQXvw8HQAA93E4HHrqqac0bdo0+fn5qXv37rrjjjvk7++vvXv3asWKFVqwYIEmTpyoMWPGGItr+fLlatKkiZYtW2bsnSkpKcbeVVI+PmdrAMnJyZo6dWqR+59++qkOHz4sPz8/FRQUmA7PxWOPPaZ+/fqpbt26Ho0DQOVAAgpUYqNHj9a0adPUsmVLLV68WA0bNnS5/8svv+i1117TsWPHjMZ1+PBhde7c2eg7f//ZLwf+/v7q3LmzFixYoClTpsjf39/lfnJysoKDg9WiRQutX7/eQ1GeFRYWprCwMI/GAKDyYAoeqKR2796tF154QTVq1NDKlSuLTcCCgoI0YsQITZgwwaU9KytLw4cPV/369WW32xUeHq577rmn2Kni+Ph42Ww27du3TzNmzNDVV18tu92uevXqacKECXI4HEX6WpaldevWOaeWu3btKunC6wzPN4W8Zs0a3XrrrapVq5bsdrsiIiLUqVMnvfHGGy79zrcGNCcnR+PGjdPVV1+twMBAhYaG6o9//KP+/e9/F+n72/jeffddtWzZUkFBQYqKitKwYcP0yy+/FHnmYgYNGqTMzMwi1eDMzEwtX75c9957r4KCgoo8l5eXp1dffVVxcXGqU6eO83+nu+66S1999ZVL3/j4eCUkJEiSEhISnL93m83m7NO1a1fZbDadOXNGo0ePVsOGDeXv76/x48cX+eznPPLII7LZbJo8eXKR+M7dmzJlSql/JwAqPyqgQCU1b948FRYW6s9//rMiIiIu2Ndutzv/e2Zmptq3b689e/aoa9eu6tevn/bt26fFixdrxYoV+uyzz9SxY8ciY4wYMULr1q3T7bffrri4OC1dulTjx49XXl6enn/+eUlS7969FR0drQkTJqhevXqKj4+XpDJvDlqxYoV69uypkJAQ9erVS1FRUcrMzNSOHTv09ttv6+GHH77g82fOnFH37t21efNm3XDDDRo+fLgyMjK0aNEiffbZZ1q4cKH69OlT5LnXXntNK1euVK9evdS9e3etXLlSM2bMUFZWlt55551SfYY777xT1atX19y5c3XXXXc5299++23l5+dr0KBBxS6POH78uIYPH65OnTrptttuU/Xq1bV37159/PHH+vTTT7V+/Xq1adNG0tnf+8mTJ/XRRx+pV69eatmy5Xnjufvuu7Vjxw7dcsstCgkJUf369c/bd9q0aVq/fr3Gjh2rHj16ON+3ZMkSvf766+revbtGjBhRqt8HAC9hAaiUunbtakmyVq9eXarnEhISLEnWqFGjXNpXrFhhSbIaNWpkFRYWOtsHDhxoSbLq169vHT582NmemZlphYSEWFdeeaWVm5vrMpYkq0uXLkXePW7cOEuStWbNmiL35s6da0my5s6d62y76667LEnW9u3bi/TPyspy+blevXpWvXr1XNomTJhgSbLuv/9+y+FwONu3bdtmBQQEWCEhIVZ2dnaR+IKDg63vv//e2X769GmrSZMmlo+Pj5WWllYkluLUq1fPstvtlmVZ1mOPPWb5+flZR44ccd6/9tprrebNm1uWZVlxcXGWJGvfvn3O+2fOnLF+/PHHIuN+++231hVXXGHFxsa6tBf3+/utLl26WJKsli1bWseOHSty/3z/22zfvt2y2+1Ww4YNrZ9++sk6dOiQFRoaatWoUaPEvwsA3ocpeKCSSk9PlyRdddVVJX4mLy9PCxcuVI0aNTR69GiXe7fddptuuukm7d69u9jp6TFjxigqKsr5c1hYmHr16qWffvpJu3btKuOnKJnipqhr1Khx0efmz58vf39/TZ482WU6+vrrr9fAgQN18uRJLV26tMhzw4YNU9OmTV3ef++998rhcGjr1q2ljn/QoEEqKCjQ/PnzJUmbNm3Szp07NWjQoPM+Y7fbVbt27SLt1157rbp166b169crPz+/1LFMmDBBoaGhJe7fokULTZkyRXv27NGjjz6q/v376/jx40pOTlatWrVK/X4A3oEEFIDT999/rzNnzqht27aqUqVKkfvdunWTpGLP7GzVqlWRtnPJ78mTJ8s1znP69esnSWrXrp0ee+wxLVmyRFlZWSV6Njs7W3v37lWjRo2KTdJNftbrr79eLVu21Ny5cyWd3XwUEBCgBx544ILPbd++Xffdd5/q1q2rgIAA57rOZcuWKS8vr8S/i99q27ZtqZ95/PHHdeutt2rBggVau3atHn30Ud1xxx2lHgeA9yABBSqpc+dqpqWllfiZ7OxsSTrvmtFzFc5z/X6rWrVqRdr8/M4uMy8sLCxxDKXRp08fLV26VM2bN9fs2bN11113KTw8XD169LjowfaX22cdNGiQdu3apdWrV+u9995Tz549L7jrfMOGDWrXrp0+/PBDtWzZUkOHDtXYsWM1btw4tWjRQpKUm5tb6jgutl64ODabTb1793b+PHTo0FKPAcC7kIAClVSHDh0kle78y3OJVUZGRrH3z03rF5eAlYdz52IWd+blqVOnin2mV69eWrdunU6cOKFPP/1UgwcP1tq1a3XLLbdcsBrp6c/6e/fff7/sdrvi4+OVnZ2tBx988IL9n3/+eeXm5mr16tX6+OOP9fLLL2vChAkaP378JR3q/9ulCCW1b98+jRgxQqGhobLZbBo8eLDb/tEBoHIgAQUqqfj4ePn6+uqNN95QZmbmBfueq5SdO4poy5YtOn36dJF+547gudAu6ktRvXp1ScVXbX9/tNDvXXnllbrlllv0xhtvKD4+XhkZGdq0adN5+1erVk0NGjTQ7t27i32fuz/r74WGhqp3795KS0tT7dq1FRcXd8H+e/bsUWhoaJETCU6fPq1t27YV6e/r6yup/KvRBQUFuv/++/XTTz9p0aJFSkxM1IYNG4oc7QUAv0UCClRSjRo10tNPP62srCzdeuut2rdvX5E+Z86c0dSpU51nPQYEBOjee+9VVlaWkpKSXPquXLlSn332mRo1auSsrpa3c8f4vPXWWy7nh6amphZ7vNH69euLTaiOHj0qSQoMDLzg+wYOHKj8/HyNGjVKlmU527/++mvNmzdPwcHBLlPL7jZ58mQtWbJES5cudVaDz6devXo6ceKEdu7c6WwrLCzUU089Vew/OM5tLDp06FC5xjxhwgSlpqbqySefVGxsrCZNmqQbbrhBkyZN0hdffFGu7wJQeXAOKFCJPffcczpz5oymTZumpk2bqnv37mrWrJn8/f21b98+rV69WseOHdNzzz3nfGbKlClat26dnnvuOW3YsEExMTHav3+/PvjgA1WpUkVz5869aHJUVu3atVOHDh30+eefq3379urcubMOHDigjz76SD179tSSJUtc+j/++OM6fPiwOnbs6Pxu9S+//FKbN29Wu3btij2v9LeefvpprVixQm+//ba+++479ejRQ0ePHtWiRYtUUFCgOXPm6Morr3TLZy3O+Q7LL87QoUP1r3/9Sx07dtQ999yjwMBArV27VmlpaeratWuRw/zbt2+voKAgTZ8+XSdOnFDNmjUlqchpB6Wxfv16Z8J57qzXgIAAvfvuu2rVqpUeeOAB7dixQyEhIWV+B4DKiQooUIn5+Pho6tSp2rJli/r37689e/Zo1qxZmjZtmjZt2qS4uDitWrVKzzzzjPOZmjVratOmTXr88ce1Z88evfTSS1q1apV69+6tTZs2XTSpu1QfffSRBgwYoN27d2vmzJk6dOiQli1bVuyu6lGjRqlbt276+uuv9frrr+vNN99Ubm6upkyZolWrVjmnnc8nMDBQn3/+ucaMGaPs7GxNmzZNS5YsUZcuXbR27dpiD6G/XNx+++1avHixGjRooAULFujdd9/V1Vdfrc2bN6tevXpF+oeGhmrx4sVq0qSJ5syZozFjxhR7wH1JnThxQg888ICCgoK0cOFCBQQEOO81bdpU06dP18GDB/XQQw+V+R0AKi+b9dt5JwAAAMDNqIACAADAKBJQAAAAGEUCCgAAAKNIQAEAAGAUCSgAAACMIgEFAACAUSSgAAAAMIoEFAAAAEaRgAIAAMAoElAAAAAYRQIKAAAAo0hAAQAAYBQJKAAAAIwiAQUAAIBRJKAAAAAwigQUAAAARpGAAgAAwCgSUAAAABhFAgoAAACjSEABAABgFAkoAAAAjCIBBQAAgFEkoAAAADCKBBQAAABGkYACAADAKBJQAAAAGEUCCgAAAKP8PB1AZZCftdfTIQBwk6BanTwdAgA3KchL89i73Zk7+Ic1cNvY5YUEFAAAwDRHoacj8Cim4AEAAGAUFVAAAADTLIenI/AoKqAAAAAwigooAACAaQ4qoAAAAIAxVEABAAAMs1gDCgAAAJhDBRQAAMA0L18DSgIKAABgGlPwAAAAgDlUQAEAAEzjqzgBAAAAc6iAAgAAmMYaUAAAAMAcKqAAAACmefkxTFRAAQAAYBQVUAAAAMO8/as4SUABAABMYwoeAAAAMIcKKAAAgGlePgVPBRQAAABGUQEFAAAwja/iBAAAAMyhAgoAAGAaa0ABAAAAc6iAAgAAmObl54CSgAIAAJjGFDwAAABgDhVQAAAA07x8Cp4KKAAAAIyiAgoAAGCYZXEQPQAAAGAMFVAAAADT2AUPAAAAmEMFFAAAwDQv3wVPAgoAAGAaU/AAAACAOVRAAQAATHNwDBMAAABgDBVQAAAA01gDCgAAAJhDBRQAAMA0Lz+GiQooAAAAjKICCgAAYJqXrwElAQUAADCNKXgAAADAHCqgAAAAplEBBQAAAMyhAgoAAGCYZfFVnAAAAIAxVEABAABMYw0oAAAAYA4VUAAAANM4iB4AAABGMQUPAAAAmEMFFAAAwDQvn4KnAgoAAACjqIACAACYxhpQAAAAwBwqoAAAAKaxBhQAAAAwhwooAACAaV6+BpQEFAAAwDQvT0CZggcAAIBRVEABAABMYxMSAAAAYA4VUAAAANNYAwoAAACYQwUUAADANNaAAgAAAOZQAQUAADDNy9eAkoACAACYxhQ8AAAAYA4VUAAAANO8fAqeCigAAACMogIKAABgGhVQAAAAwBwqoAAAAKZZlqcj8CgqoAAAADCKCigAAIBprAEFAAAAzCEBBQAAMM3hcN9VSjNnzlR0dLQCAwMVExOjzZs3X7D/9OnT1bRpUwUFBalOnTp64okndObMmVK9kyl4AAAA0y6Tr+JctGiREhMTNXv2bMXExGj69OmKi4vTrl27FB4eXqT/u+++q5EjRyo5OVk33nijfvjhB8XHx8tms2nq1Kklfi8VUAAAAC81depUPfTQQ0pISNAf/vAHzZ49W1WqVFFycnKx/Tds2KAOHTrovvvuU3R0tG6++Wbde++9F62a/h4JKAAAgGlunILPzc1Vdna2y5Wbm1skhLy8PG3dulWxsbHONh8fH8XGxio1NbXYsG+88UZt3brVmXDu3btXn3zyiW677bZSfXwSUAAAgEokKSlJwcHBLldSUlKRfllZWSosLFRERIRLe0REhNLT04sd+7777tPEiRPVsWNH+fv7q2HDhuratav+9re/lSpGElAAAADTLMtt16hRo3Tq1CmXa9SoUeUS9tq1azVp0iTNmjVL27Zt04cffqgVK1bo2WefLdU4bEICAACoROx2u+x2+0X7hYWFydfXVxkZGS7tGRkZioyMLPaZMWPGqH///ho8eLAkqXnz5srJydHDDz+sZ555Rj4+JattUgEFAAAw7TI4hikgIECtWrVSSkrKb8JyKCUlRe3bty/2mdOnTxdJMn19fSVJVim+XpQKKAAAgJdKTEzUwIED1bp1a7Vt21bTp09XTk6OEhISJEkDBgxQ7dq1nWtIe/bsqalTp+r6669XTEyMdu/erTFjxqhnz57ORLQkSEABAABMu0y+irNv377KzMzU2LFjlZ6erpYtW2rlypXOjUkHDx50qXiOHj1aNptNo0ePVlpammrWrKmePXvq+eefL9V7bVZp6qUoVn7WXk+HAMBNgmp18nQIANykIC/NY+/+5R+Jbhs7aHDJD4T3FNaAAgAAwCim4AEAAAyzHN49AU0FFAAAAEZRAQUAADDtMtmE5ClUQAEAAGAUFVAAAADTLCqgAAAAgDFUQAEAAEzz8l3wJKAAAACmsQkJAAAAMIcKKAAAgGlUQAEAAABzqIACAACYZnn3JiQqoAAAADCKCigAAIBprAEFvNfCfy7TzXcP1A3d7tC9Dw3XN//ddd6++QUF+nvyO7qlT4Ju6HaH7hr4F3258T8ufQoLC/XqG28p7k/xatWtl27pk6DZc9+V5eVTLYAnPPrIQO3+YaN+zt6jDV8uU5vWLS/Y/+67b9e336zTz9l79NW21br1lu7Oe35+fkqa9Dd9tW21Tp34nw7u36q5ya8oKirCzZ8CqJxIQOG1Pl29Ti+8+oYeHXS/Pkh+VU0b1defE0fr2ImTxfZ/9Y35+uCjT/W3Jx7VRwte1z29b9OwUc/qux92O/u8ueADLVq6Qn9L/Is+fvcNJf5lkJLfWax3Fn9s6FMBkKQ+fe7QSy+O07PPTVWbmFu04+v/6pMV76hmzRrF9m/frrXeeXum5s5dqNZt4/Txx5/pn4vf1LXXNpUkVakSpOtbNtfzk15Rm5hb1Oeeh9S0SQMt+XCuyY+FysRhue+qAGwWpZlLlp+119MhoAzufWi4ml3dRM88+RdJksPhUOydA3Tfn+7Q4P73FOnf7Y779fDAfrr37p7OtuF/e052e4CmjHtakvSXEeNUIzREz4564rx9ULEE1erk6RBQBhu+XKYt/9mhYcNHS5JsNpv2792imbPm6oUXZxbp/+47f1fVKlXU686BzrZ/f7FM23fs1JDHRhb7jtatWmhj6ieq37CNDh067J4PArcqyEvz2LtPvzjIbWNXGZHstrHLCxVQeKX8/Hz9d9f/1K5NS2ebj4+P2rVuqR3fflfsM3n5+QoICHBps9sD9NXXO50/t2x2jTb9Z7v2H/xRkvT9//Zq29c71ald6/L/EACK5e/vrxtuuE4pn3/hbLMsSymff6l27VoV+0y7mFYu/SXpX6vWnre/JAUHV5PD4dDJk9nlEzjgRbxqE1JWVpaSk5OVmpqq9PR0SVJkZKRuvPFGxcfHq2bNmh6OEKacOJmtwkKHaoRWd2mvEVpd+/4/efy9DjGt9NZ7H6p1y2aqUztKG/+zXSnrNqjQUejsM7j/Pco5fVo973tYvj4+KnQ49PjDA3V7XPdixwRQ/sLCQuXn56ejGVku7UePZurqpg2LfSYysqYyjma6tGVkZCkyovj/v2C32zVp0t/03qKl+umnn8sncHiXCjJV7i5ek4Bu2bJFcXFxqlKlimJjY9WkSRNJUkZGhmbMmKHJkyfrs88+U+vWF65U5ebmKjc316XNJzdXdrvdbbHj8jBy2J81fsoM9bzvYdlsUp1aUer9x5u0ZPm/nH1Wfr5ey/+1RlPGP61G9evp+//t1ZRXXld4WKh63XaTB6MHUF78/Pz03sLZstlsGvLYKE+HA1RIXpOADh06VH369NHs2Wf/0vgty7L0yCOPaOjQoUpNTb3gOElJSZowYYJL2+gRj2vs08PKPWa4T/WQavL19dGx4ydc2o8dP6Gw31VFzwmtHqIZk8cqNzdPJ7OzFR5WQ9P+nqyrakU6+7w8800NfuAe3RbbVZLUpGF9HUk/qn+8/T4JKGBIVtZxFRQUKDwizKU9PLym0jMyi30mPT1TEeGu1c6IiLAi/c8ln3XrXqWbbr6H6ifKzOIYJu+wY8cOPfHEE0WST+ns4vQnnnhC27dvv+g4o0aN0qlTp1yuvw57xA0Rw538/f31h6aNtek/251tDodDm7ZuV4tm11zwWbs9QBE1w1RQWKhVa/+tbp3aO++dOZMrm4/r/435+PjIwV4/wJj8/Hxt2/a1unfr6Gyz2Wzq3q2jNm7cWuwzGzdtVffuHV3aYnt0dul/Lvls1Ki+4m7pq+O/+wcsgJLzmgpoZGSkNm/erKuvvrrY+5s3b1ZExMXPc7Pb7UWm2/Pzss7TG5ezAX3v1DPPv6xrr26sZn9oqgXvL9UvZ3LV+49nK5Wjnn1J4WE19MSjCZKkr3d+r4zMY7q6cQMdzTymWckLZFmWBt3/J+eYXTvEaM789xQVEa5G9evpux92661FH+rOP97skc8IeKtpr8zR3Denaeu2r7Vly1d6fOhDqlo1SPPmL5IkzU1+RYcPH9EzoydLkl599U19nrJYTwz/sz75dLX63tNLrVpdp0f+cvb0Cj8/P72/6A1d37K5et05UL6+vor4//Whx4+fVH5+vmc+KCou1oB6h6eeekoPP/ywtm7dqh49ejiTzYyMDKWkpGjOnDl66aWXPBwlTLo1totOnDyl1/6xQFnHj+vqxg01++VnnVPwRzKOyuc3FfPcvDy9Ome+fjycripBQerUvo2SxoxQtSuvcPb52xOP6tU5b+m5l2bq+ImTqhkWqj69btOjCfcZ/3yAN/vgg49VMyxU48c+pcjImtqxY6f+ePsDOnr0bMGgbp1acvxmCjR143/0wIDHNHHC03ru2b/qf7v36e4/PaidO89+OUXt2pG6o2ecJGnbf1a5vKtH7J+0bv2Fl28BcOVV54AuWrRI06ZN09atW1VYeHbnsq+vr1q1aqXExETdc0/Rsx9LgnNAgcqLc0CBysuT54DmPPeA28auOnqB28YuL15TAZWkvn37qm/fvsrPz1dW1tl/BYeFhcnf39/DkQEAAK/CFLz38ff3V1RUlKfDAAAA8EpemYACAAB4FMcwAQAAAOZQAQUAADDNy9eAUgEFAACAUVRAAQAATLNYAwoAAAAYQwUUAADANC9fA0oCCgAAYJjFMUwAAACAOVRAAQAATPPyKXgqoAAAADCKCigAAIBpVEABAAAAc6iAAgAAmMZB9AAAAIA5VEABAABM8/I1oCSgAAAAhllenoAyBQ8AAACjqIACAACYRgUUAAAAMIcKKAAAgGkOjmECAAAAjKECCgAAYBprQAEAAABzqIACAACY5uUVUBJQAAAAwyzLuxNQpuABAABgFBVQAAAA07x8Cp4KKAAAAIyiAgoAAGAaFVAAAADAHCqgAAAAhllUQAEAAABzqIACAACYRgUUAAAAMIcKKAAAgGkOTwfgWSSgAAAAhrEJCQAAADCICigAAIBpVEABAAAAc6iAAgAAmOblm5CogAIAAMAoKqAAAACGsQseAAAAMIgKKAAAgGlevgaUBBQAAMAwpuABAAAAg6iAAgAAmOblU/BUQAEAAGAUFVAAAADDLCqgAAAAgDlUQAEAAEyjAgoAAACYQwUUAADAMG9fA0oCCgAAYJqXJ6BMwQMAAMAoKqAAAACGefsUPBVQAAAALzZz5kxFR0crMDBQMTEx2rx58wX7nzx5UkOGDFFUVJTsdruaNGmiTz75pFTvpAIKAABg2OVSAV20aJESExM1e/ZsxcTEaPr06YqLi9OuXbsUHh5epH9eXp5uuukmhYeHa/Hixapdu7YOHDigkJCQUr3XZlmWVU6fwWvlZ+31dAgA3CSoVidPhwDATQry0jz27qM9urht7PCUdSXuGxMTozZt2ui1116TJDkcDtWpU0dDhw7VyJEji/SfPXu2XnzxRX3//ffy9/cvc4xMwQMAABhmOdx35ebmKjs72+XKzc0tEkNeXp62bt2q2NhYZ5uPj49iY2OVmppabNwff/yx2rdvryFDhigiIkLNmjXTpEmTVFhYWKrPTwIKAABQiSQlJSk4ONjlSkpKKtIvKytLhYWFioiIcGmPiIhQenp6sWPv3btXixcvVmFhoT755BONGTNGL7/8sp577rlSxcgaUAAAANMsm9uGHjVqlBITE13a7HZ7uYztcDgUHh6uN954Q76+vmrVqpXS0tL04osvaty4cSUehwQUAADAMHduQrLb7SVKOMPCwuTr66uMjAyX9oyMDEVGRhb7TFRUlPz9/eXr6+tsu+aaa5Senq68vDwFBASUKEam4AEAALxQQECAWrVqpZSUFGebw+FQSkqK2rdvX+wzHTp00O7du+Vw/JpB//DDD4qKiipx8imRgAIAABhnOWxuu0ojMTFRc+bM0fz58/Xdd9/p0UcfVU5OjhISEiRJAwYM0KhRo5z9H330UR0/flzDhg3TDz/8oBUrVmjSpEkaMmRIqd7LFDwAAICX6tu3rzIzMzV27Filp6erZcuWWrlypXNj0sGDB+Xj82u9sk6dOvrss8/0xBNP6LrrrlPt2rU1bNgw/fWvfy3VezkHtBxwDihQeXEOKFB5efIc0MM3dnPb2LU2rHHb2OWFKXgAAAAYxRQ8AACAYZYbj2GqCKiAAgAAwCgqoAAAAIa58xzQioAEFAAAwLDSHpdU2TAFDwAAAKOogAIAABjm7YdgUgEFAACAUVRAAQAADGMNKAAAAGAQFVAAAADDqIACAAAABlEBBQAAMMzbd8GXKAE9ePBgmV9Qt27dMj8LAABQGXn7FHyJEtDo6GjZbKX/RdlsNhUUFJT6OQAAAFReJUpABwwYUKYEFAAAAEVZlnfnVSVKQOfNm+fmMAAAAOAt2IQEAABgmOXwdASexTFMAAAAMKrMFdDCwkK9//77Wr16tQ4fPqzc3NwifWw2m1JSUi4pQAAAgMrGwRrQ0svJydHNN9+sjRs3yrIs2Ww2Wb850Orcz2xcAgAAwO+VaQr+ueeeU2pqqiZMmKCsrCxZlqXx48fryJEjWrRokRo0aKA+ffoUWxUFAADwdpZlc9tVEZQpAf3www/Vrl07jR49WqGhoc72iIgI9enTR2vWrNHq1av14osvllugAAAAlYXlsLntqgjKlIAePHhQ7dq1+3UQHx+XaudVV12lP/7xj5o/f/6lRwgAAIBKpUxrQKtWrSofn19z1+DgYB05csSlT2Rk5CV9hScAAEBl5e3fBV+mCmi9evVckstmzZrp888/d1ZBLctSSkqKoqKiyidKAAAAVBplSkB79OihNWvWOL/nfeDAgTp48KDat2+vESNGqGPHjtq+fbvuvvvucg0WAACgMvD2NaBlmoJ/6KGHVKNGDWVmZioqKkqDBg3SV199pVmzZmn79u2SpLvvvlvjx48vx1ABAABQGdgsq/xWIWRmZmrv3r2qV6+eIiMjy2vYy15+1l5PhwDATYJqdfJ0CADcpCAvzWPv/rbB7W4bu9ne5W4bu7yU63fB16xZUzVr1izPIQEAAFDJlGsCCgAAgIurKAfGu0uZEtAGDRqUqJ/NZtOePXvK8goAAIBKy9uPYSpTAupwOIr9nvdTp07p5MmTkqSoqCgFBARcUnAAAACofMqUgO7fv/+C9xITE5WRkaFVq1aVNS4AAIBKy+HlU/BlOgf0QqKjo7Vo0SKdOHFCzzzzTHkPDwAAgAqu3BNQSfL399dNN92k999/3x3DAwAAVGiWZXPbVRG4JQGVpNOnT+v48ePuGh4AAAAVlFuOYfriiy+0cOFCNW3a1B3DAwAAVGjsgi+D7t27F9teUFCgtLQ05yalsWPHljkwAAAAVE5lSkDXrl1bbLvNZlP16tV18803KzExUTfddNOlxAYAAFApefsu+DKfAwoAAACUBV/FWQ6+veEJT4cAwE2+DIvxdAgAKqGKslvdXcq0C75BgwaaMWPGBfvMnDmzxF/ZCQAA4E0cls1tV0VQpgR0//79zq/cPJ+TJ0/qwIEDZRkeAAAAlZjbpuBPnTolu93uruEBAAAqLC8/hankCej69etdft6/f3+RNkkqLCzUoUOH9M4776hJkyaXHiEAAAAqlRInoF27dpXNdnZdgc1m0/z58zV//vxi+1qWJZvNpsmTJ5dPlAAAAJVIRVmr6S4lTkDHjh0rm80my7I0ceJEdenSRV27di3Sz9fXV6GhoerWrZuuueaa8owVAAAAlUCJE9Dx48c7//u6deuUkJCgAQMGuCMmAACASs3bj2Eq0yakNWvWlHccAAAA8BJlOoZpw4YNSkxMVHp6erH3jxw5osTERG3cuPGSggMAAKiMHG68KoIyJaAvv/yyli1bpsjIyGLvR0VFafny5Zo2bdolBQcAAFAZWbK57aoIypSAbtmyRR07drxgn86dO1MBBQAAQBFlWgN69OhR1a5d+4J9IiMjdfTo0TIFBQAAUJk5vPwk+jJVQENCQnTw4MEL9jlw4ICuuOKKMgUFAACAyqtMCWi7du20ZMkSHTp0qNj7Bw8e1NKlS3XjjTdeUnAAAACVkUM2t10VQZkS0MTERJ0+fVodOnTQW2+9pSNHjkg6u/t9/vz56tChg3755Rc9+eST5RosAAAAKr4yrQHt3Lmzpk6dqieffFIJCQmS5PyWJEny8fHRK6+8os6dO5dfpAAAAJVERdmt7i5lSkAladiwYerWrZtmz56tLVu26NSpUwoJCVHbtm31yCOPqFmzZsrNzZXdbi/PeAEAAFDBlTkBlaTrrrtOs2bNKtK+bds2DRkyRO+9956OHTt2Ka8AAACodCrKgfHuckkJ6G+dPHlSCxYs0Jtvvqmvv/5almUpKCiovIYHAACoNJiCv0SrV6/Wm2++qY8++ki5ubmyLEvt27dXQkKC+vbtWx4xAgAAoBIpUwJ66NAhzZ07V3PnztXBgwdlWZZq166ttLQ0xcfHKzk5ubzjBAAAqDSYgi+h/Px8LV26VG+++aZSUlJUWFioqlWr6v7779eAAQPUvXt3+fn5yc+v3Gb1AQAAUAmVOFusVauWjh8/LpvNpm7dumnAgAG66667VLVqVXfGBwAAUOlQAS2hY8eOycfHR0888YSefvpp1axZ051xAQAAoJIq8TchxcfHKygoSFOnTtVVV12lO+64Qx988IHy8vLcGR8AAEClY8nmtqsiKHECmpycrCNHjuj111/XDTfcoOXLl6tfv36KiIjQn//8Z3355ZfujBMAAACVRKm+C/6KK67Q4MGDlZqaqp07d2r48OEKCAjQnDlz1KVLF9lsNu3atUsHDhxwV7wAAAAVnsPmvqsiKFUC+lvXXHONXn75ZaWlpen999/XzTffLJvNpi+++EINGzZUjx499Pbbb5dnrAAAAJWCQza3XRWBzbIsq7wG+/HHHzV37lzNmzdP+/btk81mU2FhYXkNf9n6qm4vT4cAwE1yC3w9HQIAN2l3+EOPvfujyPvcNnav9HfdNnZ5KXMFtDhXXXWVxowZoz179mjVqlXq169feQ4PAABQKVhuvCoCt50a36NHD/Xo0cNdwwMAAKCC4muLAAAADPP2g+jLdQoeAAAAuBgqoAAAAIY5bBVjt7q7UAEFAACAUVRAAQAADKsou9XdhQQUAADAMDYhAQAAAAZRAQUAADCsonxnu7tQAQUAAIBRVEABAAAMc8i7S6BUQAEAAGAUCSgAAIBhlhuv0po5c6aio6MVGBiomJgYbd68uUTPvffee7LZbOrdu3ep30kCCgAA4KUWLVqkxMREjRs3Ttu2bVOLFi0UFxeno0ePXvC5/fv366mnnlKnTp3K9F4SUAAAAMMcNvddpTF16lQ99NBDSkhI0B/+8AfNnj1bVapUUXJy8nmfKSws1P33368JEyaoQYMGZfr8JKAAAACGOdx45ebmKjs72+XKzc0tEkNeXp62bt2q2NhYZ5uPj49iY2OVmpp63tgnTpyo8PBwPfjgg2X+/CSgAAAAlUhSUpKCg4NdrqSkpCL9srKyVFhYqIiICJf2iIgIpaenFzv2l19+qTfffFNz5sy5pBg5hgkAAMAwd34X/KhRo5SYmOjSZrfbL3ncn376Sf3799ecOXMUFhZ2SWORgAIAAFQidru9RAlnWFiYfH19lZGR4dKekZGhyMjIIv337Nmj/fv3q2fPns42h+Pst9r7+flp165datiwYYliZAoeAADAsMthE1JAQIBatWqllJSUX+NyOJSSkqL27dsX6X/11Vfrm2++0fbt253XHXfcoW7dumn79u2qU6dOid9NBRQAAMBLJSYmauDAgWrdurXatm2r6dOnKycnRwkJCZKkAQMGqHbt2kpKSlJgYKCaNWvm8nxISIgkFWm/GBJQAAAAwxyeDuD/9e3bV5mZmRo7dqzS09PVsmVLrVy50rkx6eDBg/LxKf8Jc5tlWe5cB+sVvqrby9MhAHCT3AJfT4cAwE3aHf7QY++ec9UDbhv7oR8XuG3s8kIFFAAAwLDLpQLqKSSgAAAAhlml/MaiyoZd8AAAADCKCigAAIBh3j4FTwUUAAAARlEBBQAAMIwKKAAAAGAQFVAAAADDvP0QdiqgAAAAMIoKKAAAgGEOLz8HlAQUAADAMDYhAQAAAAZRAQUAADCMCigAAABgEBVQAAAAwziGCQAAADCICigAAIBh3n4MExVQAAAAGEUFFAAAwDBv3wVPAgoAAGAYm5AAAAAAg6iAAgAAGObw8hooFVAAAAAYRQUUAADAMG/fhEQFFAAAAEZRAQUAADDMu1eAUgEFAACAYVRAAQAADGMNKAAAAGAQFVAAAADDHDZPR+BZJKAAAACGcRA9AAAAYBAVUAAAAMO8u/5JBRQAAACGUQEFAAAwjGOYAAAAAIOogAIAABjGLngAAADAICqgAAAAhnl3/ZMEFAAAwDg2IQEAAAAGUQEFAAAwjE1IAAAAgEFUQAEAAAzz7vonFVAAAAAYRgUUAADAMHbBAwAAAAZRAQUAADDM8vJVoCSgAAAAhjEFDwAAABhEBRQAAMAwDqIHAAAADKICCgAAYJh31z+pgAIAAMAwKqAAAACGefsaUBJQeLWwAbcp/M+95V+zun75br9+HPuGTu/4X7F9Q//UXfWmDnNpc5zJ044mfZw/B9/STmEP3KIqzRvKr3o1fX/LcP3y331u/QwAihcRf4tqPdpb/jVDdPq/+7Vv9D+Us333RZ+r0auDGv/9SR1fuUk/DJribPcPC1bdZ/oruEtL+QZX1U8b/6v9o/+hM/uOuPNjAJUSU/DwWiE9O6r2mEFKn75Iu/6YqF++26eGC8bLr0bweZ8pzM7RN60GOq+dNw52ue9TJVA5W77T4aS33B0+gAuocUcH1RuXoB+nvq9v4p5Szn/365p3x17wz7ck2a+qqbpj4pW9cWeRe02SR8peL0K7Eibrm5ufVO6Pmbpm0Xj5BNnd9TFQiTnceFUEJKDwWuGDe+nYwn/p+AcpOvO/Qzo06u9y/JKrGn1jz/uMZVkqyDz565V1yuX+iQ/XKv2VRfrpyx3uDh/ABUQ93FNH312lzEWf65f//ah9f31djl9yFX5v9/M/5OOjRjOf0I8vv6fcAxkutwIbROnK1k21b+QbytmxW2f2HNa+ka/LJzBANe7s5OZPg8rIcuN/KgISUHglm7+fqjRv6JooWpZ++nKHqtzQ9LzP+VYN0rUb5ujajW+q/j/+psAmdQxEC6A0bP5+qnpdQ5364utfGy1Lp774Wle0Ov+f76sS+yg/65QyF6YUHTPAX5LkyM1zGdORl69qba4ut9gBb0ECCq/kG1pNNj9f5WeddGkvyDop/5rVi33mzN40HRzxqvYOnqQDw6bK5mNTkw+nyD+yhoGIAZSUX+iVZ/98Z550ac/POqmAmiHFPnNl26tVs1+s9o6YVez9M7vTlPtjpuqOekC+wVVl8/dTrSF3yl4rTP4Rxf+dAVwIU/CQJB06dEiDBg26aL/c3FxlZ2e7XHlWoYEI4Wmnt+3S8X+u0S//3aefN+3U3ocnq+B4tsLuj/N0aAAugU/VQDWcMUz7RsxSwfGfiu1jFRTqhwenKLBhLbX57m213bNQ1W5sphMpWyVHxZjyBC4n7IL/f8ePH9f8+fOVnJx8wX5JSUmaMGGCS9vD1ZrokWCmYCqSwuPZsgoK5R8W4tLuFxai/MwTJRukoFCnd+6VPTqq/AMEUGYFx386++f7d9VO/7AQ5f2uKipJgdGRCqwboabz//Zro49NkhRz8ANt7/SYcg9kKOebvfrmpifle2UV2fz9VHA8W82WT9bPX+9x46dBZVVR1mq6i9ckoB9//PEF7+/du7dE44waNUqJiYkubd9de1+Z44JnWPkFOv3NHl3Z4Tqd+tems402m67scJ2y5n9SskF8fBTUtJ6y12x1X6AASs3KL1DO13sU3PE6nVi5+WyjzaZqHa9Txryif75/2Z2mHd2Gu7TV+eu98q0apP1jk5V3+JjLvcKfTkuSAutHqWqLhjr04kK3fA6gMvOaBLR3796y2WyyrPP/i8Nms110HLvdLrvd9ciNAJvvJccH847+4yPVe3mYTn+zWznb/6fwB3vKp0qgjr2/WpJUb9pw5aUf05Epb0uSIof1Vc62Xco9cES+1aoq4s93KuCqmjr23irnmL7BVyigdk35R4RKkuwNa0uS8jNPqKCYygsA9zjyxjI1nD5UP+/YrZ+/+p+iHuop3yp2Zb73uSSp4SuPKy/9mA4lvSMrN1+/7Dro8nzhqRxJcmkPvb29Co5lKzctS1WuqavoiQ/q+MrNOrWOUy9QehVlraa7eE0CGhUVpVmzZqlXr17F3t++fbtatWplOCp40sllX8ovtJqiEu+TX83q+uW/+7Sn/wTn0Ur+tcJkOX79K8I3+ArVnTJEfjWrq/DUzzr9zR79cOdfdeZ/h5x9gm9q63JYff2ZIyRJR6YtVPq09wx9MgDHPv63/GpUU50R9549iH7nPn1//7PK//8/3/baYZKjdClAQER11RufIP+wYOUfPanMD9YqbfoH7ggfqPRs1oVKgpXIHXfcoZYtW2rixInF3t+xY4euv/56OUr5F5IkfVW3+KQWQMWXW8AMB1BZtTv8ocfe3b/eXW4b++0DnvtcJeU1FdARI0YoJyfnvPcbNWqkNWvWGIwIAAB4K6+o/l2A1ySgnTpd+Jsqqlatqi5duhiKBgAAwHt5TQIKAABwuXB4eQ2Ug+gBAABgFBVQAAAAw7z9IHoqoAAAADCKCigAAIBh3n4QPRVQAAAAGEUFFAAAwDBv3wVPAgoAAGAYm5AAAAAAg6iAAgAAGMYmJAAAAMAgKqAAAACGWRZrQAEAAABjqIACAAAY5u3HMFEBBQAAgFFUQAEAAAxjFzwAAACMstz4n9KaOXOmoqOjFRgYqJiYGG3evPm8fefMmaNOnTqpevXqql69umJjYy/Y/3xIQAEAALzUokWLlJiYqHHjxmnbtm1q0aKF4uLidPTo0WL7r127Vvfee6/WrFmj1NRU1alTRzfffLPS0tJK9V6b5e3nAJSDr+r28nQIANwkt8DX0yEAcJN2hz/02Ltvq3ub28b+5OAnJe4bExOjNm3a6LXXXpMkORwO1alTR0OHDtXIkSMv+nxhYaGqV6+u1157TQMGDCjxe6mAAgAAeKG8vDxt3bpVsbGxzjYfHx/FxsYqNTW1RGOcPn1a+fn5Cg0NLdW72YQEAABgmDsnoHNzc5Wbm+vSZrfbZbfbXdqysrJUWFioiIgIl/aIiAh9//33JXrXX//6V9WqVcsliS0JKqAAAACVSFJSkoKDg12upKSkcn/P5MmT9d5772nJkiUKDAws1bNUQAEAAAxz5zFMo0aNUmJiokvb76ufkhQWFiZfX19lZGS4tGdkZCgyMvKC73jppZc0efJkrV69Wtddd12pY6QCCgAAUInY7XZVq1bN5SouAQ0ICFCrVq2UkpLibHM4HEpJSVH79u3PO/4LL7ygZ599VitXrlTr1q3LFCMVUAAAAMPKcl6nOyQmJmrgwIFq3bq12rZtq+nTpysnJ0cJCQmSpAEDBqh27drOKfwpU6Zo7NixevfddxUdHa309HRJ0hVXXKErrriixO8lAQUAADDscvku+L59+yozM1Njx45Venq6WrZsqZUrVzo3Jh08eFA+Pr9OmP/9739XXl6e/vSnP7mMM27cOI0fP77E7+Uc0HLAOaBA5cU5oEDl5clzQGPrxLlt7NWHPnPb2OWFCigAAIBh3l7/YxMSAAAAjKICCgAAYNjlsgbUU6iAAgAAwCgqoAAAAIZdLscweQoVUAAAABhFBRQAAMAwB7vgAQAAAHOogAIAABjm3fVPElAAAADjOIYJAAAAMIgKKAAAgGFUQAEAAACDqIACAAAYZnEMEwAAAGAOFVAAAADDWAMKAAAAGEQFFAAAwDDLyyugJKAAAACGsQkJAAAAMIgKKAAAgGFsQgIAAAAMogIKAABgGGtAAQAAAIOogAIAABjGGlAAAADAICqgAAAAhnEQPQAAAIxysAkJAAAAMIcKKAAAgGHePgVPBRQAAABGUQEFAAAwjDWgAAAAgEFUQAEAAAxjDSgAAABgEBVQAAAAw7x9DSgJKAAAgGFMwQMAAAAGUQEFAAAwzNun4KmAAgAAwCgqoAAAAIaxBhQAAAAwiAooAACAYZbl8HQIHkUFFAAAAEZRAQUAADDM4eVrQElAAQAADLM4hgkAAAAwhwooAACAYd4+BU8FFAAAAEZRAQUAADCMNaAAAACAQVRAAQAADHNQAQUAAADMoQIKAABgmOXlu+BJQAEAAAxjExIAAABgEBVQAAAAwziIHgAAADCICigAAIBhrAEFAAAADKICCgAAYBgH0QMAAAAGUQEFAAAwzNvXgJKAAgAAGMYxTAAAAIBBVEABAAAM8/YpeCqgAAAAMIoKKAAAgGEcwwQAAAAYRAUUAADAMItd8AAAAIA5VEABAAAM8/Y1oCSgAAAAhnEMEwAAAGAQFVAAAADD2IQEAAAAGEQFFAAAwDDWgAIAAAAGUQEFAAAwjAooAAAAYBAVUAAAAMO8u/5JBRQAAACG2SxvX4QAlEJubq6SkpI0atQo2e12T4cDoBzx5xswhwQUKIXs7GwFBwfr1KlTqlatmqfDAVCO+PMNmMMUPAAAAIwiAQUAAIBRJKAAAAAwigQUKAW73a5x48axQQGohPjzDZjDJiQAAAAYRQUUAAAARpGAAgAAwCgSUAAAABhFAgoAAACjSECBEpo5c6aio6MVGBiomJgYbd682dMhASgH69evV8+ePVWrVi3ZbDYtXbrU0yEBlR4JKFACixYtUmJiosaNG6dt27apRYsWiouL09GjRz0dGoBLlJOToxYtWmjmzJmeDgXwGhzDBJRATEyM2rRpo9dee02S5HA4VKdOHQ0dOlQjR470cHQAyovNZtOSJUvUu3dvT4cCVGpUQIGLyMvL09atWxUbG+ts8/HxUWxsrFJTUz0YGQAAFRMJKHARWVlZKiwsVEREhEt7RESE0tPTPRQVAAAVFwkoAAAAjCIBBS4iLCxMvr6+ysjIcGnPyMhQZGSkh6ICAKDiIgEFLiIgIECtWrVSSkqKs83hcCglJUXt27f3YGQAAFRMfp4OAKgIEhMTNXDgQLVu3Vpt27bV9OnTlZOTo4SEBE+HBuAS/fzzz9q9e7fz53379mn79u0KDQ1V3bp1PRgZUHlxDBNQQq+99ppefPFFpaenq2XLlpoxY4ZiYmI8HRaAS7R27Vp169atSPvAgQM1b9488wEBXoAEFAAAAEaxBhQAAABGkYACAADAKBJQAAAAGEUCCgAAAKNIQAEAAGAUCSgAAACMIgEFAACAUSSgAAAAMIoEFAAAAEaRgAIAAMAoElAAAAAYRQIKAAAAo0hAAQAAYBQJKAAAAIwiAQUAAIBRJKAAAAAwigQUAAAARpGAAgAAwCgSUAAAABhFAgoAAACjSEABAABgFAkoAAAAjCIBBQAAgFEkoAAAADCKBBQAAABGkYACAADAKBJQAAAAGEUCCgAAAKNIQAEAAGAUCSgAr7d//37ZbDbFx8e7tHft2lU2m80zQZVSdHS0oqOjPR0GAJQICSgAo84le7+9AgICVKdOHd133336+uuvPR1iuYmPj5fNZtP+/fs9HQoAXFb8PB0AAO/UsGFDPfDAA5Kkn3/+WRs3btTChQv14YcfKiUlRR06dPBwhNJbb72l06dPezoMAKh0SEABeESjRo00fvx4l7bRo0fr+eef1zPPPKO1a9d6JK7fqlu3rqdDAIBKiSl4AJeNoUOHSpK2bNkiSbLZbOratavS0tI0YMAARUZGysfHxyU5Xb9+vXr27KmwsDDZ7XY1btxYo0ePLrZyWVhYqClTpqhRo0YKDAxUo0aNlJSUJIfDUWw8F1oD+tFHH+nmm29WjRo1FBgYqOjoaPXv31/ffvutpLNrMufPny9Jql+/vnO5QdeuXV3G2bdvnwYPHqy6devKbrcrKipK8fHxOnDgwHnf26ZNGwUFBSkiIkIPPfSQTpw4cf5fKgBchqiAArjs/DbpO3bsmNq3b6/Q0FD169dPZ86cUbVq1SRJf//73zVkyBCFhISoZ8+eCg8P13/+8x89//zzWrNmjdasWaOAgADnWA8//LCSk5NVv359DRkyRGfOnNHUqVO1YcOGUsX35JNPaurUqQoNDVXv3r0VHh6uQ4cOafXq1WrVqpWaNWum4cOHa968edqxY4eGDRumkJAQSXLZKLRp0ybFxcUpJydHt99+uxo3bqz9+/frnXfe0aeffqrU1FQ1aNDA2f+tt97SwIEDVa1aNfXv318hISFavny5YmNjlZeX5/JZAeCyZgGAQfv27bMkWXFxcUXujR071pJkdevWzbIsy5JkSbISEhKsgoICl747d+60/Pz8rBYtWlhZWVku95KSkixJ1ksvveRsW7NmjSXJatGihfXzzz8723/88UcrLCzMkmQNHDjQZZwuXbpYv/9rctmyZZYkq3nz5kXem5+fb6Wnpzt/HjhwoCXJ2rdvX5HPmpeXZ0VHR1tXXnmltW3bNpd7X3zxheXr62vdfvvtzrZTp05Z1apVs6pWrWrt2rXLZZzOnTtbkqx69eoVeQ8AXI6YggfgEbt379b48eM1fvx4jRgxQp07d9bEiRMVGBio559/3tkvICBAL7zwgnx9fV2ef/3111VQUKBXX31VNWrUcLn39NNPq2bNmlq4cKGz7a233pIkjR07VlWrVnW2165dW8OGDStx3LNmzZIkvfLKK0Xe6+fnp4iIiBKNs3z5cu3fv18jRozQ9ddf73KvY8eO6tWrlz755BNlZ2dLkpYuXars7GwNGjRITZo0cfb19/d3+X0BQEXAFDwAj9izZ48mTJgg6WwSFRERofvuu08jR45U8+bNnf3q16+vsLCwIs9v3LhRkvTZZ58pJSWlyH1/f399//33zp937NghSerUqVORvsW1nc/mzZtlt9vVpUuXEj9TnHPx79q1q8hmLElKT0+Xw+HQDz/8oNatW18w/vbt28vPj7/OAVQc/I0FwCPi4uK0cuXKi/Y7X0Xx+PHjklTi6t+pU6fk4+NTbDJb0qrluXFq164tH59Lm0A6F/8777xzwX45OTnO90pSeHh4kT6+vr5FqrEAcDljCh7AZe18u9DPbUTKzs6WZVnnvc4JDg6Ww+FQVlZWkbEyMjJKHE9ISIizOnkpzsW/bNmyC8Z/rtIaHBwsSTp69GiRsQoLC3Xs2LFLigcATCIBBVAhxcTESPp1KvtiWrRoIUn64osvitwrru182rZtq9zcXK1bt+6ifc+tWy0sLCxy71z8qampJXrvheJPTU1VQUFBicYBgMsBCSiACukvf/mL/Pz8NHToUB08eLDI/ZMnT+qrr75y/ty/f39J0sSJE53T2pKUlpamV155pcTvHTJkiCRp2LBhzmn0cwoKClyqqaGhoZKkQ4cOFRmnV69eqlu3rqZOnar169cXuZ+fn68vv/zSpX+1atWUnJysH374waXf6NGjSxw/AFwOWAMKoEJq1qyZZs2apUcffVRNmzbVbbfdpoYNG+qnn37S3r17tW7dOsXHx2v27NmSpG7duikhIUFz585V8+bNdeeddyo3N1eLFi1Su3bttHz58hK997bbbtNTTz2ll156SY0bN9add96p8PBwpaWlKSUlRU899ZSGDx8uSerevbteeuklPfzww7r77rtVtWpV1atXT/3795fdbtfixYt16623qkuXLurevbuaN28um82mAwcO6IsvvlCNGjWcG6mCg4M1Y8YMxcfHq02bNurXr5+Cg4O1fPlyBQUFKSoqyi2/ZwBwC0+c/QTAe13oHNDfk2R16dLlgn02b95s9evXz6pVq5bl7+9vhYWFWTfccIM1cuRI67vvvnPpW1BQYCUlJVkNGjSwAgICrAYNGliTJk2ydu/eXeJzQM/55z//aXXr1s0KDg627Ha7FR0dbfXv39/69ttvXfq98MILVuPGjS1/f/9iP8+PP/5oDRs2zGrcuLFlt9utatWqWddcc401ePBgKyUlpch7lyxZYrVq1cqy2+1WeHi4NXjwYOv48eNWvXr1OAcUQIVhs6zfrNIHAAAA3Iw1oAAAADCKBBQAAABGkYACAADAKBJQAAAAGEUCCgAAAKNIQAEAAGAUCSgAAACMIgEFAACAUSSgAAAAMIoEFAAAAEaRgAIAAMAoElAAAAAYRQIKAAAAo/4PRu0ALQoi9WYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test = np.array([y[0] for _, y in dataset]).astype(int)\n",
    "y_preds = np.array([p[0] for p in predictions]).astype(int)\n",
    "conf_matrix = confusion_matrix(y_test, y_preds)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Change figure size and increase dpi for better resolution\n",
    "plt.figure(figsize=(8,6), dpi=100)\n",
    "# Scale up the size of all text\n",
    " \n",
    "# Plot Confusion Matrix using Seaborn heatmap()\n",
    "# Parameters:\n",
    "# first param - confusion matrix in array format   \n",
    "# annot = True: show the numbers in each heatmap cell\n",
    "# fmt = 'd': show numbers as integers. \n",
    "ax = sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', )\n",
    " \n",
    "# set x-axis label and ticks. \n",
    "ax.set_xlabel(\"Predicted\", fontsize=14, labelpad=20)\n",
    "# tick_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "# tick_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# tick_labels = [0, 1, 2, 3, 4, 5, 6]\n",
    "# tick_labels = [0, 1, 2]\n",
    "tick_labels = [0, 1]\n",
    "ax.xaxis.set_ticklabels(tick_labels)\n",
    " \n",
    "# set y-axis label and ticks\n",
    "ax.set_ylabel(\"Actual\", fontsize=14, labelpad=20)\n",
    "ax.yaxis.set_ticklabels(tick_labels)\n",
    " \n",
    "# set plot title\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=14, pad=20)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoUAAAIjCAYAAAB1bGEnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABn4UlEQVR4nO3de3zO9f/H8ee102Ub28zsQE4lp8hhag7l8CUqRMqxXw4dHJJkqFTOSklIYSWnziJElDQkGtMcQ86HvrExM2PY6fr8/vB11dU2Nnbts9nj/r1dt9vX+/P+vD+v93XNenl93p/3ZTEMwxAAAACKNBezAwAAAID5SAoBAABAUggAAACSQgAAAIikEAAAACIpBAAAgEgKAQAAIJJCAAAAiKQQAAAAIikEcB0HDhxQq1at5OvrK4vFoqVLl+bp+EePHpXFYtG8efPydNzCrFmzZmrWrJnZYQAoYkgKgULg0KFD6tu3r26//XYVK1ZMPj4+aty4sd577z1dunTJqdfu2bOndu3apTfeeEOffvqp6tev79Tr5adevXrJYrHIx8cny/fxwIEDslgsslgsmjRpUq7HP3HihEaPHq3t27fnQbQA4FxuZgcA4NpWrFihTp06yWq1qkePHqpZs6ZSU1O1YcMGDRs2TLt379ZHH33klGtfunRJUVFReu211/T888875RoVKlTQpUuX5O7u7pTxr8fNzU0XL17U8uXL1blzZ4djn3/+uYoVK6bLly/f0NgnTpzQmDFjVLFiRdWpUyfH5/344483dD0AuBkkhUABduTIEXXt2lUVKlTQmjVrFBISYj82YMAAHTx4UCtWrHDa9U+fPi1J8vPzc9o1LBaLihUr5rTxr8dqtapx48b68ssvMyWFX3zxhdq0aaNvvvkmX2K5ePGivLy85OHhkS/XA4B/4vYxUIBNnDhRFy5c0OzZsx0SwqsqV66sQYMG2f+cnp6ucePG6Y477pDValXFihX16quvKiUlxeG8ihUrqm3bttqwYYPuvfdeFStWTLfffrs++eQTe5/Ro0erQoUKkqRhw4bJYrGoYsWKkq7cdr36//9p9OjRslgsDm2rV6/WfffdJz8/PxUvXlxVq1bVq6++aj+e3ZrCNWvW6P7775e3t7f8/PzUvn177d27N8vrHTx4UL169ZKfn598fX3Vu3dvXbx4Mfs39l+6d++u77//XomJifa2LVu26MCBA+revXum/gkJCRo6dKhq1aql4sWLy8fHRw899JB27Nhh77Nu3Trdc889kqTevXvbb0NfnWezZs1Us2ZNxcTEqEmTJvLy8rK/L/9eU9izZ08VK1Ys0/xbt26tkiVL6sSJEzmeKwBkh6QQKMCWL1+u22+/XY0aNcpR/2eeeUYjR45UvXr1NGXKFDVt2lQTJkxQ165dM/U9ePCgHn/8cT3wwAN69913VbJkSfXq1Uu7d++WJHXs2FFTpkyRJHXr1k2ffvqppk6dmqv4d+/erbZt2yolJUVjx47Vu+++q0ceeUQbN2685nk//fSTWrdurVOnTmn06NEKDw/Xr7/+qsaNG+vo0aOZ+nfu3Fnnz5/XhAkT1LlzZ82bN09jxozJcZwdO3aUxWLR4sWL7W1ffPGFqlWrpnr16mXqf/jwYS1dulRt27bV5MmTNWzYMO3atUtNmza1J2jVq1fX2LFjJUl9+vTRp59+qk8//VRNmjSxj3PmzBk99NBDqlOnjqZOnarmzZtnGd97772n0qVLq2fPnsrIyJAkffjhh/rxxx/1/vvvq0yZMjmeKwBkywBQIJ07d86QZLRv3z5H/bdv325IMp555hmH9qFDhxqSjDVr1tjbKlSoYEgy1q9fb287deqUYbVajSFDhtjbjhw5Ykgy3nnnHYcxe/bsaVSoUCFTDKNGjTL++WtlypQphiTj9OnT2cZ99Rpz5861t9WpU8cIDAw0zpw5Y2/bsWOH4eLiYvTo0SPT9Z566imHMR999FGjVKlS2V7zn/Pw9vY2DMMwHn/8caNFixaGYRhGRkaGERwcbIwZMybL9+Dy5ctGRkZGpnlYrVZj7Nix9rYtW7ZkmttVTZs2NSQZERERWR5r2rSpQ9uqVasMScb48eONw4cPG8WLFzc6dOhw3TkCQE5RKQQKqKSkJElSiRIlctR/5cqVkqTw8HCH9iFDhkhSprWHNWrU0P3332//c+nSpVW1alUdPnz4hmP+t6trEb/99lvZbLYcnXPy5Elt375dvXr1kr+/v7397rvv1gMPPGCf5z/169fP4c/333+/zpw5Y38Pc6J79+5at26dYmNjtWbNGsXGxmZ561i6sg7RxeXKr8+MjAydOXPGfmt869atOb6m1WpV7969c9S3VatW6tu3r8aOHauOHTuqWLFi+vDDD3N8LQC4HpJCoIDy8fGRJJ0/fz5H/Y8dOyYXFxdVrlzZoT04OFh+fn46duyYQ3v58uUzjVGyZEmdPXv2BiPOrEuXLmrcuLGeeeYZBQUFqWvXrvr666+vmSBejbNq1aqZjlWvXl3x8fFKTk52aP/3XEqWLClJuZrLww8/rBIlSmjBggX6/PPPdc8992R6L6+y2WyaMmWK7rzzTlmtVgUEBKh06dLauXOnzp07l+Nrli1bNlcPlUyaNEn+/v7avn27pk2bpsDAwByfCwDXQ1IIFFA+Pj4qU6aMfv/991yd9+8HPbLj6uqaZbthGDd8javr3a7y9PTU+vXr9dNPP+nJJ5/Uzp071aVLFz3wwAOZ+t6Mm5nLVVarVR07dtT8+fO1ZMmSbKuEkvTmm28qPDxcTZo00WeffaZVq1Zp9erVuuuuu3JcEZWuvD+5sW3bNp06dUqStGvXrlydCwDXQ1IIFGBt27bVoUOHFBUVdd2+FSpUkM1m04EDBxza4+LilJiYaH+SOC+ULFnS4Undq/5djZQkFxcXtWjRQpMnT9aePXv0xhtvaM2aNVq7dm2WY1+Nc9++fZmO/fHHHwoICJC3t/fNTSAb3bt317Zt23T+/PksH865atGiRWrevLlmz56trl27qlWrVmrZsmWm9ySnCXpOJCcnq3fv3qpRo4b69OmjiRMnasuWLXk2PgCQFAIF2EsvvSRvb28988wziouLy3T80KFDeu+99yRduf0pKdMTwpMnT5YktWnTJs/iuuOOO3Tu3Dnt3LnT3nby5EktWbLEoV9CQkKmc69u4vzvbXKuCgkJUZ06dTR//nyHJOv333/Xjz/+aJ+nMzRv3lzjxo3TBx98oODg4Gz7ubq6ZqpCLly4UH/99ZdD29XkNasEOrdefvllHT9+XPPnz9fkyZNVsWJF9ezZM9v3EQByi82rgQLsjjvu0BdffKEuXbqoevXqDt9o8uuvv2rhwoXq1auXJKl27drq2bOnPvroIyUmJqpp06aKjo7W/Pnz1aFDh2y3O7kRXbt21csvv6xHH31UL7zwgi5evKiZM2eqSpUqDg9ajB07VuvXr1ebNm1UoUIFnTp1SjNmzNBtt92m++67L9vx33nnHT300ENq2LChnn76aV26dEnvv/++fH19NXr06Dybx7+5uLjo9ddfv26/tm3bauzYserdu7caNWqkXbt26fPPP9ftt9/u0O+OO+6Qn5+fIiIiVKJECXl7eyssLEyVKlXKVVxr1qzRjBkzNGrUKPsWOXPnzlWzZs00YsQITZw4MVfjAUCWTH76GUAO7N+/33j22WeNihUrGh4eHkaJEiWMxo0bG++//75x+fJle7+0tDRjzJgxRqVKlQx3d3ejXLlyxvDhwx36GMaVLWnatGmT6Tr/3goluy1pDMMwfvzxR6NmzZqGh4eHUbVqVeOzzz7LtCVNZGSk0b59e6NMmTKGh4eHUaZMGaNbt27G/v37M13j39u2/PTTT0bjxo0NT09Pw8fHx2jXrp2xZ88ehz5Xr/fvLW/mzp1rSDKOHDmS7XtqGI5b0mQnuy1phgwZYoSEhBienp5G48aNjaioqCy3kvn222+NGjVqGG5ubg7zbNq0qXHXXXdlec1/jpOUlGRUqFDBqFevnpGWlubQb/DgwYaLi4sRFRV1zTkAQE5YDCMXK7EBAABwS2JNIQAAAEgKAQAAQFIIAAAAkRQCAABAJIUAAAAQSSEAAABEUggAAADxjSYAAABKiz/stLHdA26/fqcCgEohAAAAqBQCAADIlmF2BKYjKQQAADBsZkdgOm4fAwAAgEohAACAbFQKqRQCAACASiEAAIDBmkIqhQAAAKBSCAAAwJpCUSkEAACAqBQCAACwT6FICgEAAPhGE3H7GAAAAKJSCAAAwO1jUSkEAACAqBQCAACwJY2oFAIAAEAFoFKYnp6u3bt3KzY2VpIUHBysGjVqyN3d3eTIAABAUcHX3JmYFNpsNo0cOVLTp0/XuXPnHI75+vrq+eef15gxY+TiQjETAADA2UxLCl955RXNmzdPb731llq3bq2goCBJUlxcnH788UeNGDFCqampevvtt80KEQAAFBWsKZTFMAzDjAsHBwdr/vz5at26dZbHV61apR49eiguLi6fIwMAAEVNyv4NThvbWuU+p42dl0y7N3v+/HmVKVMm2+MhISFKTk7Ox4gAAACKLtOSwmbNmmno0KGKj4/PdCw+Pl4vv/yymjVrlv+BAQCAoseW4bxXIWHamsKIiAg9/PDDCgkJUa1atRzWFO7atUs1atTQd999Z1Z4AAAARYppawqlK08gr1q1Sps2bXLYkqZhw4Zq1aoVTx4DAIB8kbJ3rdPGtlZv7rSx85KpSSEAAEBBQFJYADavjo6OVlRUlEOlsFGjRrrnnntMjgwAABQZbEljXqXw1KlTeuyxx7Rx40aVL1/eYU3h8ePH1bhxY33zzTcKDAy85jgpKSlKSUlxaLNarbJarU6LHQAA3FpSdkc6bWzrXS2cNnZeMm3R3nPPPaeMjAzt3btXR48e1ebNm7V582YdPXpUe/fulc1m04ABA647zoQJE+Tr6+vwmjBhQj7MAAAA3DIMm/NehYRplcISJUpo/fr1qlu3bpbHY2Ji1KxZM50/f/6a41ApBAAANytl5yqnjW29O+sv6ihoTFtTaLValZSUlO3x8+fP5yixIwEEAAC4eabdPu7SpYt69uypJUuWOCSHSUlJWrJkiXr37q1u3bqZFR4AAChCDCPDaa/CwrRK4eTJk2Wz2dS1a1elp6fLw8NDkpSamio3Nzc9/fTTmjRpklnhAQAAFCmm71OYlJSkmJgYhy1pQkND5ePjY2ZYAACgCLm83XnfolasTlunjZ2XTN2nMD4+XnPmzMlyn8JevXqpdOnSZoYHAABQZJhWKdyyZYtat24tLy8vtWzZ0mGfwsjISF28eFGrVq1S/fr1zQgPAAAUIZe3LnPa2MXqPeK0sfOSaUlhgwYNVLt2bUVERMhisTgcMwxD/fr1086dOxUVFWVGeAAAoAghKTQxKfT09NS2bdtUrVq1LI//8ccfqlu3ri5dupTPkQEAgKLmcsxSp41dLLSD08bOS6ZtSRMcHKzo6Ohsj0dHR9tvKQMAADiVLcN5r0LCtAdNhg4dqj59+igmJkYtWrTItKZw1qxZbEkDAACQT0xLCgcMGKCAgABNmTJFM2bMUEbGlUza1dVVoaGhmjdvnjp37mxWeAAAoCgpRN9R7Cym71MoSWlpaYqPj5ckBQQEyN3d3eSIAABAUXI5eqHTxi52byenjZ2XTN2n8Cp3d3eFhISYHQYAACiqbFQKTXvQBAAAAAVHgagUAgAAmIo1hVQKAQAAQKUQAACANYUiKQQAACApFLePAQAAICqFAAAAMozC83V0zkKlEAAAAFQKAQAAWFNIpRAAAACiUggAAMDm1aJSCAAAAFEpBAAAYE2hbtGk0Lf4HWaHYIr4o6vNDsEU3mWbmB2CKVJT/mt2CKawFitndgimyCii/8FKT/3L7BBQVHD7mNvHAAAAuEUrhQAAALlSRKvx/0SlEAAAAFQKAQAAWFNIpRAAAAAiKQQAALiyptBZrxswffp0VaxYUcWKFVNYWJiio6Ov2X/q1KmqWrWqPD09Va5cOQ0ePFiXL1/O1TVJCgEAAAqQBQsWKDw8XKNGjdLWrVtVu3ZttW7dWqdOncqy/xdffKFXXnlFo0aN0t69ezV79mwtWLBAr776aq6uS1IIAABQgCqFkydP1rPPPqvevXurRo0aioiIkJeXl+bMmZNl/19//VWNGzdW9+7dVbFiRbVq1UrdunW7bnXx30gKAQAADJvTXikpKUpKSnJ4paSkZBlGamqqYmJi1LJlS3ubi4uLWrZsqaioqCzPadSokWJiYuxJ4OHDh7Vy5Uo9/PDDuXoLSAoBAACcaMKECfL19XV4TZgwIcu+8fHxysjIUFBQkEN7UFCQYmNjszyne/fuGjt2rO677z65u7vrjjvuULNmzbh9DAAAkGtOvH08fPhwnTt3zuE1fPjwPAt93bp1evPNNzVjxgxt3bpVixcv1ooVKzRu3LhcjcM+hQAAAE5ktVpltVpz1DcgIECurq6Ki4tzaI+Li1NwcHCW54wYMUJPPvmknnnmGUlSrVq1lJycrD59+ui1116Ti0vOaoBUCgEAAJy4pjA3PDw8FBoaqsjISHubzWZTZGSkGjZsmOU5Fy9ezJT4ubq6XpmWYeT42lQKAQAACpDw8HD17NlT9evX17333qupU6cqOTlZvXv3liT16NFDZcuWta9LbNeunSZPnqy6desqLCxMBw8e1IgRI9SuXTt7cpgTJIUAAAA3uMm0M3Tp0kWnT5/WyJEjFRsbqzp16uiHH36wP3xy/Phxh8rg66+/LovFotdff11//fWXSpcurXbt2umNN97I1XUtRm7qioWEb/E7zA7BFPFHV5sdgim8yzYxOwRTpKb81+wQTGEtVs7sEEyRUYD+g5Wf0lP/MjsEFBGXlrzltLE9H33FaWPnJSqFAAAAuVz7dyviQRMAAAAU3Ephenq6Tpw4ofLly5sdCgAAuNUV0SUa/1Rgk8Ldu3erXr16ysjIMDsUAABwqyMp5PYxAAAATKwU1qtX75rHL126lE+RAACAIu/W24wl10xLCvfs2aOuXbuqUqVKWR4/efKk9u/fn89RAQAAFE2m3T6uWbOmwsLCNGrUqCxf/fr1Mys0B8/0+T/t3P2z4uL3KHLtN6oXevc1+3d49CFt2fqj4uL36NfNK/VAq2bZ9p3y3jidu3BI/Z/rlbdB54Evv1muVo/1VL3mj6jbsy9q15592fZNS0/XzDmf68FOvVWv+SPq2PM5bdj0W7b9P/70a9Vs/JDemhrhjNBvSr9+PbV/X5SSzh3Uhl+Wq379Otfs/1jHNtq1c52Szh3U1pif9OCD/3E43qH9Q1qx4nOdPLFLqSn/Ve27azgxeuRWv749tW/frzqXeEC/rF923c+7Y8c22rljrc4lHlDMb6v1YOvmDsfbt39QK777XCf+2qmUy3/q7gL6effv11MH92/ShaRD+nXDct1zvZ/zx9rq910/60LSIW3b+pMe+tfPuSSNHjVUfx7bqvPnDmrV91+pcuWs/8EPFFg2m/NehYRpSWHjxo21b1/2iUaJEiXUpIm5mxJ3fKyN3pzwqt6eME1N7ntEv//+h5YsnaeA0qWy7H9vWD3NnjtVn85fqPsbt9OK71bri69mqnqNKpn6tm3XSvXvqaMTJ2KdPY1c+/6nnzXx/Y/U/6kntHDO+6pauZL6hr+uM2cTs+z//kfztfDb7/Xq4P769rMP1bnDwxo0fJz27j+Yqe+uvfu08NuVqlIA/4PR6fF2emfiSI1/Y4rCwh7Szl17tOK7z1Q6m8+7QYNQffrpdM2d95XuDXtQy5b9oEULP9ZdNara+3h7e+nXjVv06mtv5tc0kEOPP95OEyeO0BtvTFVYg4e1a9cefbf802t/3p98oHnzvlJY2ENatnyVFi78WDX+9Xlv/DVar71ecD/vTp0e0aR3Rmnc+Mm6J+xB7di5RytXfJ7tvBs2qK/PP52uuXO/VP17W2vZslX6ZtFs3XXX3/MeNvQ5PT/gKT33/CtqdF87JV+8qJXffS6r1Zpf0wKQB/hGk2uIXPuNtm7dqWFDxkiSLBaL9uzboI8iPtGUyR9m6j93/jR5eXmqS6dn7W0/rVmkXbv2avCgEfa2kJAgRa77Rh079NbXiz7WzOlzNXPGvJuON6++0aTbsy+qZrUqem3Ic5KufBF3y0d7qPvjj+iZJztn6t/8kSfUp2dXdXusnb3txVfHy2r10NujXrK3Xbx4SZ2eGqjXhwzQh/O/VLXKt+uVF2++IpxX32iy4Zfl+i1mh1588XVJVz7vw4e2aMaMuXpn0vRM/T//bIa8vL306KO97G2/rF+mHTt36/nnhzv0rVDhNh3Yv0n33NNKO3buyZN4+UaTm/PL+mWKidmhFwdf+btpsVh06GC0Zsycq0mTZmTq/9mnM+Tt7alHO/a2t63/+Vvt3Llbzw981aFvhQq3af++KN1zb2vtzKPPO6++0eTXDcu15bcdGvSPn/Ojh7do+oy5mvhO5p/zLz6fKW8vL7V/tKe9beMvy7V9x24NeP7KtzT8eWyrpkz9UJOnXPm96ONTQif+u11PPTNYX3+97Kbi5RtNkF8ufT7i+p1ukOcT45w2dl7i6eNsuLu7q07dmlq39ld7m2EYWrf2V91zb90sz7nn3rpat3ajQ1tk5C8O/S0Wiz76+F1Ne+9j/bH3gHOCvwlpaWnas++AGtxTx97m4uKiBvXraMfve7M8JzUtTR4eHg5tVquHtu3c7dA2/t3patLwHjW8J+v3z0zu7u6qV6+W1qz5xd5mGIbWrPlFDRpk/VBUWFioQ39JWr36ZzUIC3VqrLh5f3/eG+xthmFozdpfsv38whrUc+gvSat/+llhhejzvjLvuxX5r5/zyDUb1KBB1vNoEBbq0F+Sfly9zt6/UqXyV/6h+4/3JinpvKKjt/F3AShkTN+nMDo6WlFRUYqNvXIbNTg4WA0bNtS9996bo/NTUlKUkpLi0GYYhiwWy03FVapUSbm5uenUqXiH9tOn4lWlyu1ZnhMUFKBTp89k6h8UVNr+58HhfZWenq6IPKgMOsPZxCRlZNhUyr+kQ3sp/5I6cjzrylTjsFB98tVi1a9TU+XKhmjTb9sV+fOvyrD9vcfkyp/Wae/+Q/rq4/ecGv+NCgjwl5ubm+LiTju0nzoVr6pVK2d5TnBwaZ2Kc/z5iDt12uHzRsFk/7xP/evzjotX1SrZfN5BpRX3r98Hp+LiC9XnfXXe//65PXXqtKpVzfoOS3Bw6UzvU1xcvIL/N+/goMD/tf2rz6l4BQcH5lXogPPxNXfmJYWnTp3SY489po0bN6p8+fIKCgqSJMXFxWnw4MFq3LixvvnmGwUGXvuXyoQJEzRmzBiHNg93PxXz8Hda7DeqTp2a6vdcLzVp/IjZoeSpVwb11ei3p6ld9z6yWKRyZULUoc0DWvLdj5Kkk3Gn9dbUDzVr6puyWj2uMxoAACYoRA+EOItpSeFzzz2njIwM7d27V1WrVnU4tm/fPj311FMaMGCAFi5ceM1xhg8frvDwcIe220Lq3HR8Z86cVXp6ugIDAxzaSwcGZPoX8VVxcfEK/Ndi7X/2b9iovkqXLqXdf/x9K8bNzU1vTHhV/Qf01t13Nb3puG9WST8fubq66EzCWYf2MwlnFfCv6uFV/iX9NO2tkUpJSVViUpICA0ppysw5uq1MsCRpz74DSjibqM5PPW8/JyPDppjtv+vLxcu1de0yubq6Om9SORAfn6D09PRMVZ/AwADFxZ3K8pzY2NMKDHL8+QgKLJ3tzwcKDvvnHfivzzso+7/fsXGnFfSv3wfX6l8QXZ33v39uAwNLKza7eceezvQ+BQUF2PvH/u/vR1BQacXG/v13JSgwQNt3OC4hAVCwmbamcNWqVZo+fXqmhFCSqlatqmnTpumHH3647jhWq1U+Pj4Or5u9dSxdWVu3fdvvatqskb3NYrGoabOG2hK9LctztkRvc+gvSc2b32fv/9VXS9WoQRvd16id/XXiRKymTZ2ljh163XTMecHd3V01qt6pzb9tt7fZbDZtjtmu2jWrX/Ncq9VDQaUDlJ6RodXrNqr5/Q0lSQ1C62jJpzO1aN50++uuaneqTavmWjRvuukJoXTl8966dZeaN7/P3maxWNS8+X3atGlrluds3hyj//yjvyS1aHG/Nm2OcWqsuHl/f96N7W0Wi0XNm92X7ee3edNWh/6S1OI/92tzIfq8r8x7p8PPrcVi0X+a36dNm7Kex6bNMfrPfxx/zlu2aGLvf+TIcZ08GecwZokSxXXvvXX5u4DCxTCc9yokTKsUWq1WJSUlZXv8/Pnzpm9nMP2DOZr54TvatnWXYmJ26LkBveXt5aXPPlskSYr4aJJOnojVmNGTJEkzZ8zTyh++0PMDn9aqVWv12ONtVbdeTQ164TVJ0tmERJ1NSHS4RlpauuLiTuvggSP5Ordr6dHlUb32xru6q9qdqlmjqj77eqkuXU5RhzYPSJKGj5ukwIBSGtz/ylOYO3f/objTZ1Ttztt16vQZzZjzmQzD0FNPPC7pyjYdd95e0eEanp7F5OdTIlO7md577yPNnj1FW2N2aMtv2zVw4DPy9vbU/E8WSJLmzJ6qEydi9fqItyRJ738wW5E/LdKLL/bR999HqnOn9goNvVvPPfeyfcySJf1UvlwZhfyvalqlypV1W7FxpwtVhelW9N60WZr98WTFbN2p37Zs18CBT8vb21OffPK1JGn27Ck6cSJWI0a8LUn6YPps/bR6oV4cdOXz7tT5kSuf94BX7GOWLOmncuXKqEzIleUwVz/vuAL0eU95b5bmzp6imK07tWXLNr0w8Fl5e3tq3vwrP+dz57ynEydO6rXX//dz/v5srYlcpMEv9tXK739Sl85Xfs77Pff3zgLT3v9Yrw5/QQcOHtbRo39qzOhhOnEiTt9+u8qUOQK4MaYlhV26dFHPnj01ZcoUtWjRQj4+PpKkpKQkRUZGKjw8XN26dTMrPEnS4m9WqFSAv159/UUFBQVo18696vhob50+deVhktvKhcj2jzUI0Zu36pmnBuv1EeEaOXqIDh06pu5d+2vvnsL1zSwPtWyqs4nn9MHHnyk+IUHV7rxDEe+Os98+Phl3Si7/qMampKbq/Vnz9d8TsfLy9NT9De/RhBHD5FOiuFlTuCELFy1XQOlSGjlyqIKDS2vHjj1q2+5J+8NG5cqVdfi8N22KUY8ez2vMmJc0buzLOnjwiB7v9Ix2/2Oj77ZtH9Dsj6fY//z55zMlSePGTda48ZPzaWbIyqJFy1U6wF8jRw5RcNCVz7vdI//+vP/+F/6mTTHq0XOgxoweprFjX9LBg0fVqdMz2vOvz/vjWX9/rp9/dmVrm3HjJ2v8+L9/Dsy0cOEylQ7w12j7z/lutWn7f/Z5ly9XxuHnPGrTb/q/Hs9r7JiXNH7cyzpw8Igee/xp7d7997zfmTRD3t5eipgxUX5+Ptq4cYvatPu/TA8BAgUaawrN26cwJSVFL774oubMmaP09HT7liapqalyc3PT008/rSlTptxQtTCv9iksbPJqn8LCJq/2KSxs2KewaMmrfQoLG/YpRH65NPel63e6QZ69Jzpt7Lxk6u3jmTNn6u2331ZMTIzDljShoaH2yiEAAIDTFdF/eP2TqfsUxsfHa86cOZn2KWzUqJF69eql0qULz/5fAAAAhZlpTx9v2bJFVapU0bRp0+Tr66smTZqoSZMm8vX11bRp01StWjX99ttvZoUHAACKEsPmvFchYVqlcODAgerUqZMiIiIybSFjGIb69eungQMHKioqyqQIAQBAUWHYCs/WMc5iWlK4Y8cOzZs3L8s9BS0WiwYPHqy6dQved+QCAADciky7fRwcHKzo6Ohsj0dHR9u/+g4AAMCpbDbnvQoJ0yqFQ4cOVZ8+fRQTE6MWLVo4fPdxZGSkZs2apUmTJpkVHgAAQJFiWlI4YMAABQQEaMqUKZoxY4YyMjIkSa6urgoNDdW8efPUuXNns8IDAABFSSF6IMRZTN2SpkuXLurSpYvS0tIUH39lN/2AgAC5u7ubGRYAAECRY2pSeJW7u7tCQkLMDgMAABRVPH1s3oMmAAAAKDgKRKUQAADAVIXoKWFnISkEAAAgKeT2MQAAAKgUAgAASAYPmlApBAAAAJVCAAAA1hRSKQQAAICoFAIAALB5tagUAgAAQFQKAQAAJIM1hSSFAAAA3D7m9jEAAABu0Urh0FINzA7BFO3rPW92CKbYWraO2SEATpee+pfZIQC3NIMtaagUAgAA4BatFAIAAOQKawqpFAIAAIBKIQAAAFvSiEohAAAARKUQAACANYUiKQQAAJDYkobbxwAAAKBSCAAAwO1jUSkEAACAqBQCAACwJY2oFAIAAEBUCgEAAFhTKCqFAAAAEJVCAAAAGexTSFIIAADA7WNuHwMAAEBUCgEAAKgUqgAkhbGxsdq8ebNiY2MlScHBwQoLC1NwcLDJkQEAABQdpiWFycnJ6tu3r7766itZLBb5+/tLkhISEmQYhrp166YPP/xQXl5eZoUIAACKCjavNm9N4aBBgxQdHa0VK1bo8uXLiouLU1xcnC5fvqyVK1cqOjpagwYNMis8AACAIsW0pPCbb77RvHnz1Lp1a7m6utrbXV1d1apVK82ZM0eLFi0yKzwAAFCU2AznvQoJ05JCm80mDw+PbI97eHjIxp5BAAAA+cK0pLBt27bq06ePtm3blunYtm3b1L9/f7Vr186EyBzV7/GABm6YquH75uqppWNUpvbt2fat9mB9Pb18nIbt/Egv752tZ1e+qVqP3pepT/dPX9GQ7REacexzBdWo4Owp3JC2Pdpq7sa5Wrp/qaZ8O0VValfJtm/5KuX1WsRrmrtxrlYeX6n2T7fP1MfFxUVPDnlSczbM0ZL9SzT7l9nq9kI3Z07hhvg/2UZV1s9Wjb2Ldfvid+V5d/bz/ifftk1U8/B3Kh/xmkO7a4Cfyk58UVWj5qvG7kWqMHeMPCqWcUbouAH9+vbUvn2/6lziAf2yfpnq169zzf4dO7bRzh1rdS7xgGJ+W60HWze3H3Nzc9Mb44cr5rfVSjizT0cO/6bZs6coJCTIybMAkBcMm+G0V2FhWlL4wQcfKCgoSKGhoSpVqpSqV6+u6tWrq1SpUqpfv74CAwP1wQcfmBWeJKlG2wZ64PUntP69xZrV9nXF7T2u7p++Iq9SPln2v5SYrA0ffKu5HUfro9bDtWPhz3pkUh/d3qSWvY+7ZzH9uWWfIt/6Kp9mkXtN2jXRsyOe1RdTv9DANgN1eO9hjftsnHxL+WbZ31rMqpPHT2ruW3OVcCohyz6P939cDz/5sGaOnKm+/+mrORPm6LF+j+mR3o84cyq54tPmfgW/+oxOTftSh9oN0uW9R1Rx/li5ZjPvq9zLBip4+FNKjv4907EKEa/Lo3ywjvcdr4NtByntr1Oq+Ol4WTytzpoGcujxx9tp4sQReuONqQpr8LB27dqj75Z/qtKlS2XZv0GDUH36yQeaN+8rhYU9pGXLV2nhwo9Vo0ZVSZKXl6fq1q2pNye8pwYNHlKXrs+qyp136JtFc/JzWgBuFLePZTEMw9Ro//jjD0VFRTlsSdOwYUNVq1bthsccV+GJPIntqaVjdGLnYf0wcv6VBotFgzZN05Z5P+rXmctzNMYzK8br4JrtWveu4/pI39sC9MLG9/TRQ68qbs+xPIk3yjibJ+NM+XaK9u/Yr5kjZ0qSLBaL5m+er+XzlmvhjIXXPHfuxrlaOmepvp39rUP76Lmjdfb0Wb330nv2ttciXlPK5RRNenHSTcU70S1v/m1z++J3dWnnAZ0cHXGlwWJR1Y3zdOaT5YqPyGZ9q4uLKn31lhIXrZZX/bvk6uOt4/3ekCR5VCqjKpEf6UDr55Ry4Lh9zGqbP1XcpE909usfbyremoe/u6nzCytrsXJ5Ms4v65cpJmaHXhw8QtKVn/NDB6M1Y+ZcTZo0I1P/zz6dIW9vTz3asbe9bf3P32rnzt16fuCrWV4jNLS2ft34nSrfGaY//zxxU/GmXP7zps4HcG3nX2jrtLFLTCscv69N/0aTatWqqXfv3ho+fLiGDx+u3r1731RCmFdc3F0VUquSjmz4R/XHMHRkw++6rd6dORqjYuO7VOr2EB3b/IeTosx7bu5uqlyrsrZv2G5vMwxD2zdsV7V6N/657Pltj+o0rqOylcpKkipVr6Qa99TQb+t+u9mQ84TF3U2eNSvrwsbtfzcahi5s3C6vutnPO/CFrso4c05nv16deUwP9yvDpKQ6jGmkpsmrfo28Ch03wN3dXfXq1dKaNRvsbYZhaM3aX9QgLDTLc8Ia1HPoL0mrf/pZYdn0lyRf3xKy2WxKTEzKm8ABOI/N5rxXIWH65tXZOXv2rJYvX64ePXpcs19KSopSUlIc2tKNDLlZXLM5I2e8SpaQi5urLsSfc2hPjk9SwB3ZrwmzlvDUi5s/kKuHm4wMm1aOmOeYWBZwPv4+cnVz1dl4x6pjYnyiyt1x4xWahTMWyquElz5c+6FsGTa5uLrok3c+0bql624y4rzhWtJHFjdXpccnOrSnxyfKesdtWZ7jVb+GSnZqpYNtX8jyeMqh/yr1r1MKGtZTf732gYxLKSr1VHu5lyktt0D/vJ4CciEgwF9ubm6KO3Xaof1UXLyqVqmc5TnBQaUVdyo+U/+goNJZ9rdarXpj/HAt+PpbnT9/IW8CBwAnKrBJ4fHjx9W7d+/rJoUTJkzQmDFjHNqa+dTUf/zudmZ42Uq5cFkfPfSqPLyLqVLju9Tq9SeUePyUjm3aa0o8BcX9be9X8w7NNXHgRB3ff1y333W7+ozqozNxZxS5KNLs8HLNxdtTt70brr9efV8ZZ7OpAqVn6Hj/N1T2rUGqsX2BjPQMXdi4XecLSHUUzuPm5qYvPp8pi8WigdncWgZQwBSitX/OYlpSmJR07dsp58+fz9E4w4cPV3h4uEPbuzX73HBcV108e1629AwVD3B8yMA7wEcXTp/L5ixJhqGzx+IkSXF7jimgchk1fu6RQpMUJiUkKSM9QyUDSjq0+wX4KeF01g+R5MTTrz2thTMWav3y9ZKko/uOKrBsoDo/17lAJIUZZ5NkpGfILcDPod0twE/ppzOv1fQoHyyPcsGqMGvk340uFknSXfu/1YGWfZV6PFaXfz+kQ21fkEsJL1nc3ZSRkHRl7eKuA86cDq4jPj5B6enpCgp0rPIFBgUoLu50lufExp1WUGDAdftfTQjLly+r1g92oUoIoNAwbU2hn5+fSpYsme2rSZMmORrHarXKx8fH4XWzt44lyZaWoZO7jqhi47v+brRYVKlxTf13a87/g25xscjVo8AWZDNJT0vXwV0HVbtxbXubxWJRncZ19MfWG18bafW0Ztp30mazycXF9GWtkiQjLV2Xfj+o4o3+nrcsFhVvVFsXt2Wed8qh/+rAgwN0sO0L9tf5nzYredMuHWz7gtJOOt5mtJ2/qIyEJHlULCPPWpV1fvVmZ08J15CWlqatW3epefPG9jaLxaLmze7Tps0xWZ6zedNWh/6S1OI/92vzP/pfTQgrV66khx7upoSERKfED8AJePrYvEphiRIl9NprryksLCzL4wcOHFDfvn3zOSpHmz7+Xu3f7auTO4/oxI5DuvepB+XuZdWOhT9LktpP7qfzsWe1ZuICSVLj5x7RiZ2HdfZYnNys7qrcvI5qPXqfVr4+1z5mMV9v+ZYNUIkgP0lSqdtDJEkXTicq+VoVyHy05OMlCn83XAd2HdD+7fvV/un2snpZtfp/D1MMmTJEZ2LPaN7b8yRdeTil/J3lr/x/DzeVCiql22vcrkvJl3Ty2ElJ0uafNqvrwK46feK0ju0/pjvuukOPPvOofrzJJ3DzUvzspbpt0mBd2nVAl3bsV6ne7eXiVUxnF/0kSSo7KVzpcWcU9858GalpStnv+NR4RlKyJDm0+zzUWBkJSUo9cUrFqlZUyMg+Slq9SRc2ZN6fE/nrvWmzNPvjyYrZulO/bdmugQOflre3pz755GtJ0uzZU3TiRKxGjHhbkvTB9Nn6afVCvTioj77/PlKdOj+i0NC79dyAVyRdSQi/+vJD1albU48+2kuurq729YYJCYlKS0szZ6IAkEOmJYX16tWTJDVt2jTL435+fjJ5txzt+W6TvEqVUNPwx1W8tK/i9hzTFz3eVnL8lVvfPmVKOWxK6e5l1UPje8snxF/pl1MVf+iElr44U3u+22TvU+WBULV/9+9k97HpAyVJP0/5RuunLs6nmV3b+uXr5ePvoyfDn1TJ0iV1eM9hjXxypBL/9xBG6TKlHap+/kH++uCHv/eUfLzf43q83+PaGbVTr3S58h/MiJERenLokxowfoB8A3yVEJeg7z//Xl+890W+zu1aklb8olh/XwUO/j+5BZTU5b2HdbTXSGX8b94eZUrn+ikyt0B/hbz2jFz/dxs6cfEanf6g4O5RWZQsWrRcpQP8NXLkEAUHldaOHXvU7pEndep/D5OUK1dWtn/8/d60KUY9eg7UmNHDNHbsSzp48Kg6dXpGe/bskySVLRusdu1aSZJ+2+L4j50HWnXS+vWbBKDgMjvnKAhM26dw1qxZunjxogYNGpTl8bi4OEVERGjUqFG5Hjuv9iksbPJqn8LCJq/2KSxs2KewaGGfQsC5kvq2dtrYPh+uctrYecm0SuGzzz57zeNBQUE3lBACAADkWiFa++cspj4BER8frzlz5mT6RpNGjRqpV69eKl066/2/AAAA8hRJoXlPH2/ZskVVqlTRtGnT5OvrqyZNmqhJkyby9fXVtGnTVK1aNf32G/u5AQAA5AfTKoUDBw5Up06dFBERIYvF4nDMMAz169dPAwcOVFRUlEkRAgCAosKgUmheUrhjxw7NmzcvU0IoXdkvbPDgwapbt64JkQEAABQ9pt0+Dg4OVnR0dLbHo6OjFRQUlI8RAQCAIovNq82rFA4dOlR9+vRRTEyMWrRoYU8A4+LiFBkZqVmzZmnSpElmhQcAAFCkmJYUDhgwQAEBAZoyZYpmzJihjIwMSZKrq6tCQ0M1b948de7c2azwAABAUZK77ya4JZm6JU2XLl3UpUsXpaWlKT7+yrcIBAQEyN3d3cywAAAAihxTk8Kr3N3dFRISYnYYAACgiOLp4wKSFAIAAJiKpNC8p48BAABQcJAUAgAA2Jz4ugHTp09XxYoVVaxYMYWFhV1zGz9JSkxM1IABAxQSEiKr1aoqVapo5cqVubomt48BAAAKkAULFig8PFwREREKCwvT1KlT1bp1a+3bt0+BgYGZ+qempuqBBx5QYGCgFi1apLJly+rYsWPy8/PL1XVJCgEAQJFXkB40mTx5sp599ln17t1bkhQREaEVK1Zozpw5euWVVzL1nzNnjhISEvTrr7/ad3CpWLFirq/L7WMAAAAnSklJUVJSksMrJSUly76pqamKiYlRy5Yt7W0uLi5q2bKloqKisjxn2bJlatiwoQYMGKCgoCDVrFlTb775pn0P6JwiKQQAAHDimsIJEybI19fX4TVhwoQsw4iPj1dGRkamr/oNCgpSbGxsluccPnxYixYtUkZGhlauXKkRI0bo3Xff1fjx43P1FnD7GAAAwImGDx+u8PBwhzar1Zpn49tsNgUGBuqjjz6yfzPcX3/9pXfeeUejRo3K8TgkhQAAoMhz5ppCq9Wa4yQwICBArq6uiouLc2iPi4tTcHBwlueEhITI3d1drq6u9rbq1asrNjZWqamp8vDwyNG1uX0MAABQQLak8fDwUGhoqCIjI/8OzWZTZGSkGjZsmOU5jRs31sGDB2Wz/X2x/fv3KyQkJMcJoURSCAAAUKCEh4dr1qxZmj9/vvbu3av+/fsrOTnZ/jRyjx49NHz4cHv//v37KyEhQYMGDdL+/fu1YsUKvfnmmxowYECursvtYwAAUOQZN7jJtDN06dJFp0+f1siRIxUbG6s6derohx9+sD98cvz4cbm4/F3XK1eunFatWqXBgwfr7rvvVtmyZTVo0CC9/PLLubquxTCMgrMxTx4ZV+EJs0MwRZRx1uwQTDHRrWgWvGse/s7sEExhLVbO7BBMkXL5T7NDAG5pZ9o1ddrYpZb/7LSx89ItWSkccexzs0MA4CQkRwCcogBVCs1SNEssAAAAcHBLVgoBAAByoyCtKTQLlUIAAABQKQQAAGBNIUkhAAAAt4/F7WMAAACISiEAAACVQlEpBAAAgKgUAgAAUCkUlUIAAACISiEAAIBkWMyOwHRUCgEAAEClEAAAgDWFJIUAAAAybNw+5vYxAAAAqBQCAABw+5hKIQAAAESlEAAAQAZb0hTcSmFycrLWr19vdhgAAABFQoGtFB48eFDNmzdXRkaG2aEAAIBbHGsKC3ClEAAAAPnHtEqhv7//NY9TIQQAAPmFfQpNTApTUlLUv39/1apVK8vjx44d05gxY/I5KgAAUBQZhtkRmM+0pLBOnToqV66cevbsmeXxHTt2kBQCAADkE9OSwjZt2igxMTHb4/7+/urRo0f+BQQAAIosbh9LFsOgYAoAAIq2Y/VaOm3sClt/ctrYeanAbkkDAACQX6gUFuAtac6ePatPPvnkuv1SUlKUlJTk8EpJScmHCAEAAG4dBTYpPH78uHr37n3dfhMmTJCvr6/Da8KECfkQIQAAuFUYhvNehYVpawqTkpKueXznzp1q2rTpdfcrTElJyVQZtFqtslqtNx0jAAAoGo7UfsBpY1fasdppY+cl09YU+vn5yWLJ/v69YRjXPH4VCSAAALhZrCk0MSksUaKEXnvtNYWFhWV5/MCBA+rbt28+RwUAAIoiwyApNC0prFevniSpadOmWR738/MTu+UAAADkD9OSwu7du+vixYvZHg8ODtaoUaPyMSIAAFBUGTazIzAfm1cDAIAi72CN1k4bu/KeVU4bOy+Zunl1fHy85syZo6ioKMXGxkq6UiFs1KiRevXqpdKlS5sZHgAAKCJsrCk0r1K4ZcsWtW7dWl5eXmrZsqWCgoIkSXFxcYqMjNTFixe1atUq1a9f34zwAABAEbK/+oNOG7vK3h+cNnZeMi0pbNCggWrXrq2IiIhMW88YhqF+/fpp586dioqKMiM8AABQhOyr9pDTxq76x/dOGzsv5SgpXLZsWY4HfOSRR3LUz9PTU9u2bVO1atWyPP7HH3+obt26unTpUo6vDQAAcCNICnO4prBDhw45GsxisVz3G0iuCg4OVnR0dLZJYXR0tP2WMgAAgDOxeXUOk0KbLe+f0x46dKj69OmjmJgYtWjRItOawlmzZmnSpEl5fl0AAIB/Yy8WE58+HjBggAICAjRlyhTNmDHDXmF0dXVVaGio5s2bp86dO5sVHgAAQJFyQw+aJCcn6+eff9bx48eVmprqcOyFF17IdRBpaWmKj4+XJAUEBMjd3T3XYwAAANyoPXe0cdrYNQ6tcNrYeSnXSeG2bdv08MMP6+LFi0pOTpa/v7/i4+Pl5eWlwMBAHT582FmxAgAAOAVJoeSS2xMGDx6sdu3a6ezZs/L09NSmTZt07NgxhYaGsgYQAAAUSjbD4rRXYZHrpHD79u0aMmSIXFxc5OrqqpSUFJUrV04TJ07Uq6++6owYAQAA4GS5Tgrd3d3l4nLltMDAQB0/flyS5Ovrqz///DNvowMAAMgHhmFx2quwyPXTx3Xr1tWWLVt05513qmnTpho5cqTi4+P16aefqmbNms6IEQAAAE6W60rhm2++qZCQEEnSG2+8oZIlS6p///46ffq0PvroozwPEAAAwNkMw3mvwsK07z4GAAAoKHZWbOe0se8+utxpY+cl0zavBgAAKCgK01PCzpLrpLBSpUqyWLJ/49inEAAAFDaF6YEQZ8l1Uvjiiy86/DktLU3btm3TDz/8oGHDhuVVXAAAAMhHuU4KBw0alGX79OnT9dtvv910QAAAAPmNJyxu4Onj7Dz00EP65ptv8mo4AAAA5KM8e9Bk0aJF8vf3z6vhAAAA8g0Pmtzg5tX/fNDEMAzFxsbq9OnTmjFjRp4GBwAAgPyR66Swffv2Dkmhi4uLSpcurWbNmqlatWp5GtyN+iGoq9khmKLDuV/NDsEUpTxLmB2CKf46u9vsEEzx++1tzQ7BFDUPf2d2CMAtjaePbyApHD16tBPCAAAAgJly/aCJq6urTp06lan9zJkzcnV1zZOgAAAA8pPNsDjtVVjkulKY3bfipaSkyMPD46YDAgAAyG/sSJOLpHDatGmSJIvFoo8//ljFixe3H8vIyND69esLzJpCAAAA5E6Ok8IpU6ZIulIpjIiIcLhV7OHhoYoVKyoiIiLvIwQAAHCywnSb11lynBQeOXJEktS8eXMtXrxYJUuWdFpQAAAAyF+5XlO4du1aZ8QBAABgGrakuYGnjx977DG9/fbbmdonTpyoTp065UlQAAAAyF+5TgrXr1+vhx9+OFP7Qw89pPXr1+dJUAAAAPnJ5sRXYZHrpPDChQtZbj3j7u6upKSkPAkKAAAA+SvXSWGtWrW0YMGCTO1fffWVatSokSdBAQAA5CdDFqe9CotcP2gyYsQIdezYUYcOHdJ//vMfSVJkZKS++OILLVq0KM8DBAAAcDYbu1fnPils166dli5dqjfffFOLFi2Sp6enateurTVr1sjf398ZMQIAAMDJcp0USlKbNm3Upk0bSVJSUpK+/PJLDR06VDExMcrIyMjTAAEAAJzNVohu8zpLrtcUXrV+/Xr17NlTZcqU0bvvvqv//Oc/2rRpU17GBgAAgHySq0phbGys5s2bp9mzZyspKUmdO3dWSkqKli5dykMmAACg0CpMD4Q4S44rhe3atVPVqlW1c+dOTZ06VSdOnND777/vzNgAAACQT3JcKfz+++/1wgsvqH///rrzzjudGRMAAEC+KkybTDtLjpPCDRs2aPbs2QoNDVX16tX15JNPqmvXrnkSRGxsrDZv3qzY2FhJUnBwsMLCwhQcHJwn4wMAAODacpwUNmjQQA0aNNDUqVO1YMECzZkzR+Hh4bLZbFq9erXKlSunEiVK5OriycnJ6tu3r7766itZLBb7ljYJCQkyDEPdunXThx9+KC8vr9zNCgAAIBdYU3gDTx97e3vrqaee0oYNG7Rr1y4NGTJEb731lgIDA/XII4/kaqxBgwYpOjpaK1as0OXLlxUXF6e4uDhdvnxZK1euVHR0tAYNGpTbEAEAAHKF7z6+iS1pJKlq1aqaOHGi/vvf/+rLL7/M9fnffPON5s2bp9atW8vV1dXe7urqqlatWmnOnDl8SwoAAEA+uKmk8CpXV1d16NBBy5Yty9V5NptNHh4e2R738PCQzWZujl2+dys13fK+Hjj2iRp8P16+de/I0XnBHRrqwbivVHfeEHubxc1VVV7vrsbrJqrlkXlqtmOGar3/nKxBJZ0V/g3r27eH9u3bqMTE/Vq//lvVr1/7mv07dmyjHTvWKDFxv3777Ue1bt3cfszNzU3jxw/Xb7/9qDNn/tDhw1s0e/YUhYQEOXsaudbzmW7atONHHTq5VctXf6k69Wpds3/b9q308+blOnRyq37auET/eeB+h+N/nd2d5avfwN7OnAZyyP/JNqqyfrZq7F2s2xe/K8+7q+ToPN+2TVTz8HcqH/GaQ7trgJ/KTnxRVaPmq8buRaowd4w8KpZxRugA8hiVwjxKCm9U27Zt1adPH23bti3TsW3btql///5q166dCZFdEdy+oaqNeVIH312kXx8YrvO7j6n+V8PlEeBzzfM8y5VWtVH/p4SovQ7trp4e8rm7og5NXqyolsO17anJ8q5cRvU+GerMaeTa44+308SJI/TGG1PVoEEb7dq1V8uXf6bSpUtl2b9Bg1B98sn7mjdvgcLCHtby5au0cOEs1ahx5T+wXl6eqlu3piZMmKYGDR5W1659dOedt2vRotn5Oa3reuTRBzVq/Eua/PYMPdisk/b8vk+ff/OhSgVk/fWN9e+to+kfv6MvP1us1k0f16oVazT7s/dVtXple586VZs6vAYPeE02m00rl63Or2khGz5t7lfwq8/o1LQvdajdIF3ee0QV54+Vaynfa57nXjZQwcOfUnL075mOVYh4XR7lg3W873gdbDtIaX+dUsVPx8viaXXWNAAgz1gMwzDtK6DPnj2r7t27a9WqVSpZsqQCAwMlSadOnVJiYqJat26tL774Qn5+frka94egvHkqusH343Vu2yHtfXXulQaLRc22Tdex2T/oyPvZVEVdLAr7drT+++U6+YdVk5uvl7b1ejfba/jUuV2NVr2pdfUG6PJfZ24q3g7nfr2p869av/5bxcTs0ODBIyVJFotFBw9u1syZ8zRp0oxM/T/9dLq8vb3UsePf1a+ff16qnTv3aODAV7O8Rmjo3dq48TvdeWcD/fnniZuKt5Rn7h5wys7y1V9qx7bf9fpLb0i6Mu8tv0dq7qwvNH3qx5n6z5w9SV7enurZdcDfY/z4hXb//odeCR+b5TVmfzZNxYt7q0uHp2863r/O7r7pMQqj329vmyfj3L74XV3aeUAnR0dcabBYVHXjPJ35ZLniI7JZtuLiokpfvaXERavlVf8uufp463i/Kz8vHpXKqErkRzrQ+jmlHDhuH7Pa5k8VN+kTnf36x5uKt+bh727qfADXtiKom9PGbhOX+yV2ZjC1UliyZEl9//332r17tyZNmqQePXqoR48emjRpknbv3q2VK1fmOiHMKxZ3V/ncXUlnftn1d6Nh6Mz6XfKrn/0tpspDHlNq/Dn99cXaHF3H3cdLhs2mtHMXbzbkPOHu7q569WppzZoN9jbDMLR27QaFhdXL8pwGDeo59Jekn35an21/SfL19ZHNZlNiYlLeBH6T3N3ddXedGvplXZS9zTAMbfh5k0LvyfrWeei9dfTLOsevdly3ZqNC76mTZf+A0qXUolUTffnZ4jyLGzfG4u4mz5qVdWHj9r8bDUMXNm6XV91q2Z4X+EJXZZw5p7NfZ670WjzcrwyTkuowppGaJq/6fOMTgIIvV19z5yzVq1dX9erVzQ7DgYe/j1zcXJV6+pxDe8rpc/K+s2yW5/jdW1W3dW+ujS1eydE1XKzuqvJ6d51c8qsyLly66ZjzQkCAv9zc3HTqVLxDe1xcvKpUyXo9ZVBQaZ06dfpf/U8rKKh0lv2tVqvGjx+ur7/+VufPX8ibwG+Sfyk/ubm5Kf60Y7X29OkzuuPOSlmeUzowQKf/1T/+9BmVDsz6Nnunbu114cJFfb+cW8dmcy3pI4ubq9LjEx3a0+MTZb3jtizP8apfQyU7tdLBti9keTzl0H+V+tcpBQ3rqb9e+0DGpRSVeqq93MuUlltg1ksQABQcNnakMT8pTE1N1dKlSxUVFeWweXWjRo3Uvn37az6IIkkpKSlKSUlxHNPIkIfFNZsznMPVu5junj5Avw+ZpbSE89ftb3FzVZ1ZgySLRbtfKlhr65zJzc1Nn38+QxaLNHDga9c/4RbS9YlHtWThd0r5ZyUJhYKLt6duezdcf736vjLOZlPdTs/Q8f5vqOxbg1Rj+wIZ6Rm6sHG7zq/7LX+DBYAbZGpSePDgQbVu3VonTpxQWFiYgoKuPI26bds2RURE6LbbbtP333+vypUrZzvGhAkTNGbMGIe2J7zu0v8Vr3lTsaUmJMmWniGP0o6Lzq2lfZVyKjFTf6+KQfIqH6h6nw6zt1lcrvyzo9Vfn+uXRuG6dCzuSvv/EsJit5XWlsfGFZgqoSTFxycoPT1dgYEBDu1BQQGKizud5TlxcacVGFj6X/1LZ+p/NSEsX76sHnywa4GpEkpSwplEpaenK+BfD9OULl1Kp/9VNb3q9Kn4TA/fBJQupdOnMq8NvbdhPVWucrv6P12wHioqqjLOJslIz5BbgJ9Du1uAn9JPn83U36N8sDzKBavCrJF/N/7v7/dd+7/VgZZ9lXo8Vpd/P6RDbV+QSwkvWdzdlJGQdGXt4q4DzpwOgDxgY/Nqc9cU9u/fX7Vq1VJcXJzWrVunBQsWaMGCBVq3bp3i4uJ01113acCAAdccY/jw4Tp37pzDq7P3zd+KNtIylLTziErd/4/k0mJRqftrKvG3/Zn6Jx88oQ1Nh+rXFi/bX6dWxShh4x792uJlXT5xJbG4mhB63R6iLZ3GK+1swUmMJCktLU1bt+5S8+aN7W0Wi0XNmjXW5s1bszxn06atDv0l6T//uc+h/9WEsHLlSnr44e5KSEh0Svw3Ki0tTTu379F9TRvY2ywWi+5rEqaYLTuyPCcmertDf0lq0ryhYrZsz9S32/89ph3bftee3/flady4MUZaui79flDFG/1jvajFouKNauvitj8y9U859F8deHCADrZ9wf46/9NmJW/apYNtX1DaScd/ONjOX1RGQpI8KpaRZ63KOr96s7OnBOAmGU583Yjp06erYsWKKlasmMLCwhQdHZ2j865+S1yHDh1yfU1TK4UbN25UdHS0fHwyb/Hi4+OjcePGKSws7JpjWK1WWa2O2z3k1a3joxErVGtaf53bfljnth1UxT4Py9XLqr+++lmSVOv955QSm6D9b3wlW0qaLvzxX4fz0//38MjVdoubq+rMHiyfWpW09f/elsXFxV6JTEu8ICMtI0/ivlnTpn2sjz9+V1u37tKWLds1cODT8vb20ieffC1Jmj17ik6ciNWIEW9LkqZPn6PVq7/WoEHP6vvv16hz50cUGnq3Bgy4srbSzc1NX34Zobp1a+rRR3vL1dXVvt4wISFRaWlp5kz0X2bNmK8pM97Uzm27tW3rLj3b/0l5entqwedLJEnvzXxTJ0+e0ltjp0qSZn/4mRZ9N099B/TUTz+uV/uOD+nuOjX10oujHcYtXsJbbdu30tgR7+TzjHAt8bOX6rZJg3Vp1wFd2rFfpXq3l4tXMZ1d9JMkqeykcKXHnVHcO/NlpKYpZf8xh/MzkpIlyaHd56HGykhIUuqJUypWtaJCRvZR0upNurAh87ZbAJCdBQsWKDw8XBEREQoLC9PUqVPVunVr7du3z75TS1aOHj2qoUOH6v7778+2z7WYmhT6+fnp6NGjqlkz61u9R48eNe3pY0mK/TZKHqV8dOdLnWQN9FPS7mP6rdtb9odPPMsGSLac/xugWIi/gh6sL0lqvHaiw7HoR8cq4dc9eRf8TVi0aLkCAvw1cmS4goJKa8eOPXrkkSftD5+UK1fGYVPxTZti1LPnCxo9eqjGjn1JBw8eVadOz2rPnisV1bJlg9WuXStJ0pYtqxyu1apVZ61f7/gEr1mWLflB/gH+Gvrq8yodGKDdu/7Q/z3e1/7wSZnbQmT7x+f9W/R2Pf/sS3rptRf08ogXdeTwMT39fwO1b+9Bh3Hbd3xYFotFS79Zma/zwbUlrfhFsf6+Chz8f3ILKKnLew/raK+RyvjfwyceZUpLudw83y3QXyGvPSPX/92GTly8Rqc/+MoJ0QPIawVpk+nJkyfr2WefVe/eV7Z6i4iI0IoVKzRnzhy98krWD7NmZGToiSee0JgxY/TLL78oMTEx19c1dZ/CkSNH6oMPPtCIESPUokUL+5rCuLg4RUZGavz48Ro4cKBGjx6dq3Hzap/Cwiav9iksbPJqn8LChn0Kixb2KQSca3Fwd6eN3ebY3EwPxWZ1p1O68gCul5eXFi1a5HALuGfPnkpMTNS3336b5TVGjRqlnTt3asmSJerVq5cSExO1dOnSXMVpaqVw7Nix8vb21jvvvKMhQ4bIYrmyyNMwDAUHB+vll1/WSy+9ZGaIAACgCLBZnPegSVYPxY4aNSrLold8fLwyMjLshbKrgoKC9Mcfmdc8S9KGDRs0e/Zsbd++/abiNH1Lmpdfflkvv/yyjhw54rAlTaVKWe8NBwAAUJgMHz5c4eHhDm1ZVQlvxPnz5/Xkk09q1qxZCggIuP4J12B6UnhVpUqVMiWCf/75p0aNGqU5c+aYFBUAACgKnLmWLrtbxVkJCAiQq6ur4uLiHNrj4uIUHBycqf+hQ4d09OhRtWvXzt52dd2/m5ub9u3bpzvuyPrLJ/7N1C1prichIUHz5883OwwAAIB84eHhodDQUEVGRtrbbDabIiMj1bBhw0z9q1Wrpl27dmn79u321yOPPKLmzZtr+/btKleuXI6vbWqlcNmyZdc8fvjw4XyKBAAAFGUF6enj8PBw9ezZU/Xr19e9996rqVOnKjk52f40co8ePVS2bFlNmDBBxYoVy7SLy9WdW7Lb3SU7piaFHTp0kMVi0bUegLY4ceEnAACAVLC++7hLly46ffq0Ro4cqdjYWNWpU0c//PCD/eGT48ePy8Ul72/2mpoUhoSEaMaMGWrfvn2Wx7dv367Q0NB8jgoAAMBczz//vJ5//vksj61bt+6a586bN++GrmnqmsLQ0FDFxMRke/x6VUQAAIC8YJPFaa/CwtRK4bBhw5ScnJzt8cqVK2vt2rX5GBEAAEDRZGpSeL3v5vP29lbTpk3zKRoAAFBUcV+ygG9JAwAAgPxRYDavBgAAMEtBevrYLFQKAQAAQKUQAACgIG1ebRaSQgAAUOTxoAm3jwEAACAqhQAAADxoIiqFAAAAEJVCAAAAHjQRlUIAAACISiEAAACVQlEpBAAAgKgUAgAAyODp41szKXww7iuzQzDFZbMDAPJBzcPfmR0CgFsQt4+5fQwAAADdopVCAACA3KBSSKUQAAAAolIIAAAgw+wACgAqhQAAAKBSCAAAYGNLGiqFAAAAoFIIAADA08ciKQQAACApFLePAQAAICqFAAAAbEkjKoUAAAAQlUIAAAC2pBGVQgAAAIhKIQAAAE8fi0ohAAAARKUQAACAp49FpRAAAACiUggAACAbtcKCXSlMTk7W+vXrzQ4DAADc4mxOfBUWBTopPHjwoJo3b252GAAAALc8bh8DAIAij5vHJieF/v7+1zyekZGRT5EAAAAUbaYmhSkpKerfv79q1aqV5fFjx45pzJgx+RwVAAAoagrT2j9nMTUprFOnjsqVK6eePXtmeXzHjh0khQAAAPnA1KSwTZs2SkxMzPa4v7+/evTokX8BAQCAIslmMTsC81kMw2BtJQAAKNJGVnzCaWOPPfq508bOS4X+6eOUlBSlpKQ4tFmtVlmtVpMiAgAAhQ2bVxeApDA1NVVLly5VVFSUYmNjJUnBwcFq1KiR2rdvLw8Pj2ueP2HChEzrDkeNGqXRo0c7K2QAAHCLISU0+fbxwYMH1bp1a504cUJhYWEKCgqSJMXFxWnz5s267bbb9P3336ty5crZjkGlEAAA3KzXKnZ32thvHP3CaWPnJVMrhVe3o9m2bZt8fHwcjiUlJalHjx4aMGCAVq1ale0YJIAAAOBmsSWNyUnhxo0bFR0dnSkhlCQfHx+NGzdOYWFhJkQGAABQtJj63cd+fn46evRotsePHj0qPz+/fIsHAAAUTTYZTnsVFqZWCp955hn16NFDI0aMUIsWLRzWFEZGRmr8+PEaOHCgmSECAAAUCaYmhWPHjpW3t7feeecdDRkyRBbLlZ0jDcNQcHCwXn75Zb300ktmhggAAIqAwlPPc54Cs3n1kSNHHLakqVSpkskRAQCAouKlit2cNvbEo186bey8ZPo+hVdVqlQpUyL4559/atSoUZozZ45JUQEAgKKAp49NftDkehISEjR//nyzwwAAALc4HjQxuVK4bNmyax4/fPhwPkUCAABQtJmaFHbo0EEWi0XXWtZ49eETAAAAZyk89TznMfX2cUhIiBYvXiybzZbla+vWrWaGBwAAUGSYmhSGhoYqJiYm2+PXqyICAADkBZsTX4WFqbePhw0bpuTk5GyPV65cWWvXrs3HiAAAAIomU5PC+++//5rHvb291bRp03yKBgAAFFUGqwoL9pY0AAAAyB8FZvNqAAAAsxSmtX/OQlIIAACKvMK0ybSzcPsYAAAAVAoBAACoE1IpBAAAgKgUAgAAsKZQVAoBAAAgKoUAAABsSSMqhQAAABCVQgAAAL7mTiSFAAAA3D4Wt48BAAAgKoUAAADcPhaVQgAAAIhKIQAAAGsKRaUQAAAAolIIAAAgm8GaQiqFAAAAoFIIAABAnZCkEAAAQDbSQm4fAwAAgEohAAAAm1eLSiEAAABEpRAAAIDNq0WlEAAAAKJSCAAAwNPHolIIAABQ4EyfPl0VK1ZUsWLFFBYWpujo6Gz7zpo1S/fff79KliypkiVLqmXLltfsnx2SQgAAUOQZTvxfbi1YsEDh4eEaNWqUtm7dqtq1a6t169Y6depUlv3XrVunbt26ae3atYqKilK5cuXUqlUr/fXXX7m6rsUw+LI/AABQtHWs8IjTxl58bFmu+oeFhemee+7RBx98IEmy2WwqV66cBg4cqFdeeeW652dkZKhkyZL64IMP1KNHjxxfl0ohAACAE6WkpCgpKcnhlZKSkmXf1NRUxcTEqGXLlvY2FxcXtWzZUlFRUTm63sWLF5WWliZ/f/9cxUlSCAAAijzDMJz2mjBhgnx9fR1eEyZMyDKO+Ph4ZWRkKCgoyKE9KChIsbGxOZrLyy+/rDJlyjgkljnB08cAAABONHz4cIWHhzu0Wa1Wp1zrrbfe0ldffaV169apWLFiuTq3QCSF0dHRioqKsmfAwcHBatiwoe69916TIwMAAEWBM7eksVqtOU4CAwIC5Orqqri4OIf2uLg4BQcHX/PcSZMm6a233tJPP/2ku+++O9dxmnr7+NSpU7r//vvVoEEDTZkyRWvWrNGaNWs0ZcoUNWjQQPfff3+2T9oAAADcajw8PBQaGqrIyEh7m81mU2RkpBo2bJjteRMnTtS4ceP0ww8/qH79+jd0bVOTwueee04ZGRnau3evjh49qs2bN2vz5s06evSo9u7dK5vNpgEDBpgZIgAAKAJsTnzlVnh4uGbNmqX58+dr79696t+/v5KTk9W7d29JUo8ePTR8+HB7/7ffflsjRozQnDlzVLFiRcXGxio2NlYXLlzI1XVNvX28atUqrV+/XlWrVs10rGrVqpo2bZqaNWuW/4EBAACYpEuXLjp9+rRGjhyp2NhY1alTRz/88IP94ZPjx4/LxeXvut7MmTOVmpqqxx9/3GGcUaNGafTo0Tm+rqlJodVqVVJSUrbHz58/77SFmAAAAFfdyCbTzvT888/r+eefz/LYunXrHP589OjRPLmmqbePu3Tpop49e2rJkiUOyWFSUpKWLFmi3r17q1u3biZGCAAAigKbDKe9CgtTK4WTJ0+WzWZT165dlZ6eLg8PD0lXNm50c3PT008/rUmTJpkZIgAAQJFQIL7mLikpSTExMQ5b0oSGhsrHx8fkyAAAQFHwULmHnDb2939+77Sx81KB2KfQx8dHzZs3v6FzU1JSMn1VTG72AwIAAEAB/5q7uLg4jR079pp9cvPVMQAAAFkpSFvSmKVA3D7Ozo4dO1SvXj1lZGRk24dKIQAAuFmtnXj7eBW3j69v586d1zy+b9++645BAggAAG5WQduSxgymJoV16tSRxWJRVsXKq+0Wi8WEyAAAAIoWU5NCf39/TZw4US1atMjy+O7du9WuXbt8jgoAABQ1hWk/QWcxNSkMDQ3ViRMnVKFChSyPJyYmZllFBAAAQN4yNSns16+fkpOTsz1evnx5zZ07Nx8jAgAARRFFqAL+9DEAAEB+aH7bA04be+1/Vztt7LxUoPcp/PPPP/XUU0+ZHQYAAMAtr0AnhQkJCZo/f77ZYQAAgFuc4cT/FRamrilctmzZNY8fPnw4nyIBAAAo2kxNCjt06JDtPoVXsU8hAABwNhuPWJh7+zgkJESLFy+WzWbL8rV161YzwwMAACgyTE0KQ0NDFRMTk+3x61URAQAA8oLhxFdhYert42HDhl1zn8LKlStr7dq1+RgRAABA0cQ+hQAAoMhrXPY/Tht7419rnDZ2XjK1UggAAFAQ8N3HBXyfQgAAAOQPKoUAAKDIYzUdlUIAAACISiEAAABrCkWlEAAAAKJSCAAAIINKIZVCAAAAUCkEAADg6WORFAIAAPCgibh9DAAAAFEpBAAA4PaxqBQCAABAVAoBAABYUygqhQAAABCVQgAAADavFpVCAAAAiEohAACAbDx9TFIIAADA7WNuHwMAAEBUCgEAALh9LCqFAAAAEJVCAAAA1hSKSiEAAABEpRAAAIA1haJSCAAAAFEpBAAAYE2hSAoBAAC4fSxuHwMAAEBUCgEAALh9LCqFAAAAEJVCAAAAGYbN7BBMR6UQAAAABTspPHv2rD755BOzwwAAALc4mwynvQqLAp0UHj9+XL179zY7DAAAgFueqWsKk5KSrnn8/Pnz+RQJAAAoygz2KTQ3KfTz85PFYsn2uGEY1zwOAACQFwrTbV5nMTUpLFGihF577TWFhYVlefzAgQPq27dvPkcFAABQ9JiaFNarV0+S1LRp0yyP+/n5Uc4FAABOR75h8oMm3bt3V7FixbI9HhwcrFGjRuVjRAAAAEWTxSA1BgAARVyIXw2njX0ycY/Txs5Lhf4bTVJSUpSSkuLQZrVaZbVaTYoIAACg8CnQ+xTGxcVp7Nix1+wzYcIE+fr6OrwmTJiQTxECAIBbgeHE/xUWBfr28Y4dO1SvXj1lZGRk24dKIQAAuFnBftWdNnZs4l6njZ2XTL19vHPnzmse37dv33XHIAEEAAA3qwDXyPKNqUlhnTp1ZLFYsvwgrrazeTUAAHA2Nq82OSn09/fXxIkT1aJFiyyP7969W+3atcvnqAAAAIoeU5PC0NBQnThxQhUqVMjyeGJiIuVcAADgdOQbJieF/fr1U3JycrbHy5cvr7lz5+ZjRAAAAEVTgX76GAAAID/4l7jTaWMnnD/gtLHzUoHep/DPP//UU089ZXYYAAAAt7wCnRQmJCRo/vz5ZocBAABucYZhOO1VWJi6pnDZsmXXPH748OF8igQAAKBoM3VNoYuLS7b7FF5lsViu+Y0mAAAAN8u3+B1OG/vchUNOGzsvmXr7OCQkRIsXL5bNZsvytXXrVjPDAwAARQS3j01OCkNDQxUTE5Pt8etVEQEAAJA3TF1TOGzYsGvuU1i5cmWtXbs2HyMCAABFkY0iFPsUAgAAFPeq5LSxL1w84rSx85KplUIAAICCwBA1sgK9TyEAAADyB5VCAABQ5LGmkEohAAAARKUQAACALfBEpRAAAACiUggAAMDTxyIpBAAA4PaxuH0MAAAAUSkEAACgUigqhQAAABCVQgAAAB4zEZVCAAAAiKQwT6WkpGj06NFKSUkxO5R8xbyZd1HAvJl3UVBU5y1J6al/Oe1VWFgMVlbmmaSkJPn6+urcuXPy8fExO5x8w7yZd1HAvJl3UVBU540rqBQCAACApBAAAAAkhQAAABBJYZ6yWq0aNWqUrFar2aHkK+bNvIsC5s28i4KiOm9cwYMmAAAAoFIIAAAAkkIAAACIpBAAAAAiKQQAAIBICnNs+vTpqlixoooVK6awsDBFR0fbj12+fFkDBgxQqVKlVLx4cT322GOKi4tzOP/48eNq06aNvLy8FBgYqGHDhik9PT2/p5FrNzvvF154QaGhobJarapTp04+R3/jrjXvjz76SM2aNZOPj48sFosSExMznZ+QkKAnnnhCPj4+8vPz09NPP60LFy7k4wxuzM3O+4033lCjRo3k5eUlPz+//Av8Jqxfv17t2rVTmTJlZLFYtHTpUofjhmFo5MiRCgkJkaenp1q2bKkDBw449CmMn3dezPtW/LwXL16sVq1aqVSpUrJYLNq+fXumMXLyu6+gyYt55+R3AAo3ksIcWLBggcLDwzVq1Cht3bpVtWvXVuvWrXXq1ClJ0uDBg7V8+XItXLhQP//8s06cOKGOHTvaz8/IyFCbNm2UmpqqX3/9VfPnz9e8efM0cuRIs6aUIzc776ueeuopdenSJb/Dv2HXm/fFixf14IMP6tVXX812jCeeeEK7d+/W6tWr9d1332n9+vXq06dPfk3hhuTFvFNTU9WpUyf1798/v8K+acnJyapdu7amT5+e5fGJEydq2rRpioiI0ObNm+Xt7a3WrVvr8uXL9j6F8fPOi3nfip93cnKy7rvvPr399tvZjpHT330FSV7MOye/A1DIGbiue++91xgwYID9zxkZGUaZMmWMCRMmGImJiYa7u7uxcOFC+/G9e/cakoyoqCjDMAxj5cqVhouLixEbG2vvM3PmTMPHx8dISUnJv4nk0s3O+59GjRpl1K5dOz/CvmnXmvc/rV271pBknD171qF9z549hiRjy5Yt9rbvv//esFgsxl9//eXU2G/Gzc77n+bOnWv4+vo6KVLnkWQsWbLE/mebzWYEBwcb77zzjr0tMTHRsFqtxpdffmkYRuH9vP/pRub9T7fK5/1PR44cMSQZ27Ztc2jP7e++guhG5v1POfkdgMKJSuF1pKamKiYmRi1btrS3ubi4qGXLloqKilJMTIzS0tIcjlerVk3ly5dXVFSUJCkqKkq1atVSUFCQvU/r1q2VlJSk3bt3599kciEv5l0YXW/eOREVFSU/Pz/Vr1/f3tayZUu5uLho8+bNeR5zXsiLed+Kjhw5otjYWIf3xdfXV2FhYQ5/vwvb5309OZl3UXWr/u4DJG4fX1d8fLwyMjIcEjpJCgoKUmxsrGJjY+Xh4ZFpPc3V45IUGxub5flXjxVEeTHvwuh6886J2NhYBQYGOrS5ubnJ39+/wL43eTHvW9HVuV/rfSmMn/f15GTeRdWt+rsPkEgKAQAAIJLC6woICJCrq2umJ8vi4uIUHBys4OBgpaamZnoK6+pxSQoODs7y/KvHCqK8mHdhdL1550RwcLD94Yyr0tPTlZCQUGDfm7yY963o6tyv9b4Uxs/7enIy76LqVv3dB0gkhdfl4eGh0NBQRUZG2ttsNpsiIyPVsGFDhYaGyt3d3eH4vn37dPz4cTVs2FCS1LBhQ+3atcvhPxyrV6+Wj4+PatSokX+TyYW8mHdhdL1550TDhg2VmJiomJgYe9uaNWtks9kUFhaW5zHnhbyY962oUqVKCg4OdnhfkpKStHnzZoe/34Xt876enMy7qLpVf/cBkuRmdgCFQXh4uHr27Kn69evr3nvv1dSpU5WcnKzevXvL19dXTz/9tMLDw+Xv7y8fHx8NHDhQDRs2VIMGDSRJrVq1Uo0aNfTkk09q4sSJio2N1euvv64BAwbIarWaPLvs3ey8JengwYO6cOGCYmNjdenSJfveVzVq1JCHh4dJM7u2a81bkn1N5cGDByVJu3btUokSJVS+fHn5+/urevXqevDBB/Xss88qIiJCaWlpev7559W1a1eVKVPGzKld083OW7qyH2dCQoKOHz+ujIwM++dduXJlFS9e3JR5Xc+FCxfsc5KuPGSxfft2+fv7q3z58nrxxRc1fvx43XnnnapUqZJGjBihMmXKqEOHDpJUaD/vm523dGt+3lfnc+LECUlXEj5J9jskOf3dV9Dc7LylnP0OQCFn9uPPhcX7779vlC9f3vDw8DDuvfdeY9OmTfZjly5dMp577jmjZMmShpeXl/Hoo48aJ0+edDj/6NGjxkMPPWR4enoaAQEBxpAhQ4y0tLT8nkau3ey8mzZtakjK9Dpy5Eg+zyR3rjXvUaNGZTmnuXPn2vucOXPG6Natm1G8eHHDx8fH6N27t3H+/HkTZpI7Nzvvnj17Ztln7dq1+T+ZHLq6vca/Xz179jQM48r2LCNGjDCCgoIMq9VqtGjRwti3b5/DGIXx886Led+Kn/fcuXOzPD5q1Cj7GDn53VfQ5MW8c/I7AIWbxTAMI2/SSwAAABRWrCkEAAAASSEAAABICgEAACCSQgAAAIikEAAAACIpBAAAgEgKAQAAIJJCAAAAiKQQQAHWq1cvh69Va9asmV588cV8j2PdunWyWCxKTEzM92sDQH4hKQSQa7169ZLFYpHFYpGHh4cqV66ssWPHKj093anXXbx4scaNG5ejviRyAJA7bmYHAKBwevDBBzV37lylpKRo5cqVGjBggNzd3TV8+HCHfqmpqfLw8MiTa/r7++fJOACAzKgUArghVqtVwcHBqlChgvr376+WLVtq2bJl9lu+b7zxhsqUKaOqVatKkv7880917txZfn5+8vf3V/v27XX06FH7eBkZGQoPD5efn59KlSqll156Sf/+avZ/3z5OSUnRyy+/rHLlyslqtapy5cqaPXu2jh49qubNm0uSSpYsKYvFol69ekmSbDabJkyYoEqVKsnT01O1a9fWokWLHK6zcuVKValSRZ6enmrevLlDnABwqyIpBJAnPD09lZqaKkmKjIzUvn37tHr1an333XdKS0tT69atVaJECf3yyy/auHGjihcvrgcffNB+zrvvvqt58+Zpzpw52rBhgxISErRkyZJrXrNHjx768ssvNW3aNO3du1cffvihihcvrnLlyumbb76RJO3bt08nT57Ue++9J0maMGGCPvnkE0VERGj37t0aPHiw/u///k8///yzpCvJa8eOHdWuXTtt375dzzzzjF555RVnvW0AUGBw+xjATTEMQ5GRkVq1apUGDhyo06dPy9vbWx9//LH9tvFnn30mm82mjz/+WBaLRZI0d+5c+fn5ad26dWrVqpWmTp2q4cOHq2PHjpKkiIgIrVq1Ktvr7t+/X19//bVWr16tli1bSpJuv/12+/Grt5oDAwPl5+cn6Upl8c0339RPP/2khg0b2s/ZsGGDPvzwQzVt2lQzZ87UHXfcoXfffVeSVLVqVe3atUtvv/12Hr5rAFDwkBQCuCHfffedihcvrrS0NNlsNnXv3l2jR4/WgAEDVKtWLYd1hDt27NDBgwdVokQJhzEuX76sQ4cO6dy5czp58qTCwsLsx9zc3FS/fv1Mt5Cv2r59u1xdXdW0adMcx3zw4EFdvHhRDzzwgEN7amqq6tatK0nau3evQxyS7AkkANzKSAoB3JDmzZtr5syZ8vDwUJkyZeTm9vevE29vb4e+Fy5cUGhoqD7//PNM45QuXfqGru/p6Znrcy5cuCBJWrFihcqWLetwzGq13lAcAHCrICkEcEO8vb1VuXLlHPWtV6+eFixYoMDAQPn4+GTZJyQkRJs3b1aTJk0kSenp6YqJiVG9evWy7F+rVi3ZbDb9/PPP9tvH/3S1UpmRkWFvq1GjhqxWq44fP55thbF69epatmyZQ9umTZuuP0kAKOR40ASA0z3xxBMKCAhQ+/bt9csvv+jIkSNat26dXnjhBf33v/+VJA0aNEhvvfWWli5dqj/++EPPPffcNfcYrFixonr27KmnnnpKS5cutY/59ddfS5IqVKggi8Wi7777TqdPn9aFCxdUokQJDR06VIMHD9b8+fN16NAhbd26Ve+//77mz58vSerXr58OHDigYcOGad++ffriiy80b948Z79FAGA6kkIATufl5aX169erfPny6tixo6pXr66nn35aly9ftlcOhwwZoieffFI9e/ZUw4YNVaJECT366KPXHHfmzJl6/PHH9dxzz6latWp69tlnlZycLEkqW7asxowZo1deeUVBQUF6/vnnJUnjxo3TiBEjNGHCBFWvXl0PPvigVqxYoUqVKkmSypcvr2+++UZLly5V7dq1FRERoTfffNOJ7w4AFAwWI7tV3AAAACgyqBQCAACApBAAAAAkhQAAABBJIQAAAERSCAAAAJEUAgAAQCSFAAAAEEkhAAAARFIIAAAAkRQCAABAJIUAAACQ9P/d2v7/feA3mAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = [\"000\", \"001\", \"010\", \"011\", \"100\", \"101\", \"110\", \"111\"]\n",
    "\n",
    "y_test = np.array([y for _, y in dataset]).astype(int)\n",
    "y_preds = np.array([p for p in predictions]).astype(int)\n",
    "\n",
    "y_test = [''.join(map(str, label)) for label in y_test]\n",
    "y_preds = [''.join(map(str, label)) for label in y_preds]\n",
    "\n",
    "\n",
    "# Compute confusion matrix with all possible labels\n",
    "conf_matrix = confusion_matrix(y_test, y_preds, labels=categories)\n",
    "row_sums = conf_matrix.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / row_sums\n",
    "conf_matrix_normalized[conf_matrix == 0] = np.nan\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8,6), dpi=100)\n",
    "ax = sns.heatmap(conf_matrix_normalized, annot=True, fmt=\".2f\", mask=np.isnan(conf_matrix_normalized))\n",
    "\n",
    "# Set tick labels\n",
    "ax.set_xticks(np.arange(len(categories)) + 0.5)\n",
    "ax.set_yticks(np.arange(len(categories)) + 0.5)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_yticklabels(categories)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
