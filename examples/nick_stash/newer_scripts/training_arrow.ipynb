{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cc_hardware.utils.manager import Manager\n",
    "from cc_hardware.utils.file_handlers import PklReader\n",
    "from cc_hardware.utils import register_cli, run_cli\n",
    "from cc_hardware.utils import get_logger\n",
    "\n",
    "from cc_hardware.algos.datasets import HistogramDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramDataset(Dataset):\n",
    "    def __init__(self, pkl_path: Path = None, predict_magnitude: bool = False, rolling_window=5, merge: bool = False):\n",
    "        if pkl_path is None:\n",
    "            self.data = []\n",
    "            self.inputs = []\n",
    "            self.targets = []\n",
    "            return\n",
    "\n",
    "        self.data = PklReader.load_all(pkl_path)\n",
    "        inputs = dict(\n",
    "            histogram=[],\n",
    "            position=[],\n",
    "        )\n",
    "        # print(type(self.data))\n",
    "        for d in self.data:\n",
    "            # print(type(d))\n",
    "            # print(d.keys())\n",
    "            # print(d)\n",
    "            if \"has_masks\" in d and not d[\"has_masks\"]:\n",
    "                d[\"position\"] = [0, 0, 0]\n",
    "                print(\"mask continue\")\n",
    "                continue\n",
    "            if \"histogram\" not in d or \"position\" not in d:\n",
    "                print(\"hist continue\")\n",
    "                continue\n",
    "            inputs[\"histogram\"].append(torch.tensor(d[\"histogram\"]))\n",
    "            \n",
    "            pos = d[\"position\"]\n",
    "            # print(f\"position: {pos}\")\n",
    "            inputs[\"position\"].append(torch.tensor((pos[\"x\"], pos[\"y\"])))\n",
    "            # print(f\"inputs: {inputs}\")\n",
    "            # raise Exception(\"exit\")\n",
    "        # print(inputs['position'])\n",
    "        # print(f\"original targets shape: {torch.stack(inputs['position']).shape}\")\n",
    "        \n",
    "        if len(inputs[\"histogram\"][0].shape) == 2:\n",
    "            # single capture per location\n",
    "            # reading input as list of location samples: (pixels, bins)\n",
    "            # reading targets as list of location samples: ((x, y) position)\n",
    "            self.raw_inputs = torch.stack(inputs[\"histogram\"]).float()\n",
    "            num_samples = self.raw_inputs.shape[0]\n",
    "            width = np.sqrt(self.raw_inputs.shape[1]).astype(int)\n",
    "            height = width\n",
    "            bins = self.raw_inputs.shape[2]\n",
    "            self.raw_inputs = torch.reshape(self.raw_inputs, (num_samples, width, height, bins))\n",
    "            self.raw_targets = torch.stack(inputs[\"position\"]).float()\n",
    "        elif len(inputs[\"histogram\"][0].shape) == 3:\n",
    "            # multiple captures per location:\n",
    "            # reading input as list of location samples: (captures per location, pixels, bins)\n",
    "            # reading targets as list of location samples: ((x, y) position)\n",
    "            self.raw_inputs = torch.stack(inputs[\"histogram\"]).float()  # (locations, captures, pixels, bins)\n",
    "            samples_per_location = inputs[\"histogram\"][0].shape[0]\n",
    "            # print(f\"samples per location: {samples_per_location}\")\n",
    "            # print(f\"raw shape: {self.raw_inputs.shape}\")\n",
    "            if rolling_window is not None:\n",
    "                sliding_mean = np.array(\n",
    "                    [self.raw_inputs[:, j - rolling_window + 1: j + 1, :, :].mean(axis=1)\n",
    "                        for j in range(rolling_window - 1, samples_per_location)\n",
    "                    ]).swapaxes(0, 1)\n",
    "                # print(f\"sliding mean shape: {sliding_mean.shape}\")\n",
    "                self.raw_inputs = torch.tensor(sliding_mean)\n",
    "            samples_per_location = self.raw_inputs.shape[1]\n",
    "            # print(f\"samples_per_location: {samples_per_location}\")\n",
    "            # print(f\"shape: {self.raw_inputs.shape}\")\n",
    "            self.raw_inputs = torch.reshape(self.raw_inputs, \n",
    "                (self.raw_inputs.shape[0] * self.raw_inputs.shape[1], self.raw_inputs.shape[2], self.raw_inputs.shape[3]))\n",
    "            # self.raw_inputs = torch.concat(inputs[\"histogram\"], dim=0).float()  # (all captures, pixels, bins)\n",
    "            num_samples = self.raw_inputs.shape[0]\n",
    "            width = np.sqrt(self.raw_inputs.shape[1]).astype(int)\n",
    "            height = width\n",
    "            bins = self.raw_inputs.shape[2]\n",
    "            self.raw_inputs = torch.reshape(self.raw_inputs, (num_samples, width, height, bins))\n",
    "            self.raw_targets = torch.stack(inputs[\"position\"], dim=0).float()  # (location samples, (x, y) position)\n",
    "            self.raw_targets = torch.repeat_interleave(self.raw_targets, samples_per_location, dim=0)\n",
    "\n",
    "        # raw_inputs: (all captures, width, height, bins)\n",
    "        # raw_targets: (all captures, (x, y) position)\n",
    "\n",
    "        self.START_BIN = 0\n",
    "        self.END_BIN = bins\n",
    "        self.inputs = self.raw_inputs\n",
    "        self.targets = self.raw_targets\n",
    "\n",
    "        if predict_magnitude:\n",
    "            self.targets = torch.linalg.norm(self.targets, dim=1, keepdim=True)\n",
    "\n",
    "        if merge:\n",
    "            self.inputs = self.inputs.sum(dim=(1), keepdim=True)\n",
    "\n",
    "    def set_start_bin(self, start_bin: int):\n",
    "        self.START_BIN = start_bin\n",
    "        self.inputs = self.raw_inputs[:, :, :, self.START_BIN:self.END_BIN]\n",
    "\n",
    "    def set_end_bin(self, end_bin: int):\n",
    "        self.END_BIN = end_bin\n",
    "        self.inputs = self.raw_inputs[:, :, :, self.START_BIN:self.END_BIN]\n",
    "\n",
    "    def get_raw_bin_num(self):\n",
    "        return self.raw_inputs.shape[3]\n",
    "\n",
    "    def get_bin_num(self):\n",
    "        return self.inputs.shape[3]\n",
    "    \n",
    "    def augment(self, factor: int):\n",
    "        std = self.inputs.std(dim=0)\n",
    "        self.inputs = self.inputs.repeat_interleave(factor, dim=0)\n",
    "        self.inputs += torch.normal(torch.zeros_like(self.inputs), std)\n",
    "        self.targets = self.targets.repeat_interleave(factor, dim=0)\n",
    "\n",
    "    def get_mean_capture(self):\n",
    "        return self.inputs.mean(dim=0)\n",
    "    \n",
    "    def set_zero(self, zero: torch.Tensor):\n",
    "        # sets the empty capture of the dataset\n",
    "        self.inputs = self.inputs - zero\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        combined_dataset = HistogramDataset()\n",
    "        combined_dataset.raw_inputs = torch.cat((self.raw_inputs, other.raw_inputs), dim=0)\n",
    "        combined_dataset.raw_targets = torch.cat((self.raw_targets, other.raw_targets), dim=0)\n",
    "        combined_dataset.START_BIN = self.START_BIN\n",
    "        combined_dataset.END_BIN = self.END_BIN\n",
    "        combined_dataset.inputs = combined_dataset.raw_inputs[:, :, :, self.START_BIN:self.END_BIN]\n",
    "        combined_dataset.targets = combined_dataset.raw_targets\n",
    "        return combined_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = \"../../datasets/robotics/localization/demo_arrow/capture_retro_1.pkl\"\n",
    "dataset_1 = HistogramDataset(\n",
    "    pkl_path,\n",
    "    rolling_window=5\n",
    ")\n",
    "# pkl_path = \"../../datasets/robotics/localization/10x10_samples/capture_4.pkl\"\n",
    "# dataset_1 = dataset_1 + HistogramDataset(\n",
    "#     pkl_path,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9600, 8, 8, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1.raw_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9600, 8, 8, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9600, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9600, 8, 8, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9600, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.augment(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([96000, 8, 8, 16])\n",
      "target shape: torch.Size([96000, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f'input shape: {dataset.inputs.shape}')\n",
    "print(f'target shape: {dataset.targets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes for training, validation, and test sets\n",
    "train_size = int(0.5 * len(dataset))\n",
    "val_size = int(0.25 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size],\n",
    "                                                        generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x shape: torch.Size([32, 8, 8, 16])\n",
      "batch_y shape: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_loader:\n",
    "    print(f'batch_x shape: {batch_x.shape}')\n",
    "    print(f'batch_y shape: {batch_y.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = 16\n",
    "WIDTH = 8\n",
    "HEIGHT = 8\n",
    "\n",
    "# NUM_BINS = 32\n",
    "# WIDTH = 4\n",
    "# HEIGHT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocationCNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(in_channels=(end_bin - start_bin), out_channels=16, kernel_size=3, padding=1)\n",
    "        # self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=1)\n",
    "        # self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        out_channels = 4\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=out_channels, kernel_size=(3, 3, 7), padding=(1, 1, 3))\n",
    "        self.batchnorm3d = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels * NUM_BINS * HEIGHT * WIDTH, 128)\n",
    "\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 output dimensions (x, y)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # # print(f'x shape at start: {x.shape}')\n",
    "        # x = self.relu(self.conv1(x))\n",
    "        # # print(f'x shape after conv1: {x.shape}')\n",
    "        # x = self.batchnorm1(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # print(f'x shape after pool1: {x.shape}')\n",
    "        # x = self.relu(self.conv2(x))\n",
    "        # # print(f'x shape after conv2: {x.shape}')\n",
    "        # x = self.batchnorm2(x)\n",
    "\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "\n",
    "        # x = self.pool(x)\n",
    "        # print(f'x shape after pool2: {x.shape}')\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLocation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLocation, self).__init__()\n",
    "\n",
    "        # in: (n, HEIGHT, WIDTH, 42 or 16)\n",
    "        self.conv_channels = 4\n",
    "        self.conv_channels2 = 8\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=self.conv_channels, kernel_size=(3, 3, 7), padding=(1, 1, 3))\n",
    "        # (n, 4, HEIGHT, WIDTH, 42 or 16)\n",
    "        self.batchnorm3d = nn.BatchNorm3d(self.conv_channels)\n",
    "        self.batchnorm3d2 = nn.BatchNorm3d(self.conv_channels2)\n",
    "        # reshape to (n, 4, HEIGHT x WIDTH, 42)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 4, HEIGHT, WIDTH, 21)\n",
    "        self.conv3d2 = nn.Conv3d(in_channels=self.conv_channels, out_channels=self.conv_channels2, kernel_size=(3, 3, 5), padding=(1, 1, 2))\n",
    "        # (n, 8, HEIGHT, WIDTH, 21)\n",
    "        # reshape to (n, 8, HEIGHT x WIDTH, 21)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=3, padding=0)\n",
    "        # (n, 8, HEIGHT, WIDTH, 7)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.conv_channels2 * HEIGHT * WIDTH * 7, 128)\n",
    "        # self.fc1 = nn.Linear(self.conv_channels * NUM_BINS * HEIGHT * WIDTH / 2 / 2 / 2, 128)\n",
    "\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 output dimensions (x, y)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels * HEIGHT * WIDTH, NUM_BINS))\n",
    "        x = self.pool1(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels, HEIGHT, WIDTH, -1))\n",
    "        x = self.relu(self.conv3d2(x))\n",
    "        x = self.batchnorm3d2(x)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2 * HEIGHT * WIDTH, -1))\n",
    "        x = self.pool2(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2, HEIGHT, WIDTH, -1))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLocation8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLocation8, self).__init__()\n",
    "\n",
    "        # in: (n, HEIGHT, WIDTH, 16)\n",
    "        self.conv_channels = 4\n",
    "        self.conv_channels2 = 8\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=self.conv_channels, kernel_size=(3, 3, 7), padding=(1, 1, 3))\n",
    "        # (n, 4, HEIGHT, WIDTH, 16)\n",
    "        self.batchnorm3d = nn.BatchNorm3d(self.conv_channels)\n",
    "        self.batchnorm3d2 = nn.BatchNorm3d(self.conv_channels2)\n",
    "        # reshape to (n, 4, HEIGHT x WIDTH, 16)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 4, HEIGHT, WIDTH, 8)\n",
    "        self.conv3d2 = nn.Conv3d(in_channels=self.conv_channels, out_channels=self.conv_channels2, kernel_size=(3, 3, 5), padding=(1, 1, 2))\n",
    "        # (n, 8, HEIGHT, WIDTH, 8)\n",
    "        # reshape to (n, 8, HEIGHT x WIDTH, 8)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 8, HEIGHT, WIDTH, 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.conv_channels2 * HEIGHT * WIDTH * 4, 128)\n",
    "        # self.fc1 = nn.Linear(self.conv_channels * NUM_BINS * HEIGHT * WIDTH / 2 / 2 / 2, 128)\n",
    "\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 output dimensions (x, y)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels * HEIGHT * WIDTH, NUM_BINS))\n",
    "        x = self.pool1(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels, HEIGHT, WIDTH, -1))\n",
    "        x = self.relu(self.conv3d2(x))\n",
    "        x = self.batchnorm3d2(x)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2 * HEIGHT * WIDTH, -1))\n",
    "        x = self.pool2(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2, HEIGHT, WIDTH, -1))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLocation8v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLocation8v2, self).__init__()\n",
    "\n",
    "        # in: (n, HEIGHT, WIDTH, 16)\n",
    "        self.conv_channels = 4\n",
    "        self.conv_channels2 = 8\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=self.conv_channels, kernel_size=(3, 3, 7), padding=(1, 1, 3))\n",
    "        # (n, 4, HEIGHT, WIDTH, 16)\n",
    "        self.batchnorm3d = nn.BatchNorm3d(self.conv_channels)\n",
    "        self.batchnorm3d2 = nn.BatchNorm3d(self.conv_channels2)\n",
    "        # reshape to (n, 4, HEIGHT x WIDTH, 16)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 4, HEIGHT, WIDTH, 8)\n",
    "        self.conv3d2 = nn.Conv3d(in_channels=self.conv_channels, out_channels=self.conv_channels2, kernel_size=(3, 3, 5), padding=(1, 1, 2))\n",
    "        # (n, 8, HEIGHT, WIDTH, 8)\n",
    "        # reshape to (n, 8, HEIGHT x WIDTH, 8)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 8, HEIGHT, WIDTH, 4)\n",
    "        self.conv3d3 = nn.Conv3d(in_channels=self.conv_channels2, out_channels=self.conv_channels2, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        # (n, 8, HEIGHT, WIDTH, 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.conv_channels2 * HEIGHT * WIDTH * 4, 128)\n",
    "        # self.fc1 = nn.Linear(self.conv_channels * NUM_BINS * HEIGHT * WIDTH / 2 / 2 / 2, 128)\n",
    "\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 output dimensions (x, y)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels * HEIGHT * WIDTH, NUM_BINS))\n",
    "        x = self.pool1(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels, HEIGHT, WIDTH, -1))\n",
    "        x = self.relu(self.conv3d2(x))\n",
    "        x = self.batchnorm3d2(x)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2 * HEIGHT * WIDTH, -1))\n",
    "        x = self.pool2(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2, HEIGHT, WIDTH, -1))\n",
    "        x = self.relu(self.conv3d3(x))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLocation8Light(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLocation8Light, self).__init__()\n",
    "\n",
    "        # in: (n, HEIGHT, WIDTH, 16)\n",
    "        self.conv_channels = 4\n",
    "        self.conv_channels2 = 8\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=self.conv_channels, kernel_size=(3, 3, 7), padding=(1, 1, 3))\n",
    "        # (n, 4, HEIGHT, WIDTH, 16)\n",
    "        self.batchnorm3d = nn.BatchNorm3d(self.conv_channels)\n",
    "        self.batchnorm3d2 = nn.BatchNorm3d(self.conv_channels2)\n",
    "        # reshape to (n, 4, HEIGHT x WIDTH, 16)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 4, HEIGHT, WIDTH, 8)\n",
    "        self.pool_flat = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 4, HEIGHT / 2, WIDTH / 2, 8)\n",
    "        self.conv3d2 = nn.Conv3d(in_channels=self.conv_channels, out_channels=self.conv_channels2, kernel_size=(3, 3, 5), padding=(1, 1, 2))\n",
    "        # (n, 8, HEIGHT / 2, WIDTH / 2, 8)\n",
    "        # reshape to (n, 8, HEIGHT / 2 x WIDTH / 2, 8)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 8, HEIGHT / 2, WIDTH / 2, 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.conv_channels2 * HEIGHT * WIDTH, 128)\n",
    "        # self.fc1 = nn.Linear(self.conv_channels * NUM_BINS * HEIGHT * WIDTH / 2 / 2 / 2, 128)\n",
    "\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 output dimensions (x, y)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in: (n, HEIGHT, WIDTH, 16)\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "        # (n, 4, HEIGHT, WIDTH, 16)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels * HEIGHT * WIDTH, NUM_BINS))\n",
    "        # reshape to (n, 4 x HEIGHT x WIDTH, 16)\n",
    "        x = self.pool1(x)\n",
    "        # (n, 4 x HEIGHT x WIDTH, 8)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels, HEIGHT, WIDTH, -1))\n",
    "        # (n, 4, HEIGHT, WIDTH, 8)\n",
    "\n",
    "        x = torch.transpose(x, 2, 4)\n",
    "        # (n, 4, 8, WIDTH, HEIGHT)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels * 8, WIDTH, HEIGHT))\n",
    "        # reshape to (n, 4 x 8, WIDTH, HEIGHT)\n",
    "        x = self.pool_flat(x)\n",
    "        # reshape to (n, 4 x 8, WIDTH / 2, HEIGHT / 2)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels, 8, 4, 4))\n",
    "        # reshape to (n, 4, 8, WIDTH / 2, HEIGHT / 2)\n",
    "        x = torch.transpose(x, 2, 4)\n",
    "        # (n, 4, HEIGHT / 2, WIDTH / 2, 8)\n",
    "\n",
    "        # x = torch.reshape(x, (x.shape[0], self.conv_channels, HEIGHT, WIDTH, -1))\n",
    "        # (n, 4, HEIGHT / 2, WIDTH / 2, 8)\n",
    "        x = self.relu(self.conv3d2(x))\n",
    "        x = self.batchnorm3d2(x)\n",
    "        # (n, 8, HEIGHT / 2, WIDTH / 2, 8)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2 * HEIGHT * WIDTH, -1))\n",
    "        # reshape to (n, 8 x HEIGHT / 2 x WIDTH / 2, 8)\n",
    "        x = self.pool2(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2, HEIGHT, WIDTH, -1))\n",
    "        # (n, 8, HEIGHT / 2, WIDTH / 2, 4)\n",
    "        \n",
    "        # print(x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLocation2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLocation2, self).__init__()\n",
    "\n",
    "        # in: (n, 4, 4, 42)\n",
    "        self.conv_channels = 4\n",
    "        self.conv_channels2 = 8\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=self.conv_channels, kernel_size=(3, 3, 7), padding=(1, 1, 3))\n",
    "        # (n, 4, 4, 4, 42)\n",
    "        self.batchnorm3d = nn.BatchNorm3d(self.conv_channels)\n",
    "        self.batchnorm3d2 = nn.BatchNorm3d(self.conv_channels2)\n",
    "        # reshape to (n, 4, 4 x 4, 42)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 4, 4, 4, 21)\n",
    "        self.conv3d2 = nn.Conv3d(in_channels=self.conv_channels, out_channels=self.conv_channels2, kernel_size=(3, 3, 5), padding=(1, 1, 2))\n",
    "        # (n, 8, 4, 4, 21)\n",
    "        # reshape to (n, 8, 4 x 4, 7)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=3, padding=0)\n",
    "        # (n, 8, 4, 4, 7)\n",
    "        self.conv3d3 = nn.Conv3d(in_channels=self.conv_channels2, out_channels=self.conv_channels2, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        # (n, 8, 4, 4, 7)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.conv_channels2 * HEIGHT * WIDTH * 7, 128)\n",
    "        # self.fc1 = nn.Linear(self.conv_channels * NUM_BINS * HEIGHT * WIDTH / 2 / 2 / 2, 128)\n",
    "\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 output dimensions (x, y)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels * 4 * 4, NUM_BINS))\n",
    "        x = self.pool1(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels, 4, 4, -1))\n",
    "        x = self.relu(self.conv3d2(x))\n",
    "        x = self.batchnorm3d2(x)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2 * 4 * 4, -1))\n",
    "        x = self.pool2(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2, 4, 4, -1))\n",
    "        # print(x.shape)\n",
    "        x = self.relu(self.conv3d3(x))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLocation4Light(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLocation4Light, self).__init__()\n",
    "\n",
    "        # in: (n, HEIGHT, WIDTH, 32)\n",
    "        self.conv_channels = 4\n",
    "        self.conv_channels2 = 8\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=self.conv_channels, kernel_size=(3, 3, 7), padding=(1, 1, 3))\n",
    "        # (n, 4, HEIGHT, WIDTH, 32)\n",
    "        self.batchnorm3d = nn.BatchNorm3d(self.conv_channels)\n",
    "        self.batchnorm3d2 = nn.BatchNorm3d(self.conv_channels2)\n",
    "        # reshape to (n, 4, HEIGHT x WIDTH, 32)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 4, HEIGHT, WIDTH, 16)\n",
    "        self.pool_flat = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 4, HEIGHT / 2, WIDTH / 2, 16)\n",
    "        self.conv3d2 = nn.Conv3d(in_channels=self.conv_channels, out_channels=self.conv_channels2, kernel_size=(3, 3, 5), padding=(1, 1, 2))\n",
    "        # (n, 8, HEIGHT / 2, WIDTH / 2, 16)\n",
    "        # reshape to (n, 8, HEIGHT / 2 x WIDTH / 2, 16)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        # (n, 8, HEIGHT / 2, WIDTH / 2, 8)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.conv_channels2 * HEIGHT * WIDTH * 2, 128)\n",
    "\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 output dimensions (x, y)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in: (n, HEIGHT, WIDTH, 32)\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "        # (n, 4, HEIGHT, WIDTH, 32)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels * HEIGHT * WIDTH, NUM_BINS))\n",
    "        # reshape to (n, 4 x HEIGHT x WIDTH, 32)\n",
    "        x = self.pool1(x)\n",
    "        # (n, 4 x HEIGHT x WIDTH, 16)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels, HEIGHT, WIDTH, -1))\n",
    "        # (n, 4, HEIGHT, WIDTH, 16)\n",
    "\n",
    "        x = torch.transpose(x, 2, 4)\n",
    "        # (n, 4, 16, WIDTH, HEIGHT)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels * 16, WIDTH, HEIGHT))\n",
    "        # reshape to (n, 4 x 16, WIDTH, HEIGHT)\n",
    "        x = self.pool_flat(x)\n",
    "        # reshape to (n, 4 x 16, WIDTH / 2, HEIGHT / 2)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels, 16, 2, 2))\n",
    "        # reshape to (n, 4, 16, WIDTH / 2, HEIGHT / 2)\n",
    "        x = torch.transpose(x, 2, 4)\n",
    "        # (n, 4, HEIGHT / 2, WIDTH / 2, 16)\n",
    "\n",
    "        x = self.relu(self.conv3d2(x))\n",
    "        x = self.batchnorm3d2(x)\n",
    "        # (n, 8, HEIGHT / 2, WIDTH / 2, 16)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2 * HEIGHT * WIDTH, -1))\n",
    "        # reshape to (n, 8 x HEIGHT /2  x WIDTH /2, 16)\n",
    "        x = self.pool2(x)\n",
    "        # print(x.shape)\n",
    "        # (n, 8 x HEIGHT /2  x WIDTH /2, 8)\n",
    "        x = torch.reshape(x, (x.shape[0], self.conv_channels2, HEIGHT, WIDTH, -1))\n",
    "        # (n, 8, HEIGHT / 2, WIDTH / 2, 8)\n",
    "        \n",
    "        # print(x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepLocation8(\n",
      "  (conv3d): Conv3d(1, 4, kernel_size=(3, 3, 7), stride=(1, 1, 1), padding=(1, 1, 3))\n",
      "  (batchnorm3d): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3d2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3d2): Conv3d(4, 8, kernel_size=(3, 3, 5), stride=(1, 1, 1), padding=(1, 1, 2))\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc1_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.7, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = DeepLocation8().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 264514\n",
      "Total number of trainable parameters: 264514\n"
     ]
    }
   ],
   "source": [
    "# Calculate total number of parameters\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# Calculate total number of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLocation8(\n",
       "  (conv3d): Conv3d(1, 4, kernel_size=(3, 3, 7), stride=(1, 1, 1), padding=(1, 1, 3))\n",
       "  (batchnorm3d): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3d2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3d2): Conv3d(4, 8, kernel_size=(3, 3, 5), stride=(1, 1, 1), padding=(1, 1, 2))\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (fc1_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv3d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MSELoss for euclidean distance to true location\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, clipping=False, debug=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        if len(X) < batch_size:\n",
    "            continue\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # print(f'pred shape: {pred.shape}, y shape: {y.shape}')\n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(f'loss: {loss.item()}')\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        if clipping:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Adjust max_norm as needed\n",
    "        \n",
    "        if debug:\n",
    "            # Inspect gradients for each layer\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:  # Only check if gradient is computed\n",
    "                    print(f\"Layer: {name} | Gradient mean: {param.grad.abs().mean().item()} | Gradient max: {param.grad.abs().max().item()}\")\n",
    "                else:\n",
    "                    print(f\"Layer: {name} has no gradient.\")\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= len(dataloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    # size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    # test_loss, correct = 0, 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            # y = y.unsqueeze_(1)\n",
    "            # print(X.shape)\n",
    "            # print(y.shape)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # final_pred = torch.round(torch.clamp(pred, min=0, max=1))\n",
    "            # final_pred = torch.round(torch.sigmoid(pred))\n",
    "            \n",
    "            # print(final_pred.shape)\n",
    "            # print(\"true\")\n",
    "            # print(y)\n",
    "            # print(\"pred\")\n",
    "            # print(final_pred)\n",
    "            # print(\"diff\")\n",
    "            # print(final_pred - y)\n",
    "            # exact_match = torch.all(final_pred == torch.round(y), dim=1)\n",
    "            # correct += torch.sum(exact_match).item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_early_stopping(train_loader, val_loader, model, loss_fn, optimizer, \n",
    "    epochs=50, early_stopping=True, patience=5, threshold=0.15, clipping=False, debug=False):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss = train(train_loader, model, loss_fn, optimizer, clipping=clipping, debug=debug)\n",
    "        val_loss = test(val_loader, model, loss_fn)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                if val_loss / best_val_loss > 1 + threshold:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {t+1}\")\n",
    "                        break\n",
    "        # print(f'patience_counter: {patience_counter}')\n",
    "\n",
    "        best_model = model.__class__().to(device)\n",
    "        best_model.load_state_dict(best_model_state)\n",
    "    return best_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 536.262024  [   32/48000]\n",
      "loss: 27.098471  [ 3232/48000]\n",
      "loss: 36.673786  [ 6432/48000]\n",
      "loss: 29.187811  [ 9632/48000]\n",
      "loss: 31.017727  [12832/48000]\n",
      "loss: 24.668493  [16032/48000]\n",
      "loss: 15.378814  [19232/48000]\n",
      "loss: 24.885490  [22432/48000]\n",
      "loss: 22.495411  [25632/48000]\n",
      "loss: 17.373211  [28832/48000]\n",
      "loss: 16.889500  [32032/48000]\n",
      "loss: 22.160856  [35232/48000]\n",
      "loss: 16.332251  [38432/48000]\n",
      "loss: 16.871567  [41632/48000]\n",
      "loss: 19.148268  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 21.277752 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 30.202023  [   32/48000]\n",
      "loss: 18.084728  [ 3232/48000]\n",
      "loss: 24.990395  [ 6432/48000]\n",
      "loss: 26.495132  [ 9632/48000]\n",
      "loss: 24.524754  [12832/48000]\n",
      "loss: 22.449631  [16032/48000]\n",
      "loss: 25.522778  [19232/48000]\n",
      "loss: 27.963928  [22432/48000]\n",
      "loss: 13.331367  [25632/48000]\n",
      "loss: 13.939528  [28832/48000]\n",
      "loss: 17.548454  [32032/48000]\n",
      "loss: 35.703030  [35232/48000]\n",
      "loss: 14.564808  [38432/48000]\n",
      "loss: 30.865002  [41632/48000]\n",
      "loss: 17.116283  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 9.961467 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 14.175428  [   32/48000]\n",
      "loss: 17.969671  [ 3232/48000]\n",
      "loss: 20.125673  [ 6432/48000]\n",
      "loss: 14.612112  [ 9632/48000]\n",
      "loss: 19.327097  [12832/48000]\n",
      "loss: 19.329435  [16032/48000]\n",
      "loss: 25.067570  [19232/48000]\n",
      "loss: 14.698644  [22432/48000]\n",
      "loss: 14.618466  [25632/48000]\n",
      "loss: 14.863817  [28832/48000]\n",
      "loss: 23.866858  [32032/48000]\n",
      "loss: 27.923105  [35232/48000]\n",
      "loss: 25.062624  [38432/48000]\n",
      "loss: 38.476028  [41632/48000]\n",
      "loss: 13.632561  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 11.514768 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 20.115458  [   32/48000]\n",
      "loss: 13.736315  [ 3232/48000]\n",
      "loss: 38.412636  [ 6432/48000]\n",
      "loss: 16.299585  [ 9632/48000]\n",
      "loss: 15.622581  [12832/48000]\n",
      "loss: 25.592323  [16032/48000]\n",
      "loss: 23.760944  [19232/48000]\n",
      "loss: 25.433601  [22432/48000]\n",
      "loss: 21.105059  [25632/48000]\n",
      "loss: 18.628033  [28832/48000]\n",
      "loss: 20.703449  [32032/48000]\n",
      "loss: 16.369869  [35232/48000]\n",
      "loss: 17.108929  [38432/48000]\n",
      "loss: 14.502993  [41632/48000]\n",
      "loss: 27.892826  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 7.572054 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 17.034807  [   32/48000]\n",
      "loss: 16.185326  [ 3232/48000]\n",
      "loss: 22.497780  [ 6432/48000]\n",
      "loss: 25.136421  [ 9632/48000]\n",
      "loss: 9.859077  [12832/48000]\n",
      "loss: 10.238087  [16032/48000]\n",
      "loss: 20.176455  [19232/48000]\n",
      "loss: 30.547119  [22432/48000]\n",
      "loss: 16.798784  [25632/48000]\n",
      "loss: 14.088024  [28832/48000]\n",
      "loss: 18.740162  [32032/48000]\n",
      "loss: 21.760281  [35232/48000]\n",
      "loss: 48.368004  [38432/48000]\n",
      "loss: 39.479675  [41632/48000]\n",
      "loss: 16.130854  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 5.490084 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 19.375393  [   32/48000]\n",
      "loss: 18.303131  [ 3232/48000]\n",
      "loss: 12.391984  [ 6432/48000]\n",
      "loss: 9.919376  [ 9632/48000]\n",
      "loss: 31.254805  [12832/48000]\n",
      "loss: 18.026043  [16032/48000]\n",
      "loss: 18.667740  [19232/48000]\n",
      "loss: 12.479166  [22432/48000]\n",
      "loss: 13.843014  [25632/48000]\n",
      "loss: 18.843252  [28832/48000]\n",
      "loss: 23.311371  [32032/48000]\n",
      "loss: 13.059983  [35232/48000]\n",
      "loss: 17.375237  [38432/48000]\n",
      "loss: 22.039074  [41632/48000]\n",
      "loss: 22.674974  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.411150 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 37.972107  [   32/48000]\n",
      "loss: 20.704256  [ 3232/48000]\n",
      "loss: 13.216362  [ 6432/48000]\n",
      "loss: 11.493054  [ 9632/48000]\n",
      "loss: 19.676167  [12832/48000]\n",
      "loss: 22.165205  [16032/48000]\n",
      "loss: 30.297722  [19232/48000]\n",
      "loss: 17.844940  [22432/48000]\n",
      "loss: 14.193975  [25632/48000]\n",
      "loss: 10.800991  [28832/48000]\n",
      "loss: 14.593200  [32032/48000]\n",
      "loss: 13.078108  [35232/48000]\n",
      "loss: 11.298496  [38432/48000]\n",
      "loss: 31.052902  [41632/48000]\n",
      "loss: 24.980177  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.083610 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 18.648146  [   32/48000]\n",
      "loss: 24.792210  [ 3232/48000]\n",
      "loss: 16.384472  [ 6432/48000]\n",
      "loss: 21.723122  [ 9632/48000]\n",
      "loss: 13.411156  [12832/48000]\n",
      "loss: 13.341224  [16032/48000]\n",
      "loss: 11.005308  [19232/48000]\n",
      "loss: 14.453931  [22432/48000]\n",
      "loss: 7.534626  [25632/48000]\n",
      "loss: 10.908044  [28832/48000]\n",
      "loss: 16.906219  [32032/48000]\n",
      "loss: 8.669483  [35232/48000]\n",
      "loss: 17.855429  [38432/48000]\n",
      "loss: 19.013144  [41632/48000]\n",
      "loss: 53.275780  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.948546 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 9.050726  [   32/48000]\n",
      "loss: 21.204428  [ 3232/48000]\n",
      "loss: 14.788699  [ 6432/48000]\n",
      "loss: 14.821943  [ 9632/48000]\n",
      "loss: 14.342675  [12832/48000]\n",
      "loss: 20.077747  [16032/48000]\n",
      "loss: 14.835476  [19232/48000]\n",
      "loss: 19.135704  [22432/48000]\n",
      "loss: 15.686570  [25632/48000]\n",
      "loss: 11.384168  [28832/48000]\n",
      "loss: 12.697018  [32032/48000]\n",
      "loss: 15.795174  [35232/48000]\n",
      "loss: 16.081055  [38432/48000]\n",
      "loss: 23.836718  [41632/48000]\n",
      "loss: 12.032999  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.683569 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 48.649639  [   32/48000]\n",
      "loss: 12.866457  [ 3232/48000]\n",
      "loss: 13.139904  [ 6432/48000]\n",
      "loss: 11.851703  [ 9632/48000]\n",
      "loss: 30.700054  [12832/48000]\n",
      "loss: 13.244123  [16032/48000]\n",
      "loss: 29.439911  [19232/48000]\n",
      "loss: 10.706691  [22432/48000]\n",
      "loss: 7.460032  [25632/48000]\n",
      "loss: 17.510603  [28832/48000]\n",
      "loss: 10.616711  [32032/48000]\n",
      "loss: 19.411804  [35232/48000]\n",
      "loss: 12.178608  [38432/48000]\n",
      "loss: 15.626945  [41632/48000]\n",
      "loss: 11.559255  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 7.738259 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 18.157352  [   32/48000]\n",
      "loss: 18.672152  [ 3232/48000]\n",
      "loss: 11.586527  [ 6432/48000]\n",
      "loss: 11.161412  [ 9632/48000]\n",
      "loss: 16.282331  [12832/48000]\n",
      "loss: 23.252491  [16032/48000]\n",
      "loss: 13.317571  [19232/48000]\n",
      "loss: 12.684610  [22432/48000]\n",
      "loss: 6.791024  [25632/48000]\n",
      "loss: 12.759291  [28832/48000]\n",
      "loss: 20.657471  [32032/48000]\n",
      "loss: 16.773125  [35232/48000]\n",
      "loss: 10.953734  [38432/48000]\n",
      "loss: 17.490553  [41632/48000]\n",
      "loss: 18.346169  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.977006 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 9.139573  [   32/48000]\n",
      "loss: 9.469574  [ 3232/48000]\n",
      "loss: 33.452187  [ 6432/48000]\n",
      "loss: 15.291144  [ 9632/48000]\n",
      "loss: 11.205236  [12832/48000]\n",
      "loss: 14.700960  [16032/48000]\n",
      "loss: 45.269272  [19232/48000]\n",
      "loss: 10.299314  [22432/48000]\n",
      "loss: 12.111744  [25632/48000]\n",
      "loss: 14.394690  [28832/48000]\n",
      "loss: 23.283367  [32032/48000]\n",
      "loss: 12.882828  [35232/48000]\n",
      "loss: 12.605986  [38432/48000]\n",
      "loss: 15.782352  [41632/48000]\n",
      "loss: 15.534407  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.212264 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 13.375374  [   32/48000]\n",
      "loss: 21.253693  [ 3232/48000]\n",
      "loss: 21.677692  [ 6432/48000]\n",
      "loss: 10.213684  [ 9632/48000]\n",
      "loss: 15.946096  [12832/48000]\n",
      "loss: 16.439262  [16032/48000]\n",
      "loss: 12.037422  [19232/48000]\n",
      "loss: 17.221493  [22432/48000]\n",
      "loss: 16.859446  [25632/48000]\n",
      "loss: 20.978140  [28832/48000]\n",
      "loss: 13.078186  [32032/48000]\n",
      "loss: 16.355247  [35232/48000]\n",
      "loss: 19.176636  [38432/48000]\n",
      "loss: 15.722706  [41632/48000]\n",
      "loss: 19.066341  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.499401 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 12.882220  [   32/48000]\n",
      "loss: 24.828053  [ 3232/48000]\n",
      "loss: 24.762360  [ 6432/48000]\n",
      "loss: 16.306089  [ 9632/48000]\n",
      "loss: 15.382586  [12832/48000]\n",
      "loss: 24.217903  [16032/48000]\n",
      "loss: 16.447525  [19232/48000]\n",
      "loss: 12.890034  [22432/48000]\n",
      "loss: 14.903078  [25632/48000]\n",
      "loss: 16.883812  [28832/48000]\n",
      "loss: 16.957790  [32032/48000]\n",
      "loss: 14.441877  [35232/48000]\n",
      "loss: 8.828957  [38432/48000]\n",
      "loss: 9.306154  [41632/48000]\n",
      "loss: 12.199304  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.609586 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 13.166618  [   32/48000]\n",
      "loss: 16.337370  [ 3232/48000]\n",
      "loss: 15.498288  [ 6432/48000]\n",
      "loss: 9.429298  [ 9632/48000]\n",
      "loss: 18.032948  [12832/48000]\n",
      "loss: 16.051918  [16032/48000]\n",
      "loss: 20.570793  [19232/48000]\n",
      "loss: 12.448130  [22432/48000]\n",
      "loss: 27.941853  [25632/48000]\n",
      "loss: 17.060631  [28832/48000]\n",
      "loss: 11.330363  [32032/48000]\n",
      "loss: 14.135451  [35232/48000]\n",
      "loss: 15.884511  [38432/48000]\n",
      "loss: 15.616523  [41632/48000]\n",
      "loss: 19.812307  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.462146 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 13.477430  [   32/48000]\n",
      "loss: 18.787136  [ 3232/48000]\n",
      "loss: 31.903103  [ 6432/48000]\n",
      "loss: 11.110512  [ 9632/48000]\n",
      "loss: 14.828859  [12832/48000]\n",
      "loss: 11.935496  [16032/48000]\n",
      "loss: 14.246121  [19232/48000]\n",
      "loss: 11.474051  [22432/48000]\n",
      "loss: 15.490436  [25632/48000]\n",
      "loss: 9.951240  [28832/48000]\n",
      "loss: 13.785565  [32032/48000]\n",
      "loss: 16.337929  [35232/48000]\n",
      "loss: 16.678883  [38432/48000]\n",
      "loss: 11.667646  [41632/48000]\n",
      "loss: 12.232825  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.898235 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 12.448267  [   32/48000]\n",
      "loss: 10.752653  [ 3232/48000]\n",
      "loss: 10.409109  [ 6432/48000]\n",
      "loss: 18.463184  [ 9632/48000]\n",
      "loss: 9.940306  [12832/48000]\n",
      "loss: 25.444242  [16032/48000]\n",
      "loss: 15.817523  [19232/48000]\n",
      "loss: 17.745911  [22432/48000]\n",
      "loss: 19.404282  [25632/48000]\n",
      "loss: 24.088591  [28832/48000]\n",
      "loss: 13.445284  [32032/48000]\n",
      "loss: 11.718782  [35232/48000]\n",
      "loss: 12.404481  [38432/48000]\n",
      "loss: 34.468567  [41632/48000]\n",
      "loss: 16.038837  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 5.148538 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 14.885313  [   32/48000]\n",
      "loss: 29.548826  [ 3232/48000]\n",
      "loss: 14.379334  [ 6432/48000]\n",
      "loss: 20.292433  [ 9632/48000]\n",
      "loss: 17.300383  [12832/48000]\n",
      "loss: 11.425112  [16032/48000]\n",
      "loss: 29.842999  [19232/48000]\n",
      "loss: 11.085867  [22432/48000]\n",
      "loss: 24.837666  [25632/48000]\n",
      "loss: 20.440174  [28832/48000]\n",
      "loss: 15.281926  [32032/48000]\n",
      "loss: 7.912969  [35232/48000]\n",
      "loss: 16.871363  [38432/48000]\n",
      "loss: 18.429039  [41632/48000]\n",
      "loss: 9.175319  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 6.355366 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 12.504954  [   32/48000]\n",
      "loss: 12.246332  [ 3232/48000]\n",
      "loss: 19.243145  [ 6432/48000]\n",
      "loss: 24.351837  [ 9632/48000]\n",
      "loss: 44.108353  [12832/48000]\n",
      "loss: 22.156422  [16032/48000]\n",
      "loss: 12.026614  [19232/48000]\n",
      "loss: 8.303862  [22432/48000]\n",
      "loss: 12.416577  [25632/48000]\n",
      "loss: 26.338818  [28832/48000]\n",
      "loss: 12.060411  [32032/48000]\n",
      "loss: 13.285244  [35232/48000]\n",
      "loss: 20.902891  [38432/48000]\n",
      "loss: 17.896204  [41632/48000]\n",
      "loss: 10.543064  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.783033 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 11.593586  [   32/48000]\n",
      "loss: 11.437375  [ 3232/48000]\n",
      "loss: 17.237053  [ 6432/48000]\n",
      "loss: 9.301971  [ 9632/48000]\n",
      "loss: 10.777450  [12832/48000]\n",
      "loss: 17.420609  [16032/48000]\n",
      "loss: 14.165941  [19232/48000]\n",
      "loss: 25.234383  [22432/48000]\n",
      "loss: 13.052339  [25632/48000]\n",
      "loss: 23.621117  [28832/48000]\n",
      "loss: 12.248783  [32032/48000]\n",
      "loss: 13.670485  [35232/48000]\n",
      "loss: 21.581985  [38432/48000]\n",
      "loss: 37.464188  [41632/48000]\n",
      "loss: 15.505424  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.804949 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 15.798537  [   32/48000]\n",
      "loss: 8.600118  [ 3232/48000]\n",
      "loss: 7.027937  [ 6432/48000]\n",
      "loss: 8.684433  [ 9632/48000]\n",
      "loss: 18.519451  [12832/48000]\n",
      "loss: 15.791435  [16032/48000]\n",
      "loss: 15.639164  [19232/48000]\n",
      "loss: 12.663620  [22432/48000]\n",
      "loss: 9.006599  [25632/48000]\n",
      "loss: 11.830927  [28832/48000]\n",
      "loss: 10.776299  [32032/48000]\n",
      "loss: 14.767488  [35232/48000]\n",
      "loss: 20.602936  [38432/48000]\n",
      "loss: 20.989628  [41632/48000]\n",
      "loss: 8.303184  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.932590 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 14.666330  [   32/48000]\n",
      "loss: 15.336555  [ 3232/48000]\n",
      "loss: 13.454717  [ 6432/48000]\n",
      "loss: 14.145501  [ 9632/48000]\n",
      "loss: 14.731187  [12832/48000]\n",
      "loss: 14.075109  [16032/48000]\n",
      "loss: 16.568321  [19232/48000]\n",
      "loss: 14.038801  [22432/48000]\n",
      "loss: 8.877411  [25632/48000]\n",
      "loss: 15.492381  [28832/48000]\n",
      "loss: 17.455839  [32032/48000]\n",
      "loss: 12.646726  [35232/48000]\n",
      "loss: 13.332533  [38432/48000]\n",
      "loss: 20.908743  [41632/48000]\n",
      "loss: 9.851954  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 5.280617 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 11.366300  [   32/48000]\n",
      "loss: 28.586712  [ 3232/48000]\n",
      "loss: 24.108755  [ 6432/48000]\n",
      "loss: 21.072231  [ 9632/48000]\n",
      "loss: 16.598063  [12832/48000]\n",
      "loss: 11.297317  [16032/48000]\n",
      "loss: 10.333748  [19232/48000]\n",
      "loss: 23.069357  [22432/48000]\n",
      "loss: 23.737137  [25632/48000]\n",
      "loss: 19.194790  [28832/48000]\n",
      "loss: 12.533025  [32032/48000]\n",
      "loss: 12.762966  [35232/48000]\n",
      "loss: 11.487279  [38432/48000]\n",
      "loss: 9.261854  [41632/48000]\n",
      "loss: 12.095084  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.331887 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 14.805710  [   32/48000]\n",
      "loss: 15.114655  [ 3232/48000]\n",
      "loss: 8.722477  [ 6432/48000]\n",
      "loss: 8.718118  [ 9632/48000]\n",
      "loss: 26.028381  [12832/48000]\n",
      "loss: 14.539463  [16032/48000]\n",
      "loss: 12.633718  [19232/48000]\n",
      "loss: 9.501091  [22432/48000]\n",
      "loss: 15.482512  [25632/48000]\n",
      "loss: 17.619465  [28832/48000]\n",
      "loss: 17.309654  [32032/48000]\n",
      "loss: 26.447823  [35232/48000]\n",
      "loss: 23.174385  [38432/48000]\n",
      "loss: 9.892941  [41632/48000]\n",
      "loss: 11.947970  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.368759 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 13.317617  [   32/48000]\n",
      "loss: 22.738134  [ 3232/48000]\n",
      "loss: 16.413445  [ 6432/48000]\n",
      "loss: 14.144216  [ 9632/48000]\n",
      "loss: 19.961895  [12832/48000]\n",
      "loss: 9.158469  [16032/48000]\n",
      "loss: 14.527369  [19232/48000]\n",
      "loss: 16.365707  [22432/48000]\n",
      "loss: 13.792109  [25632/48000]\n",
      "loss: 14.872561  [28832/48000]\n",
      "loss: 16.811829  [32032/48000]\n",
      "loss: 13.466566  [35232/48000]\n",
      "loss: 11.966598  [38432/48000]\n",
      "loss: 19.806644  [41632/48000]\n",
      "loss: 11.071924  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.260881 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 22.880524  [   32/48000]\n",
      "loss: 13.532110  [ 3232/48000]\n",
      "loss: 11.070210  [ 6432/48000]\n",
      "loss: 17.432756  [ 9632/48000]\n",
      "loss: 14.979895  [12832/48000]\n",
      "loss: 15.435525  [16032/48000]\n",
      "loss: 11.543430  [19232/48000]\n",
      "loss: 11.438389  [22432/48000]\n",
      "loss: 14.060408  [25632/48000]\n",
      "loss: 14.769138  [28832/48000]\n",
      "loss: 17.285881  [32032/48000]\n",
      "loss: 10.931607  [35232/48000]\n",
      "loss: 8.741084  [38432/48000]\n",
      "loss: 11.046570  [41632/48000]\n",
      "loss: 11.771038  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.649026 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 11.186816  [   32/48000]\n",
      "loss: 18.478291  [ 3232/48000]\n",
      "loss: 16.981308  [ 6432/48000]\n",
      "loss: 22.028137  [ 9632/48000]\n",
      "loss: 12.239545  [12832/48000]\n",
      "loss: 15.497571  [16032/48000]\n",
      "loss: 12.117181  [19232/48000]\n",
      "loss: 22.894176  [22432/48000]\n",
      "loss: 9.325087  [25632/48000]\n",
      "loss: 12.333421  [28832/48000]\n",
      "loss: 12.158260  [32032/48000]\n",
      "loss: 18.705935  [35232/48000]\n",
      "loss: 15.219258  [38432/48000]\n",
      "loss: 11.553686  [41632/48000]\n",
      "loss: 14.288334  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.331119 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 15.857627  [   32/48000]\n",
      "loss: 28.599667  [ 3232/48000]\n",
      "loss: 20.799356  [ 6432/48000]\n",
      "loss: 13.352620  [ 9632/48000]\n",
      "loss: 16.079733  [12832/48000]\n",
      "loss: 8.650034  [16032/48000]\n",
      "loss: 8.261333  [19232/48000]\n",
      "loss: 19.271294  [22432/48000]\n",
      "loss: 10.433557  [25632/48000]\n",
      "loss: 12.875561  [28832/48000]\n",
      "loss: 41.789772  [32032/48000]\n",
      "loss: 9.669636  [35232/48000]\n",
      "loss: 19.071484  [38432/48000]\n",
      "loss: 14.340787  [41632/48000]\n",
      "loss: 18.016298  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.681966 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 17.484554  [   32/48000]\n",
      "loss: 13.862314  [ 3232/48000]\n",
      "loss: 27.235546  [ 6432/48000]\n",
      "loss: 14.883699  [ 9632/48000]\n",
      "loss: 13.469805  [12832/48000]\n",
      "loss: 12.505404  [16032/48000]\n",
      "loss: 13.009663  [19232/48000]\n",
      "loss: 11.824158  [22432/48000]\n",
      "loss: 28.526058  [25632/48000]\n",
      "loss: 24.199949  [28832/48000]\n",
      "loss: 23.202747  [32032/48000]\n",
      "loss: 12.739793  [35232/48000]\n",
      "loss: 12.621925  [38432/48000]\n",
      "loss: 18.938116  [41632/48000]\n",
      "loss: 14.699522  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.152295 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 9.601009  [   32/48000]\n",
      "loss: 26.264576  [ 3232/48000]\n",
      "loss: 10.765652  [ 6432/48000]\n",
      "loss: 23.467884  [ 9632/48000]\n",
      "loss: 16.040552  [12832/48000]\n",
      "loss: 15.300396  [16032/48000]\n",
      "loss: 12.119663  [19232/48000]\n",
      "loss: 9.827624  [22432/48000]\n",
      "loss: 10.745092  [25632/48000]\n",
      "loss: 14.790801  [28832/48000]\n",
      "loss: 27.594259  [32032/48000]\n",
      "loss: 6.946231  [35232/48000]\n",
      "loss: 13.753747  [38432/48000]\n",
      "loss: 11.232769  [41632/48000]\n",
      "loss: 21.692657  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.672390 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 11.648846  [   32/48000]\n",
      "loss: 14.735285  [ 3232/48000]\n",
      "loss: 19.313770  [ 6432/48000]\n",
      "loss: 25.019665  [ 9632/48000]\n",
      "loss: 13.774382  [12832/48000]\n",
      "loss: 13.169952  [16032/48000]\n",
      "loss: 7.573112  [19232/48000]\n",
      "loss: 25.045237  [22432/48000]\n",
      "loss: 22.435169  [25632/48000]\n",
      "loss: 9.531849  [28832/48000]\n",
      "loss: 14.307528  [32032/48000]\n",
      "loss: 10.845901  [35232/48000]\n",
      "loss: 9.571651  [38432/48000]\n",
      "loss: 10.726225  [41632/48000]\n",
      "loss: 20.549339  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.436597 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 15.586864  [   32/48000]\n",
      "loss: 18.822224  [ 3232/48000]\n",
      "loss: 28.973593  [ 6432/48000]\n",
      "loss: 16.186556  [ 9632/48000]\n",
      "loss: 12.056305  [12832/48000]\n",
      "loss: 25.765465  [16032/48000]\n",
      "loss: 11.371958  [19232/48000]\n",
      "loss: 15.143578  [22432/48000]\n",
      "loss: 10.749905  [25632/48000]\n",
      "loss: 14.607523  [28832/48000]\n",
      "loss: 11.666951  [32032/48000]\n",
      "loss: 19.901337  [35232/48000]\n",
      "loss: 32.040207  [38432/48000]\n",
      "loss: 11.465961  [41632/48000]\n",
      "loss: 50.348152  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.188886 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 14.927212  [   32/48000]\n",
      "loss: 13.194176  [ 3232/48000]\n",
      "loss: 15.165432  [ 6432/48000]\n",
      "loss: 12.916664  [ 9632/48000]\n",
      "loss: 15.618851  [12832/48000]\n",
      "loss: 19.941528  [16032/48000]\n",
      "loss: 17.803062  [19232/48000]\n",
      "loss: 11.322920  [22432/48000]\n",
      "loss: 13.314274  [25632/48000]\n",
      "loss: 8.758663  [28832/48000]\n",
      "loss: 12.636320  [32032/48000]\n",
      "loss: 11.280191  [35232/48000]\n",
      "loss: 9.438517  [38432/48000]\n",
      "loss: 16.369850  [41632/48000]\n",
      "loss: 16.912254  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.921427 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 11.237627  [   32/48000]\n",
      "loss: 13.588509  [ 3232/48000]\n",
      "loss: 17.149843  [ 6432/48000]\n",
      "loss: 22.118896  [ 9632/48000]\n",
      "loss: 14.310653  [12832/48000]\n",
      "loss: 14.414249  [16032/48000]\n",
      "loss: 12.314440  [19232/48000]\n",
      "loss: 16.411495  [22432/48000]\n",
      "loss: 13.221489  [25632/48000]\n",
      "loss: 14.500526  [28832/48000]\n",
      "loss: 11.181976  [32032/48000]\n",
      "loss: 12.800941  [35232/48000]\n",
      "loss: 8.315719  [38432/48000]\n",
      "loss: 9.125337  [41632/48000]\n",
      "loss: 12.273030  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.209656 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 8.739324  [   32/48000]\n",
      "loss: 15.370353  [ 3232/48000]\n",
      "loss: 11.056377  [ 6432/48000]\n",
      "loss: 12.139429  [ 9632/48000]\n",
      "loss: 11.668520  [12832/48000]\n",
      "loss: 12.190238  [16032/48000]\n",
      "loss: 18.410309  [19232/48000]\n",
      "loss: 14.097815  [22432/48000]\n",
      "loss: 13.170603  [25632/48000]\n",
      "loss: 21.581863  [28832/48000]\n",
      "loss: 15.738438  [32032/48000]\n",
      "loss: 13.606369  [35232/48000]\n",
      "loss: 20.902479  [38432/48000]\n",
      "loss: 9.964922  [41632/48000]\n",
      "loss: 19.099985  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.895569 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 9.985479  [   32/48000]\n",
      "loss: 17.404844  [ 3232/48000]\n",
      "loss: 9.791672  [ 6432/48000]\n",
      "loss: 15.288210  [ 9632/48000]\n",
      "loss: 20.740564  [12832/48000]\n",
      "loss: 16.035694  [16032/48000]\n",
      "loss: 26.668453  [19232/48000]\n",
      "loss: 15.607074  [22432/48000]\n",
      "loss: 16.864689  [25632/48000]\n",
      "loss: 22.515762  [28832/48000]\n",
      "loss: 10.920206  [32032/48000]\n",
      "loss: 15.379248  [35232/48000]\n",
      "loss: 13.946146  [38432/48000]\n",
      "loss: 16.643173  [41632/48000]\n",
      "loss: 10.272551  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 5.658753 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 14.178331  [   32/48000]\n",
      "loss: 16.727463  [ 3232/48000]\n",
      "loss: 20.155899  [ 6432/48000]\n",
      "loss: 10.190263  [ 9632/48000]\n",
      "loss: 10.455235  [12832/48000]\n",
      "loss: 11.411515  [16032/48000]\n",
      "loss: 20.309448  [19232/48000]\n",
      "loss: 10.636578  [22432/48000]\n",
      "loss: 16.308069  [25632/48000]\n",
      "loss: 13.705330  [28832/48000]\n",
      "loss: 23.717358  [32032/48000]\n",
      "loss: 9.524570  [35232/48000]\n",
      "loss: 12.778734  [38432/48000]\n",
      "loss: 11.779917  [41632/48000]\n",
      "loss: 13.000225  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.059042 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 17.562531  [   32/48000]\n",
      "loss: 27.508202  [ 3232/48000]\n",
      "loss: 17.091293  [ 6432/48000]\n",
      "loss: 14.740793  [ 9632/48000]\n",
      "loss: 5.719921  [12832/48000]\n",
      "loss: 26.595695  [16032/48000]\n",
      "loss: 19.279221  [19232/48000]\n",
      "loss: 11.674563  [22432/48000]\n",
      "loss: 8.608983  [25632/48000]\n",
      "loss: 18.838394  [28832/48000]\n",
      "loss: 14.190437  [32032/48000]\n",
      "loss: 15.717560  [35232/48000]\n",
      "loss: 18.902924  [38432/48000]\n",
      "loss: 11.001703  [41632/48000]\n",
      "loss: 11.464458  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.168852 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 9.989841  [   32/48000]\n",
      "loss: 12.534473  [ 3232/48000]\n",
      "loss: 15.080880  [ 6432/48000]\n",
      "loss: 8.939759  [ 9632/48000]\n",
      "loss: 15.055495  [12832/48000]\n",
      "loss: 11.430271  [16032/48000]\n",
      "loss: 7.486248  [19232/48000]\n",
      "loss: 17.647625  [22432/48000]\n",
      "loss: 10.188532  [25632/48000]\n",
      "loss: 23.932465  [28832/48000]\n",
      "loss: 22.942987  [32032/48000]\n",
      "loss: 13.178122  [35232/48000]\n",
      "loss: 12.211420  [38432/48000]\n",
      "loss: 12.856612  [41632/48000]\n",
      "loss: 20.360458  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.553654 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 17.031010  [   32/48000]\n",
      "loss: 12.720808  [ 3232/48000]\n",
      "loss: 8.422852  [ 6432/48000]\n",
      "loss: 23.583931  [ 9632/48000]\n",
      "loss: 9.960613  [12832/48000]\n",
      "loss: 8.890484  [16032/48000]\n",
      "loss: 11.419779  [19232/48000]\n",
      "loss: 10.099752  [22432/48000]\n",
      "loss: 10.569011  [25632/48000]\n",
      "loss: 18.377750  [28832/48000]\n",
      "loss: 10.769529  [32032/48000]\n",
      "loss: 16.564585  [35232/48000]\n",
      "loss: 14.818770  [38432/48000]\n",
      "loss: 6.340641  [41632/48000]\n",
      "loss: 15.445643  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.062430 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 29.074720  [   32/48000]\n",
      "loss: 10.750326  [ 3232/48000]\n",
      "loss: 11.492249  [ 6432/48000]\n",
      "loss: 10.511896  [ 9632/48000]\n",
      "loss: 20.132351  [12832/48000]\n",
      "loss: 12.638870  [16032/48000]\n",
      "loss: 19.572784  [19232/48000]\n",
      "loss: 14.809546  [22432/48000]\n",
      "loss: 9.083878  [25632/48000]\n",
      "loss: 21.640978  [28832/48000]\n",
      "loss: 17.689131  [32032/48000]\n",
      "loss: 16.015430  [35232/48000]\n",
      "loss: 13.547354  [38432/48000]\n",
      "loss: 18.644466  [41632/48000]\n",
      "loss: 12.166268  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.051792 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 14.857117  [   32/48000]\n",
      "loss: 14.996706  [ 3232/48000]\n",
      "loss: 10.228489  [ 6432/48000]\n",
      "loss: 17.545437  [ 9632/48000]\n",
      "loss: 8.051637  [12832/48000]\n",
      "loss: 10.175721  [16032/48000]\n",
      "loss: 11.599361  [19232/48000]\n",
      "loss: 13.321972  [22432/48000]\n",
      "loss: 19.618675  [25632/48000]\n",
      "loss: 20.327387  [28832/48000]\n",
      "loss: 11.787771  [32032/48000]\n",
      "loss: 11.882488  [35232/48000]\n",
      "loss: 9.665289  [38432/48000]\n",
      "loss: 16.308100  [41632/48000]\n",
      "loss: 22.515724  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.179706 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 13.269464  [   32/48000]\n",
      "loss: 9.708434  [ 3232/48000]\n",
      "loss: 35.538116  [ 6432/48000]\n",
      "loss: 14.855925  [ 9632/48000]\n",
      "loss: 10.655731  [12832/48000]\n",
      "loss: 15.488162  [16032/48000]\n",
      "loss: 14.646145  [19232/48000]\n",
      "loss: 15.682245  [22432/48000]\n",
      "loss: 22.312576  [25632/48000]\n",
      "loss: 15.842876  [28832/48000]\n",
      "loss: 17.771444  [32032/48000]\n",
      "loss: 17.457901  [35232/48000]\n",
      "loss: 10.402901  [38432/48000]\n",
      "loss: 16.367054  [41632/48000]\n",
      "loss: 36.529541  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 5.011516 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 22.210197  [   32/48000]\n",
      "loss: 10.623893  [ 3232/48000]\n",
      "loss: 9.351412  [ 6432/48000]\n",
      "loss: 21.675488  [ 9632/48000]\n",
      "loss: 8.053376  [12832/48000]\n",
      "loss: 9.026476  [16032/48000]\n",
      "loss: 17.364613  [19232/48000]\n",
      "loss: 22.443214  [22432/48000]\n",
      "loss: 17.426647  [25632/48000]\n",
      "loss: 11.773438  [28832/48000]\n",
      "loss: 13.072564  [32032/48000]\n",
      "loss: 19.622688  [35232/48000]\n",
      "loss: 10.400411  [38432/48000]\n",
      "loss: 28.188622  [41632/48000]\n",
      "loss: 16.150110  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.717786 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 38.641281  [   32/48000]\n",
      "loss: 14.009615  [ 3232/48000]\n",
      "loss: 26.012928  [ 6432/48000]\n",
      "loss: 11.712576  [ 9632/48000]\n",
      "loss: 19.300222  [12832/48000]\n",
      "loss: 9.840393  [16032/48000]\n",
      "loss: 15.352072  [19232/48000]\n",
      "loss: 20.729879  [22432/48000]\n",
      "loss: 12.837408  [25632/48000]\n",
      "loss: 28.586384  [28832/48000]\n",
      "loss: 14.655527  [32032/48000]\n",
      "loss: 31.234196  [35232/48000]\n",
      "loss: 15.916117  [38432/48000]\n",
      "loss: 23.287910  [41632/48000]\n",
      "loss: 10.004759  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.989961 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 18.199720  [   32/48000]\n",
      "loss: 9.895965  [ 3232/48000]\n",
      "loss: 21.582144  [ 6432/48000]\n",
      "loss: 11.404523  [ 9632/48000]\n",
      "loss: 18.118481  [12832/48000]\n",
      "loss: 14.516460  [16032/48000]\n",
      "loss: 19.054729  [19232/48000]\n",
      "loss: 9.710514  [22432/48000]\n",
      "loss: 10.716507  [25632/48000]\n",
      "loss: 13.702560  [28832/48000]\n",
      "loss: 7.218266  [32032/48000]\n",
      "loss: 15.543526  [35232/48000]\n",
      "loss: 8.445354  [38432/48000]\n",
      "loss: 19.571655  [41632/48000]\n",
      "loss: 12.574743  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.503130 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 13.617144  [   32/48000]\n",
      "loss: 21.306328  [ 3232/48000]\n",
      "loss: 9.630168  [ 6432/48000]\n",
      "loss: 9.439882  [ 9632/48000]\n",
      "loss: 11.733138  [12832/48000]\n",
      "loss: 6.768678  [16032/48000]\n",
      "loss: 13.290332  [19232/48000]\n",
      "loss: 19.258808  [22432/48000]\n",
      "loss: 18.997198  [25632/48000]\n",
      "loss: 18.555466  [28832/48000]\n",
      "loss: 12.871407  [32032/48000]\n",
      "loss: 9.766905  [35232/48000]\n",
      "loss: 8.592526  [38432/48000]\n",
      "loss: 14.499154  [41632/48000]\n",
      "loss: 21.536621  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.646154 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 7.761415  [   32/48000]\n",
      "loss: 22.541878  [ 3232/48000]\n",
      "loss: 12.082502  [ 6432/48000]\n",
      "loss: 13.050050  [ 9632/48000]\n",
      "loss: 11.281094  [12832/48000]\n",
      "loss: 11.056131  [16032/48000]\n",
      "loss: 13.411354  [19232/48000]\n",
      "loss: 17.721741  [22432/48000]\n",
      "loss: 16.094221  [25632/48000]\n",
      "loss: 14.626444  [28832/48000]\n",
      "loss: 13.267351  [32032/48000]\n",
      "loss: 20.942375  [35232/48000]\n",
      "loss: 17.702469  [38432/48000]\n",
      "loss: 15.938313  [41632/48000]\n",
      "loss: 9.527421  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.275680 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 16.563974  [   32/48000]\n",
      "loss: 19.567879  [ 3232/48000]\n",
      "loss: 10.196203  [ 6432/48000]\n",
      "loss: 34.175304  [ 9632/48000]\n",
      "loss: 14.891851  [12832/48000]\n",
      "loss: 11.010670  [16032/48000]\n",
      "loss: 16.196825  [19232/48000]\n",
      "loss: 13.317664  [22432/48000]\n",
      "loss: 19.948895  [25632/48000]\n",
      "loss: 10.079243  [28832/48000]\n",
      "loss: 10.347109  [32032/48000]\n",
      "loss: 15.920776  [35232/48000]\n",
      "loss: 16.904106  [38432/48000]\n",
      "loss: 16.054415  [41632/48000]\n",
      "loss: 21.526400  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.206348 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 11.507151  [   32/48000]\n",
      "loss: 10.522709  [ 3232/48000]\n",
      "loss: 10.018840  [ 6432/48000]\n",
      "loss: 18.516708  [ 9632/48000]\n",
      "loss: 8.062195  [12832/48000]\n",
      "loss: 21.134756  [16032/48000]\n",
      "loss: 18.590305  [19232/48000]\n",
      "loss: 12.243431  [22432/48000]\n",
      "loss: 35.337765  [25632/48000]\n",
      "loss: 13.360622  [28832/48000]\n",
      "loss: 4.852742  [32032/48000]\n",
      "loss: 30.479164  [35232/48000]\n",
      "loss: 14.542217  [38432/48000]\n",
      "loss: 12.924627  [41632/48000]\n",
      "loss: 13.083775  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.495801 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 9.969657  [   32/48000]\n",
      "loss: 9.995284  [ 3232/48000]\n",
      "loss: 25.857500  [ 6432/48000]\n",
      "loss: 7.544930  [ 9632/48000]\n",
      "loss: 16.081264  [12832/48000]\n",
      "loss: 14.724906  [16032/48000]\n",
      "loss: 12.584750  [19232/48000]\n",
      "loss: 20.830532  [22432/48000]\n",
      "loss: 14.088711  [25632/48000]\n",
      "loss: 18.429688  [28832/48000]\n",
      "loss: 11.429319  [32032/48000]\n",
      "loss: 18.111406  [35232/48000]\n",
      "loss: 16.830799  [38432/48000]\n",
      "loss: 21.317564  [41632/48000]\n",
      "loss: 7.361569  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.927277 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 9.331041  [   32/48000]\n",
      "loss: 12.682423  [ 3232/48000]\n",
      "loss: 13.671492  [ 6432/48000]\n",
      "loss: 10.627678  [ 9632/48000]\n",
      "loss: 13.402977  [12832/48000]\n",
      "loss: 9.704009  [16032/48000]\n",
      "loss: 17.801430  [19232/48000]\n",
      "loss: 10.852424  [22432/48000]\n",
      "loss: 19.134808  [25632/48000]\n",
      "loss: 30.637167  [28832/48000]\n",
      "loss: 13.898462  [32032/48000]\n",
      "loss: 12.071725  [35232/48000]\n",
      "loss: 17.057055  [38432/48000]\n",
      "loss: 14.843184  [41632/48000]\n",
      "loss: 17.274990  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.440809 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 7.175340  [   32/48000]\n",
      "loss: 7.572195  [ 3232/48000]\n",
      "loss: 16.626511  [ 6432/48000]\n",
      "loss: 14.578739  [ 9632/48000]\n",
      "loss: 22.822870  [12832/48000]\n",
      "loss: 12.796799  [16032/48000]\n",
      "loss: 9.785198  [19232/48000]\n",
      "loss: 9.520430  [22432/48000]\n",
      "loss: 17.511116  [25632/48000]\n",
      "loss: 14.798128  [28832/48000]\n",
      "loss: 12.306095  [32032/48000]\n",
      "loss: 9.836434  [35232/48000]\n",
      "loss: 10.018446  [38432/48000]\n",
      "loss: 10.096498  [41632/48000]\n",
      "loss: 12.752297  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.845033 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 18.512039  [   32/48000]\n",
      "loss: 12.093435  [ 3232/48000]\n",
      "loss: 15.190445  [ 6432/48000]\n",
      "loss: 11.003679  [ 9632/48000]\n",
      "loss: 15.015025  [12832/48000]\n",
      "loss: 29.801788  [16032/48000]\n",
      "loss: 9.544405  [19232/48000]\n",
      "loss: 26.117134  [22432/48000]\n",
      "loss: 16.722364  [25632/48000]\n",
      "loss: 12.276457  [28832/48000]\n",
      "loss: 12.437568  [32032/48000]\n",
      "loss: 17.033840  [35232/48000]\n",
      "loss: 9.598278  [38432/48000]\n",
      "loss: 15.864831  [41632/48000]\n",
      "loss: 12.627715  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.644825 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 10.149698  [   32/48000]\n",
      "loss: 13.968230  [ 3232/48000]\n",
      "loss: 9.361464  [ 6432/48000]\n",
      "loss: 20.058226  [ 9632/48000]\n",
      "loss: 10.915936  [12832/48000]\n",
      "loss: 10.594508  [16032/48000]\n",
      "loss: 9.773846  [19232/48000]\n",
      "loss: 23.376808  [22432/48000]\n",
      "loss: 17.447140  [25632/48000]\n",
      "loss: 10.844112  [28832/48000]\n",
      "loss: 22.376966  [32032/48000]\n",
      "loss: 10.585348  [35232/48000]\n",
      "loss: 10.626316  [38432/48000]\n",
      "loss: 11.842745  [41632/48000]\n",
      "loss: 9.147951  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.135315 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 13.831726  [   32/48000]\n",
      "loss: 15.070797  [ 3232/48000]\n",
      "loss: 7.034565  [ 6432/48000]\n",
      "loss: 8.281416  [ 9632/48000]\n",
      "loss: 24.968441  [12832/48000]\n",
      "loss: 15.290707  [16032/48000]\n",
      "loss: 14.919609  [19232/48000]\n",
      "loss: 12.613491  [22432/48000]\n",
      "loss: 20.800407  [25632/48000]\n",
      "loss: 8.467926  [28832/48000]\n",
      "loss: 15.714626  [32032/48000]\n",
      "loss: 10.134042  [35232/48000]\n",
      "loss: 11.952631  [38432/48000]\n",
      "loss: 9.351334  [41632/48000]\n",
      "loss: 11.867558  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.213824 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 11.873276  [   32/48000]\n",
      "loss: 11.475496  [ 3232/48000]\n",
      "loss: 29.121920  [ 6432/48000]\n",
      "loss: 24.024866  [ 9632/48000]\n",
      "loss: 7.942931  [12832/48000]\n",
      "loss: 28.276716  [16032/48000]\n",
      "loss: 10.960815  [19232/48000]\n",
      "loss: 16.654091  [22432/48000]\n",
      "loss: 13.885532  [25632/48000]\n",
      "loss: 12.125439  [28832/48000]\n",
      "loss: 9.961070  [32032/48000]\n",
      "loss: 9.065254  [35232/48000]\n",
      "loss: 8.816921  [38432/48000]\n",
      "loss: 53.819626  [41632/48000]\n",
      "loss: 20.808292  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.117432 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 8.707930  [   32/48000]\n",
      "loss: 12.573189  [ 3232/48000]\n",
      "loss: 18.396774  [ 6432/48000]\n",
      "loss: 25.509645  [ 9632/48000]\n",
      "loss: 18.191288  [12832/48000]\n",
      "loss: 9.700268  [16032/48000]\n",
      "loss: 12.172490  [19232/48000]\n",
      "loss: 8.546592  [22432/48000]\n",
      "loss: 14.287348  [25632/48000]\n",
      "loss: 12.308250  [28832/48000]\n",
      "loss: 22.715689  [32032/48000]\n",
      "loss: 13.113493  [35232/48000]\n",
      "loss: 12.469076  [38432/48000]\n",
      "loss: 35.815712  [41632/48000]\n",
      "loss: 10.360561  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.388431 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 9.547139  [   32/48000]\n",
      "loss: 13.173843  [ 3232/48000]\n",
      "loss: 16.597775  [ 6432/48000]\n",
      "loss: 17.445385  [ 9632/48000]\n",
      "loss: 14.100313  [12832/48000]\n",
      "loss: 22.476461  [16032/48000]\n",
      "loss: 22.595049  [19232/48000]\n",
      "loss: 13.898495  [22432/48000]\n",
      "loss: 27.727329  [25632/48000]\n",
      "loss: 13.714945  [28832/48000]\n",
      "loss: 18.963720  [32032/48000]\n",
      "loss: 17.182613  [35232/48000]\n",
      "loss: 11.175652  [38432/48000]\n",
      "loss: 10.860857  [41632/48000]\n",
      "loss: 15.218096  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.471353 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 8.547892  [   32/48000]\n",
      "loss: 15.889282  [ 3232/48000]\n",
      "loss: 15.737402  [ 6432/48000]\n",
      "loss: 9.896031  [ 9632/48000]\n",
      "loss: 9.235299  [12832/48000]\n",
      "loss: 11.796782  [16032/48000]\n",
      "loss: 6.739728  [19232/48000]\n",
      "loss: 7.331820  [22432/48000]\n",
      "loss: 11.591185  [25632/48000]\n",
      "loss: 12.845983  [28832/48000]\n",
      "loss: 14.445515  [32032/48000]\n",
      "loss: 13.828684  [35232/48000]\n",
      "loss: 25.087059  [38432/48000]\n",
      "loss: 20.151918  [41632/48000]\n",
      "loss: 30.372234  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.688062 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 10.625689  [   32/48000]\n",
      "loss: 8.660981  [ 3232/48000]\n",
      "loss: 17.299225  [ 6432/48000]\n",
      "loss: 7.900115  [ 9632/48000]\n",
      "loss: 15.216885  [12832/48000]\n",
      "loss: 14.127993  [16032/48000]\n",
      "loss: 19.780933  [19232/48000]\n",
      "loss: 9.779286  [22432/48000]\n",
      "loss: 11.116035  [25632/48000]\n",
      "loss: 9.471265  [28832/48000]\n",
      "loss: 13.887266  [32032/48000]\n",
      "loss: 15.367422  [35232/48000]\n",
      "loss: 11.262678  [38432/48000]\n",
      "loss: 18.393969  [41632/48000]\n",
      "loss: 9.239014  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.058545 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 23.522266  [   32/48000]\n",
      "loss: 9.081215  [ 3232/48000]\n",
      "loss: 10.508007  [ 6432/48000]\n",
      "loss: 13.845263  [ 9632/48000]\n",
      "loss: 17.217308  [12832/48000]\n",
      "loss: 10.787651  [16032/48000]\n",
      "loss: 13.797867  [19232/48000]\n",
      "loss: 17.255795  [22432/48000]\n",
      "loss: 12.650049  [25632/48000]\n",
      "loss: 12.430027  [28832/48000]\n",
      "loss: 13.858862  [32032/48000]\n",
      "loss: 11.618393  [35232/48000]\n",
      "loss: 8.858690  [38432/48000]\n",
      "loss: 22.852589  [41632/48000]\n",
      "loss: 12.870155  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.790879 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 13.700682  [   32/48000]\n",
      "loss: 15.130169  [ 3232/48000]\n",
      "loss: 20.272518  [ 6432/48000]\n",
      "loss: 15.408863  [ 9632/48000]\n",
      "loss: 17.184294  [12832/48000]\n",
      "loss: 11.276449  [16032/48000]\n",
      "loss: 10.265244  [19232/48000]\n",
      "loss: 10.728112  [22432/48000]\n",
      "loss: 10.845974  [25632/48000]\n",
      "loss: 13.996032  [28832/48000]\n",
      "loss: 9.206646  [32032/48000]\n",
      "loss: 10.374795  [35232/48000]\n",
      "loss: 14.235800  [38432/48000]\n",
      "loss: 33.849701  [41632/48000]\n",
      "loss: 8.710674  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.396041 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 13.716286  [   32/48000]\n",
      "loss: 9.678343  [ 3232/48000]\n",
      "loss: 20.168743  [ 6432/48000]\n",
      "loss: 7.215686  [ 9632/48000]\n",
      "loss: 22.832829  [12832/48000]\n",
      "loss: 10.586550  [16032/48000]\n",
      "loss: 12.715674  [19232/48000]\n",
      "loss: 9.125993  [22432/48000]\n",
      "loss: 8.747376  [25632/48000]\n",
      "loss: 11.892675  [28832/48000]\n",
      "loss: 8.756034  [32032/48000]\n",
      "loss: 12.537565  [35232/48000]\n",
      "loss: 12.168381  [38432/48000]\n",
      "loss: 13.244332  [41632/48000]\n",
      "loss: 14.424016  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.138817 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 10.737962  [   32/48000]\n",
      "loss: 16.766212  [ 3232/48000]\n",
      "loss: 16.771116  [ 6432/48000]\n",
      "loss: 15.791327  [ 9632/48000]\n",
      "loss: 14.203968  [12832/48000]\n",
      "loss: 13.777484  [16032/48000]\n",
      "loss: 22.154160  [19232/48000]\n",
      "loss: 19.985449  [22432/48000]\n",
      "loss: 11.987352  [25632/48000]\n",
      "loss: 12.655457  [28832/48000]\n",
      "loss: 12.972810  [32032/48000]\n",
      "loss: 14.036222  [35232/48000]\n",
      "loss: 7.544246  [38432/48000]\n",
      "loss: 29.769901  [41632/48000]\n",
      "loss: 14.559690  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.392203 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 15.153972  [   32/48000]\n",
      "loss: 24.458832  [ 3232/48000]\n",
      "loss: 16.702179  [ 6432/48000]\n",
      "loss: 10.807802  [ 9632/48000]\n",
      "loss: 8.930021  [12832/48000]\n",
      "loss: 13.123362  [16032/48000]\n",
      "loss: 17.826826  [19232/48000]\n",
      "loss: 11.589186  [22432/48000]\n",
      "loss: 10.666382  [25632/48000]\n",
      "loss: 10.555687  [28832/48000]\n",
      "loss: 13.762108  [32032/48000]\n",
      "loss: 10.153789  [35232/48000]\n",
      "loss: 22.191769  [38432/48000]\n",
      "loss: 9.815175  [41632/48000]\n",
      "loss: 10.487949  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.731418 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 10.438544  [   32/48000]\n",
      "loss: 8.361992  [ 3232/48000]\n",
      "loss: 9.488148  [ 6432/48000]\n",
      "loss: 10.403128  [ 9632/48000]\n",
      "loss: 21.799358  [12832/48000]\n",
      "loss: 23.367039  [16032/48000]\n",
      "loss: 10.617324  [19232/48000]\n",
      "loss: 11.731799  [22432/48000]\n",
      "loss: 13.822815  [25632/48000]\n",
      "loss: 15.877325  [28832/48000]\n",
      "loss: 11.737923  [32032/48000]\n",
      "loss: 12.972738  [35232/48000]\n",
      "loss: 27.565672  [38432/48000]\n",
      "loss: 12.809359  [41632/48000]\n",
      "loss: 12.561217  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 5.068236 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 15.240897  [   32/48000]\n",
      "loss: 15.517888  [ 3232/48000]\n",
      "loss: 11.737179  [ 6432/48000]\n",
      "loss: 15.661499  [ 9632/48000]\n",
      "loss: 20.464600  [12832/48000]\n",
      "loss: 18.312502  [16032/48000]\n",
      "loss: 22.347527  [19232/48000]\n",
      "loss: 14.831822  [22432/48000]\n",
      "loss: 9.035400  [25632/48000]\n",
      "loss: 15.703878  [28832/48000]\n",
      "loss: 18.813644  [32032/48000]\n",
      "loss: 25.741493  [35232/48000]\n",
      "loss: 12.726990  [38432/48000]\n",
      "loss: 14.424453  [41632/48000]\n",
      "loss: 7.879932  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.225563 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 8.418617  [   32/48000]\n",
      "loss: 10.634655  [ 3232/48000]\n",
      "loss: 18.224648  [ 6432/48000]\n",
      "loss: 15.337119  [ 9632/48000]\n",
      "loss: 24.593754  [12832/48000]\n",
      "loss: 20.364294  [16032/48000]\n",
      "loss: 30.239941  [19232/48000]\n",
      "loss: 30.438789  [22432/48000]\n",
      "loss: 9.258245  [25632/48000]\n",
      "loss: 11.522816  [28832/48000]\n",
      "loss: 16.406416  [32032/48000]\n",
      "loss: 26.198811  [35232/48000]\n",
      "loss: 7.190955  [38432/48000]\n",
      "loss: 9.551861  [41632/48000]\n",
      "loss: 25.622192  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.826732 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 10.101542  [   32/48000]\n",
      "loss: 11.977012  [ 3232/48000]\n",
      "loss: 10.549471  [ 6432/48000]\n",
      "loss: 8.299827  [ 9632/48000]\n",
      "loss: 13.123914  [12832/48000]\n",
      "loss: 13.057045  [16032/48000]\n",
      "loss: 11.491647  [19232/48000]\n",
      "loss: 16.085857  [22432/48000]\n",
      "loss: 11.935520  [25632/48000]\n",
      "loss: 14.572929  [28832/48000]\n",
      "loss: 19.711765  [32032/48000]\n",
      "loss: 12.180067  [35232/48000]\n",
      "loss: 10.552597  [38432/48000]\n",
      "loss: 10.789460  [41632/48000]\n",
      "loss: 11.649245  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.417503 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 20.924591  [   32/48000]\n",
      "loss: 20.460182  [ 3232/48000]\n",
      "loss: 10.835962  [ 6432/48000]\n",
      "loss: 9.733916  [ 9632/48000]\n",
      "loss: 11.577251  [12832/48000]\n",
      "loss: 8.317944  [16032/48000]\n",
      "loss: 12.430704  [19232/48000]\n",
      "loss: 11.516256  [22432/48000]\n",
      "loss: 14.997268  [25632/48000]\n",
      "loss: 17.045414  [28832/48000]\n",
      "loss: 21.045921  [32032/48000]\n",
      "loss: 11.240149  [35232/48000]\n",
      "loss: 14.575802  [38432/48000]\n",
      "loss: 18.942881  [41632/48000]\n",
      "loss: 16.703821  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.712900 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 31.778248  [   32/48000]\n",
      "loss: 6.963127  [ 3232/48000]\n",
      "loss: 8.379439  [ 6432/48000]\n",
      "loss: 22.335312  [ 9632/48000]\n",
      "loss: 14.066151  [12832/48000]\n",
      "loss: 31.287472  [16032/48000]\n",
      "loss: 16.943253  [19232/48000]\n",
      "loss: 13.173090  [22432/48000]\n",
      "loss: 16.224539  [25632/48000]\n",
      "loss: 9.069010  [28832/48000]\n",
      "loss: 13.970522  [32032/48000]\n",
      "loss: 15.252158  [35232/48000]\n",
      "loss: 13.653345  [38432/48000]\n",
      "loss: 15.952610  [41632/48000]\n",
      "loss: 15.054625  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.904258 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 19.080952  [   32/48000]\n",
      "loss: 22.793625  [ 3232/48000]\n",
      "loss: 12.524623  [ 6432/48000]\n",
      "loss: 8.653906  [ 9632/48000]\n",
      "loss: 8.545137  [12832/48000]\n",
      "loss: 7.850505  [16032/48000]\n",
      "loss: 10.711601  [19232/48000]\n",
      "loss: 13.321547  [22432/48000]\n",
      "loss: 7.934708  [25632/48000]\n",
      "loss: 15.098804  [28832/48000]\n",
      "loss: 15.201260  [32032/48000]\n",
      "loss: 13.532133  [35232/48000]\n",
      "loss: 16.763783  [38432/48000]\n",
      "loss: 22.093277  [41632/48000]\n",
      "loss: 10.714408  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.724756 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 19.296650  [   32/48000]\n",
      "loss: 12.432823  [ 3232/48000]\n",
      "loss: 18.250061  [ 6432/48000]\n",
      "loss: 7.517415  [ 9632/48000]\n",
      "loss: 11.744612  [12832/48000]\n",
      "loss: 14.501747  [16032/48000]\n",
      "loss: 13.173359  [19232/48000]\n",
      "loss: 21.156542  [22432/48000]\n",
      "loss: 17.209206  [25632/48000]\n",
      "loss: 15.101966  [28832/48000]\n",
      "loss: 38.423637  [32032/48000]\n",
      "loss: 9.172618  [35232/48000]\n",
      "loss: 12.428292  [38432/48000]\n",
      "loss: 29.696552  [41632/48000]\n",
      "loss: 15.116182  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.299527 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 9.159558  [   32/48000]\n",
      "loss: 16.685926  [ 3232/48000]\n",
      "loss: 16.254618  [ 6432/48000]\n",
      "loss: 11.899198  [ 9632/48000]\n",
      "loss: 14.005393  [12832/48000]\n",
      "loss: 8.118650  [16032/48000]\n",
      "loss: 15.448660  [19232/48000]\n",
      "loss: 20.531643  [22432/48000]\n",
      "loss: 8.283808  [25632/48000]\n",
      "loss: 9.896850  [28832/48000]\n",
      "loss: 19.191940  [32032/48000]\n",
      "loss: 19.280930  [35232/48000]\n",
      "loss: 9.453701  [38432/48000]\n",
      "loss: 9.156916  [41632/48000]\n",
      "loss: 12.970286  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.602646 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 11.975845  [   32/48000]\n",
      "loss: 10.007308  [ 3232/48000]\n",
      "loss: 11.862228  [ 6432/48000]\n",
      "loss: 7.200621  [ 9632/48000]\n",
      "loss: 30.009138  [12832/48000]\n",
      "loss: 10.392960  [16032/48000]\n",
      "loss: 11.731818  [19232/48000]\n",
      "loss: 11.907122  [22432/48000]\n",
      "loss: 8.272380  [25632/48000]\n",
      "loss: 15.552444  [28832/48000]\n",
      "loss: 11.469810  [32032/48000]\n",
      "loss: 17.146721  [35232/48000]\n",
      "loss: 8.577116  [38432/48000]\n",
      "loss: 12.968632  [41632/48000]\n",
      "loss: 11.770660  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.233484 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 14.670946  [   32/48000]\n",
      "loss: 17.838371  [ 3232/48000]\n",
      "loss: 23.506771  [ 6432/48000]\n",
      "loss: 11.197624  [ 9632/48000]\n",
      "loss: 14.840964  [12832/48000]\n",
      "loss: 19.804281  [16032/48000]\n",
      "loss: 17.464842  [19232/48000]\n",
      "loss: 22.695555  [22432/48000]\n",
      "loss: 13.612751  [25632/48000]\n",
      "loss: 15.048004  [28832/48000]\n",
      "loss: 22.277470  [32032/48000]\n",
      "loss: 11.415924  [35232/48000]\n",
      "loss: 13.130050  [38432/48000]\n",
      "loss: 15.778394  [41632/48000]\n",
      "loss: 15.747824  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.971597 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 11.567642  [   32/48000]\n",
      "loss: 11.540353  [ 3232/48000]\n",
      "loss: 16.391376  [ 6432/48000]\n",
      "loss: 7.888246  [ 9632/48000]\n",
      "loss: 11.498705  [12832/48000]\n",
      "loss: 10.214645  [16032/48000]\n",
      "loss: 21.966541  [19232/48000]\n",
      "loss: 20.038479  [22432/48000]\n",
      "loss: 13.281838  [25632/48000]\n",
      "loss: 27.333492  [28832/48000]\n",
      "loss: 15.580204  [32032/48000]\n",
      "loss: 17.313812  [35232/48000]\n",
      "loss: 44.514832  [38432/48000]\n",
      "loss: 26.137783  [41632/48000]\n",
      "loss: 8.441258  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.966692 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 24.707588  [   32/48000]\n",
      "loss: 10.267384  [ 3232/48000]\n",
      "loss: 27.498383  [ 6432/48000]\n",
      "loss: 7.472511  [ 9632/48000]\n",
      "loss: 9.946596  [12832/48000]\n",
      "loss: 17.588659  [16032/48000]\n",
      "loss: 21.010494  [19232/48000]\n",
      "loss: 18.912022  [22432/48000]\n",
      "loss: 9.772339  [25632/48000]\n",
      "loss: 8.735259  [28832/48000]\n",
      "loss: 12.739758  [32032/48000]\n",
      "loss: 23.298971  [35232/48000]\n",
      "loss: 11.552212  [38432/48000]\n",
      "loss: 11.915432  [41632/48000]\n",
      "loss: 10.991652  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.031670 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 14.048489  [   32/48000]\n",
      "loss: 9.513853  [ 3232/48000]\n",
      "loss: 19.514513  [ 6432/48000]\n",
      "loss: 20.550890  [ 9632/48000]\n",
      "loss: 15.883461  [12832/48000]\n",
      "loss: 8.770460  [16032/48000]\n",
      "loss: 32.148121  [19232/48000]\n",
      "loss: 19.946108  [22432/48000]\n",
      "loss: 28.279896  [25632/48000]\n",
      "loss: 9.474779  [28832/48000]\n",
      "loss: 18.824394  [32032/48000]\n",
      "loss: 10.984711  [35232/48000]\n",
      "loss: 25.681141  [38432/48000]\n",
      "loss: 13.428425  [41632/48000]\n",
      "loss: 12.141079  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.262639 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 8.733166  [   32/48000]\n",
      "loss: 8.679663  [ 3232/48000]\n",
      "loss: 9.200132  [ 6432/48000]\n",
      "loss: 10.991394  [ 9632/48000]\n",
      "loss: 17.396439  [12832/48000]\n",
      "loss: 18.549917  [16032/48000]\n",
      "loss: 13.992139  [19232/48000]\n",
      "loss: 10.888971  [22432/48000]\n",
      "loss: 21.005491  [25632/48000]\n",
      "loss: 11.098200  [28832/48000]\n",
      "loss: 11.446044  [32032/48000]\n",
      "loss: 7.895287  [35232/48000]\n",
      "loss: 23.031485  [38432/48000]\n",
      "loss: 9.090718  [41632/48000]\n",
      "loss: 11.048243  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.615507 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 13.369230  [   32/48000]\n",
      "loss: 10.427495  [ 3232/48000]\n",
      "loss: 13.060451  [ 6432/48000]\n",
      "loss: 15.578322  [ 9632/48000]\n",
      "loss: 12.372072  [12832/48000]\n",
      "loss: 14.156460  [16032/48000]\n",
      "loss: 11.555576  [19232/48000]\n",
      "loss: 10.749372  [22432/48000]\n",
      "loss: 13.411179  [25632/48000]\n",
      "loss: 15.196798  [28832/48000]\n",
      "loss: 10.223095  [32032/48000]\n",
      "loss: 21.075354  [35232/48000]\n",
      "loss: 14.990376  [38432/48000]\n",
      "loss: 23.730461  [41632/48000]\n",
      "loss: 15.367929  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.046402 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 11.431273  [   32/48000]\n",
      "loss: 22.281120  [ 3232/48000]\n",
      "loss: 12.772680  [ 6432/48000]\n",
      "loss: 13.250694  [ 9632/48000]\n",
      "loss: 11.982293  [12832/48000]\n",
      "loss: 19.026691  [16032/48000]\n",
      "loss: 9.736805  [19232/48000]\n",
      "loss: 14.180326  [22432/48000]\n",
      "loss: 17.370857  [25632/48000]\n",
      "loss: 13.893949  [28832/48000]\n",
      "loss: 6.599187  [32032/48000]\n",
      "loss: 11.375336  [35232/48000]\n",
      "loss: 17.188473  [38432/48000]\n",
      "loss: 12.237752  [41632/48000]\n",
      "loss: 11.596720  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.522363 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 11.896758  [   32/48000]\n",
      "loss: 7.921607  [ 3232/48000]\n",
      "loss: 8.894668  [ 6432/48000]\n",
      "loss: 17.068193  [ 9632/48000]\n",
      "loss: 10.797683  [12832/48000]\n",
      "loss: 10.064049  [16032/48000]\n",
      "loss: 9.840723  [19232/48000]\n",
      "loss: 10.581714  [22432/48000]\n",
      "loss: 8.705476  [25632/48000]\n",
      "loss: 11.980513  [28832/48000]\n",
      "loss: 10.208599  [32032/48000]\n",
      "loss: 17.538897  [35232/48000]\n",
      "loss: 7.699181  [38432/48000]\n",
      "loss: 14.514524  [41632/48000]\n",
      "loss: 13.017109  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.462820 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 13.820282  [   32/48000]\n",
      "loss: 19.929806  [ 3232/48000]\n",
      "loss: 12.433691  [ 6432/48000]\n",
      "loss: 11.743813  [ 9632/48000]\n",
      "loss: 8.737877  [12832/48000]\n",
      "loss: 20.564762  [16032/48000]\n",
      "loss: 13.996471  [19232/48000]\n",
      "loss: 15.401198  [22432/48000]\n",
      "loss: 11.128841  [25632/48000]\n",
      "loss: 18.079876  [28832/48000]\n",
      "loss: 11.361868  [32032/48000]\n",
      "loss: 20.673679  [35232/48000]\n",
      "loss: 9.673679  [38432/48000]\n",
      "loss: 8.830490  [41632/48000]\n",
      "loss: 11.491634  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.295145 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 26.073221  [   32/48000]\n",
      "loss: 10.040140  [ 3232/48000]\n",
      "loss: 9.755977  [ 6432/48000]\n",
      "loss: 10.648077  [ 9632/48000]\n",
      "loss: 12.167492  [12832/48000]\n",
      "loss: 21.161247  [16032/48000]\n",
      "loss: 5.761684  [19232/48000]\n",
      "loss: 13.286890  [22432/48000]\n",
      "loss: 13.771346  [25632/48000]\n",
      "loss: 11.343294  [28832/48000]\n",
      "loss: 12.140407  [32032/48000]\n",
      "loss: 15.461345  [35232/48000]\n",
      "loss: 16.524387  [38432/48000]\n",
      "loss: 14.717863  [41632/48000]\n",
      "loss: 11.810390  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.560420 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 16.037312  [   32/48000]\n",
      "loss: 14.816317  [ 3232/48000]\n",
      "loss: 9.133497  [ 6432/48000]\n",
      "loss: 12.898048  [ 9632/48000]\n",
      "loss: 11.715185  [12832/48000]\n",
      "loss: 18.202785  [16032/48000]\n",
      "loss: 13.876219  [19232/48000]\n",
      "loss: 11.055170  [22432/48000]\n",
      "loss: 12.234809  [25632/48000]\n",
      "loss: 8.334318  [28832/48000]\n",
      "loss: 9.653388  [32032/48000]\n",
      "loss: 8.647734  [35232/48000]\n",
      "loss: 15.718157  [38432/48000]\n",
      "loss: 26.236917  [41632/48000]\n",
      "loss: 15.664757  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.776572 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 10.798113  [   32/48000]\n",
      "loss: 10.354841  [ 3232/48000]\n",
      "loss: 12.007586  [ 6432/48000]\n",
      "loss: 11.207871  [ 9632/48000]\n",
      "loss: 16.308914  [12832/48000]\n",
      "loss: 12.769847  [16032/48000]\n",
      "loss: 15.306174  [19232/48000]\n",
      "loss: 29.906382  [22432/48000]\n",
      "loss: 11.813302  [25632/48000]\n",
      "loss: 20.863495  [28832/48000]\n",
      "loss: 11.249704  [32032/48000]\n",
      "loss: 21.407410  [35232/48000]\n",
      "loss: 26.757164  [38432/48000]\n",
      "loss: 12.236280  [41632/48000]\n",
      "loss: 15.299377  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.052133 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 8.703444  [   32/48000]\n",
      "loss: 16.378473  [ 3232/48000]\n",
      "loss: 11.693438  [ 6432/48000]\n",
      "loss: 7.768209  [ 9632/48000]\n",
      "loss: 9.529956  [12832/48000]\n",
      "loss: 10.860945  [16032/48000]\n",
      "loss: 10.959066  [19232/48000]\n",
      "loss: 9.865661  [22432/48000]\n",
      "loss: 36.111130  [25632/48000]\n",
      "loss: 7.972302  [28832/48000]\n",
      "loss: 18.744698  [32032/48000]\n",
      "loss: 23.082180  [35232/48000]\n",
      "loss: 10.894217  [38432/48000]\n",
      "loss: 10.686378  [41632/48000]\n",
      "loss: 16.444931  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.446057 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 24.252584  [   32/48000]\n",
      "loss: 13.683369  [ 3232/48000]\n",
      "loss: 9.761314  [ 6432/48000]\n",
      "loss: 14.068576  [ 9632/48000]\n",
      "loss: 16.436478  [12832/48000]\n",
      "loss: 25.663040  [16032/48000]\n",
      "loss: 24.233582  [19232/48000]\n",
      "loss: 7.932244  [22432/48000]\n",
      "loss: 12.193403  [25632/48000]\n",
      "loss: 10.620274  [28832/48000]\n",
      "loss: 10.041613  [32032/48000]\n",
      "loss: 11.649467  [35232/48000]\n",
      "loss: 9.765170  [38432/48000]\n",
      "loss: 16.048569  [41632/48000]\n",
      "loss: 14.518576  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.832347 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 9.634077  [   32/48000]\n",
      "loss: 7.935348  [ 3232/48000]\n",
      "loss: 12.800712  [ 6432/48000]\n",
      "loss: 16.005440  [ 9632/48000]\n",
      "loss: 15.935138  [12832/48000]\n",
      "loss: 10.226460  [16032/48000]\n",
      "loss: 8.901531  [19232/48000]\n",
      "loss: 11.302012  [22432/48000]\n",
      "loss: 14.357827  [25632/48000]\n",
      "loss: 8.321257  [28832/48000]\n",
      "loss: 17.051117  [32032/48000]\n",
      "loss: 11.450047  [35232/48000]\n",
      "loss: 20.119411  [38432/48000]\n",
      "loss: 7.748511  [41632/48000]\n",
      "loss: 11.763856  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.686845 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 13.450927  [   32/48000]\n",
      "loss: 9.568897  [ 3232/48000]\n",
      "loss: 19.852394  [ 6432/48000]\n",
      "loss: 13.599422  [ 9632/48000]\n",
      "loss: 10.481680  [12832/48000]\n",
      "loss: 14.237944  [16032/48000]\n",
      "loss: 8.548355  [19232/48000]\n",
      "loss: 15.577609  [22432/48000]\n",
      "loss: 18.995258  [25632/48000]\n",
      "loss: 13.116537  [28832/48000]\n",
      "loss: 11.015027  [32032/48000]\n",
      "loss: 9.898643  [35232/48000]\n",
      "loss: 19.417772  [38432/48000]\n",
      "loss: 9.744138  [41632/48000]\n",
      "loss: 12.012973  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.413790 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 13.699956  [   32/48000]\n",
      "loss: 21.927328  [ 3232/48000]\n",
      "loss: 20.950077  [ 6432/48000]\n",
      "loss: 8.519884  [ 9632/48000]\n",
      "loss: 11.958302  [12832/48000]\n",
      "loss: 9.176699  [16032/48000]\n",
      "loss: 12.829704  [19232/48000]\n",
      "loss: 14.699493  [22432/48000]\n",
      "loss: 12.326173  [25632/48000]\n",
      "loss: 12.241524  [28832/48000]\n",
      "loss: 17.176971  [32032/48000]\n",
      "loss: 15.771146  [35232/48000]\n",
      "loss: 9.964745  [38432/48000]\n",
      "loss: 8.181927  [41632/48000]\n",
      "loss: 13.890906  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.415186 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 16.120419  [   32/48000]\n",
      "loss: 12.416601  [ 3232/48000]\n",
      "loss: 11.615585  [ 6432/48000]\n",
      "loss: 8.983768  [ 9632/48000]\n",
      "loss: 28.520821  [12832/48000]\n",
      "loss: 24.279640  [16032/48000]\n",
      "loss: 13.385873  [19232/48000]\n",
      "loss: 20.063137  [22432/48000]\n",
      "loss: 10.874557  [25632/48000]\n",
      "loss: 8.384298  [28832/48000]\n",
      "loss: 12.837213  [32032/48000]\n",
      "loss: 18.191608  [35232/48000]\n",
      "loss: 10.485349  [38432/48000]\n",
      "loss: 16.358759  [41632/48000]\n",
      "loss: 11.861700  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.349774 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 10.101546  [   32/48000]\n",
      "loss: 22.981070  [ 3232/48000]\n",
      "loss: 10.186165  [ 6432/48000]\n",
      "loss: 23.027962  [ 9632/48000]\n",
      "loss: 21.875864  [12832/48000]\n",
      "loss: 24.155268  [16032/48000]\n",
      "loss: 19.146339  [19232/48000]\n",
      "loss: 10.229783  [22432/48000]\n",
      "loss: 10.384024  [25632/48000]\n",
      "loss: 11.477301  [28832/48000]\n",
      "loss: 23.352331  [32032/48000]\n",
      "loss: 10.059970  [35232/48000]\n",
      "loss: 13.121362  [38432/48000]\n",
      "loss: 8.089520  [41632/48000]\n",
      "loss: 9.495069  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.770162 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 8.422082  [   32/48000]\n",
      "loss: 9.457655  [ 3232/48000]\n",
      "loss: 15.301840  [ 6432/48000]\n",
      "loss: 13.111588  [ 9632/48000]\n",
      "loss: 9.870540  [12832/48000]\n",
      "loss: 11.643040  [16032/48000]\n",
      "loss: 29.153986  [19232/48000]\n",
      "loss: 24.957973  [22432/48000]\n",
      "loss: 9.708016  [25632/48000]\n",
      "loss: 32.739227  [28832/48000]\n",
      "loss: 8.470213  [32032/48000]\n",
      "loss: 15.772554  [35232/48000]\n",
      "loss: 18.273209  [38432/48000]\n",
      "loss: 16.946703  [41632/48000]\n",
      "loss: 12.377296  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.538289 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 7.320713  [   32/48000]\n",
      "loss: 16.599613  [ 3232/48000]\n",
      "loss: 24.850428  [ 6432/48000]\n",
      "loss: 10.910337  [ 9632/48000]\n",
      "loss: 14.709917  [12832/48000]\n",
      "loss: 11.642109  [16032/48000]\n",
      "loss: 10.609435  [19232/48000]\n",
      "loss: 7.681788  [22432/48000]\n",
      "loss: 13.374630  [25632/48000]\n",
      "loss: 17.183149  [28832/48000]\n",
      "loss: 13.670889  [32032/48000]\n",
      "loss: 9.427720  [35232/48000]\n",
      "loss: 11.592646  [38432/48000]\n",
      "loss: 11.121799  [41632/48000]\n",
      "loss: 10.819064  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 4.383175 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 24.404087  [   32/48000]\n",
      "loss: 11.286966  [ 3232/48000]\n",
      "loss: 21.326536  [ 6432/48000]\n",
      "loss: 12.125870  [ 9632/48000]\n",
      "loss: 12.003769  [12832/48000]\n",
      "loss: 10.535683  [16032/48000]\n",
      "loss: 11.100430  [19232/48000]\n",
      "loss: 14.865537  [22432/48000]\n",
      "loss: 11.273293  [25632/48000]\n",
      "loss: 13.836079  [28832/48000]\n",
      "loss: 11.247141  [32032/48000]\n",
      "loss: 12.253075  [35232/48000]\n",
      "loss: 22.675976  [38432/48000]\n",
      "loss: 7.676892  [41632/48000]\n",
      "loss: 9.700329  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.114065 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 25.978050  [   32/48000]\n",
      "loss: 10.318981  [ 3232/48000]\n",
      "loss: 7.641387  [ 6432/48000]\n",
      "loss: 10.763498  [ 9632/48000]\n",
      "loss: 17.503130  [12832/48000]\n",
      "loss: 11.030364  [16032/48000]\n",
      "loss: 9.439557  [19232/48000]\n",
      "loss: 17.350660  [22432/48000]\n",
      "loss: 10.067312  [25632/48000]\n",
      "loss: 9.454015  [28832/48000]\n",
      "loss: 17.850445  [32032/48000]\n",
      "loss: 12.567506  [35232/48000]\n",
      "loss: 12.505533  [38432/48000]\n",
      "loss: 9.609301  [41632/48000]\n",
      "loss: 13.425787  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 2.993640 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 20.895081  [   32/48000]\n",
      "loss: 6.596814  [ 3232/48000]\n",
      "loss: 7.210359  [ 6432/48000]\n",
      "loss: 11.511677  [ 9632/48000]\n",
      "loss: 16.616323  [12832/48000]\n",
      "loss: 10.193415  [16032/48000]\n",
      "loss: 7.361921  [19232/48000]\n",
      "loss: 10.820030  [22432/48000]\n",
      "loss: 14.654933  [25632/48000]\n",
      "loss: 12.122537  [28832/48000]\n",
      "loss: 13.455279  [32032/48000]\n",
      "loss: 14.385788  [35232/48000]\n",
      "loss: 8.766755  [38432/48000]\n",
      "loss: 6.844996  [41632/48000]\n",
      "loss: 16.762774  [44832/48000]\n",
      "Test Error: \n",
      " Avg loss: 3.451771 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model, train_losses, val_losses = train_early_stopping(\n",
    "    train_loader, val_loader, model, loss_fn, optimizer, epochs=100, early_stopping=True,\n",
    "    threshold=0.4, patience=10,\n",
    "    clipping=True)\n",
    "\n",
    "# plot training and validation losses\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '../../models/robotics/demo/demo_model_1.mdl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gwjrml9d76v_zwbyb2x6yc0r0000gn/T/ipykernel_39721/3376311315.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepLocation8().to(device)\n",
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 3.443261 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.443260994911194"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out of sample test\n",
    "model.eval()\n",
    "# test(test_loader, model, loss_fn)\n",
    "test(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([17.7640, 36.5456], device='mps:0'), Actual label: tensor([19.4444, 37.3333], device='mps:0'), Distance: 1.8559\n",
      "Prediction: tensor([18.9485, 30.4962], device='mps:0'), Actual label: tensor([19.4444, 32.6667], device='mps:0'), Distance: 2.2264\n",
      "Prediction: tensor([34.2352,  9.1067], device='mps:0'), Actual label: tensor([35.0000,  9.3333], device='mps:0'), Distance: 0.7977\n",
      "Prediction: tensor([28.7053, 18.1529], device='mps:0'), Actual label: tensor([31.1111, 18.6667], device='mps:0'), Distance: 2.4600\n",
      "Prediction: tensor([20.2127, 14.4444], device='mps:0'), Actual label: tensor([23.3333, 14.0000], device='mps:0'), Distance: 3.1522\n",
      "Prediction: tensor([ 9.0680, 19.7447], device='mps:0'), Actual label: tensor([ 7.7778, 18.6667], device='mps:0'), Distance: 1.6813\n",
      "Prediction: tensor([ 7.4840, 10.1587], device='mps:0'), Actual label: tensor([7.7778, 9.3333], device='mps:0'), Distance: 0.8761\n",
      "Prediction: tensor([1.6323e-02, 4.4839e+01], device='mps:0'), Actual label: tensor([ 0., 42.], device='mps:0'), Distance: 2.8389\n",
      "Prediction: tensor([17.0154, 44.2996], device='mps:0'), Actual label: tensor([15.5556, 42.0000], device='mps:0'), Distance: 2.7239\n",
      "Prediction: tensor([ 8.1354, 25.1166], device='mps:0'), Actual label: tensor([ 7.7778, 23.3333], device='mps:0'), Distance: 1.8188\n",
      "Prediction: tensor([23.1160, 21.7583], device='mps:0'), Actual label: tensor([27.2222, 23.3333], device='mps:0'), Distance: 4.3979\n",
      "Prediction: tensor([34.0217, -0.4358], device='mps:0'), Actual label: tensor([35.,  0.], device='mps:0'), Distance: 1.0709\n",
      "Prediction: tensor([28.8594, 23.5120], device='mps:0'), Actual label: tensor([31.1111, 23.3333], device='mps:0'), Distance: 2.2588\n",
      "Prediction: tensor([ 0.6136, 20.6418], device='mps:0'), Actual label: tensor([ 0.0000, 18.6667], device='mps:0'), Distance: 2.0682\n",
      "Prediction: tensor([17.9225, 14.8522], device='mps:0'), Actual label: tensor([19.4444, 14.0000], device='mps:0'), Distance: 1.7443\n",
      "Prediction: tensor([ 4.1438, 27.1486], device='mps:0'), Actual label: tensor([ 3.8889, 28.0000], device='mps:0'), Distance: 0.8887\n",
      "Prediction: tensor([15.2420, 19.2317], device='mps:0'), Actual label: tensor([19.4444, 18.6667], device='mps:0'), Distance: 4.2403\n",
      "Prediction: tensor([13.9590, 11.7879], device='mps:0'), Actual label: tensor([11.6667,  9.3333], device='mps:0'), Distance: 3.3585\n",
      "Prediction: tensor([6.7304, 7.9924], device='mps:0'), Actual label: tensor([7.7778, 9.3333], device='mps:0'), Distance: 1.7015\n",
      "Prediction: tensor([1.3146, 1.3360], device='mps:0'), Actual label: tensor([0., 0.], device='mps:0'), Distance: 1.8743\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # Get a single example from the test dataset\n",
    "    example_data, example_label = test_dataset[i]\n",
    "    example_label = example_label.to(device)\n",
    "\n",
    "    # Move the example data to the appropriate device\n",
    "    example_data = example_data.unsqueeze(0).to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get the model's prediction\n",
    "    with torch.no_grad():\n",
    "        example_data = example_data.to(device)\n",
    "        output = model(example_data).squeeze()\n",
    "\n",
    "    print(f'Prediction: {output}, Actual label: {example_label}, Distance: {torch.norm(output - example_label):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = \"../../datasets/robotics/localization/demo_arrow/capture_retro_mini_1.pkl\"\n",
    "dataset_test = HistogramDataset(\n",
    "    pkl_path, rolling_window=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.set_start_bin(6)\n",
    "dataset_test.set_end_bin(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 8, 8, 16])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 8, 8, 16])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_test_loader.dataset.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 3.682401 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.6824012492832385"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(external_test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_loss(y_pred, y_true):\n",
    "    return torch.mean(torch.pow(y_pred[:, 0] - y_true[:, 0], 2))\n",
    "\n",
    "def y_loss(y_pred, y_true):\n",
    "    return torch.mean(torch.pow(y_pred[:, 1] - y_true[:, 1], 2))\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return torch.mean(torch.pow(y_pred - y_true, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 5.405213 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.405213465816097"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(external_test_loader, model, x_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 1.959589 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9595888162914075"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(external_test_loader, model, y_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 3.682401 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.682401173993161"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(external_test_loader, model, mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([17.7640, 36.5456]), Actual label: tensor([19.4444, 37.3333]), Distance: 1.8559\n",
      "Prediction: tensor([18.9485, 30.4962]), Actual label: tensor([19.4444, 32.6667]), Distance: 2.2264\n",
      "Prediction: tensor([34.2352,  9.1067]), Actual label: tensor([35.0000,  9.3333]), Distance: 0.7977\n",
      "Prediction: tensor([28.7053, 18.1529]), Actual label: tensor([31.1111, 18.6667]), Distance: 2.4600\n",
      "Prediction: tensor([20.2127, 14.4444]), Actual label: tensor([23.3333, 14.0000]), Distance: 3.1522\n",
      "Prediction: tensor([ 9.0680, 19.7447]), Actual label: tensor([ 7.7778, 18.6667]), Distance: 1.6813\n",
      "Prediction: tensor([ 7.4840, 10.1587]), Actual label: tensor([7.7778, 9.3333]), Distance: 0.8761\n",
      "Prediction: tensor([1.6323e-02, 4.4839e+01]), Actual label: tensor([ 0., 42.]), Distance: 2.8389\n",
      "Prediction: tensor([17.0154, 44.2996]), Actual label: tensor([15.5556, 42.0000]), Distance: 2.7239\n",
      "Prediction: tensor([ 8.1354, 25.1166]), Actual label: tensor([ 7.7778, 23.3333]), Distance: 1.8188\n",
      "Prediction: tensor([23.1160, 21.7583]), Actual label: tensor([27.2222, 23.3333]), Distance: 4.3979\n",
      "Prediction: tensor([34.0217, -0.4358]), Actual label: tensor([35.,  0.]), Distance: 1.0709\n",
      "Prediction: tensor([28.8594, 23.5120]), Actual label: tensor([31.1111, 23.3333]), Distance: 2.2588\n",
      "Prediction: tensor([ 0.6136, 20.6418]), Actual label: tensor([ 0.0000, 18.6667]), Distance: 2.0682\n",
      "Prediction: tensor([17.9225, 14.8522]), Actual label: tensor([19.4444, 14.0000]), Distance: 1.7443\n",
      "Prediction: tensor([ 4.1438, 27.1486]), Actual label: tensor([ 3.8889, 28.0000]), Distance: 0.8887\n",
      "Prediction: tensor([15.2420, 19.2317]), Actual label: tensor([19.4444, 18.6667]), Distance: 4.2403\n",
      "Prediction: tensor([13.9590, 11.7879]), Actual label: tensor([11.6667,  9.3333]), Distance: 3.3585\n",
      "Prediction: tensor([6.7304, 7.9924]), Actual label: tensor([7.7778, 9.3333]), Distance: 1.7015\n",
      "Prediction: tensor([1.3146, 1.3360]), Actual label: tensor([0., 0.]), Distance: 1.8743\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # Get a single example from the test dataset\n",
    "    example_data, example_label = test_dataset[i]\n",
    "    example_label = example_label.to(device)\n",
    "\n",
    "    # Move the example data to the appropriate device\n",
    "    example_data = example_data.unsqueeze(0).to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get the model's prediction\n",
    "    with torch.no_grad():\n",
    "        example_data = example_data.to(device)\n",
    "        output = model(example_data).squeeze()\n",
    "\n",
    "    print(f'Prediction: {output.cpu()}, Actual label: {example_label.cpu()}, Distance: {torch.norm(output - example_label):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
