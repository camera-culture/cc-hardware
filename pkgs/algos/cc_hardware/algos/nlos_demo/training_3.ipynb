{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "height = 4\n",
    "width = 4\n",
    "depth = 24\n",
    "\n",
    "start_bin = 4\n",
    "end_bin = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all_hists_rel = np.empty((0, end_bin - start_bin, height, width))\n",
    "combined_labels = np.empty((0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1597,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hists = []\n",
    "labels = []\n",
    "zero_hists = []\n",
    "\n",
    "zone_to_label = {\n",
    "    0: [0, 0, 0],\n",
    "    1: [1, 0, 0],\n",
    "    2: [0, 1, 0],\n",
    "    3: [0, 0, 1],\n",
    "    4: [1, 1, 0],\n",
    "    5: [1, 0, 1],\n",
    "    6: [0, 1, 1],\n",
    "    7: [1, 1, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     hists = np.load(f'datasets/display-box-3/histograms_{i}.npy')\n",
    "#     all_hists.append(hists)\n",
    "\n",
    "#     if i == 0:\n",
    "#         zero_hists.append(hists)\n",
    "\n",
    "#     labels += [zone_to_label[i]] * len(hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1599,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    hists = np.load(f'datasets/display-box-11/histograms_{i}.npy')\n",
    "\n",
    "    hists = hists.reshape(-1, height, width, depth)\n",
    "    # move depth to the front\n",
    "    hists = np.moveaxis(hists, -1, 1)\n",
    "\n",
    "    # look at first 10 bins\n",
    "    # data = np.array(hists[:, :10, :, :])\n",
    "    # data = np.array(hists[:, :, :, :])\n",
    "    data = hists\n",
    "\n",
    "    # Compute the mean and standard deviation for each position (10, 4, 4) across all samples\n",
    "    mean = data.mean(axis=0)  # Shape: (10, 4, 4)\n",
    "    std = data.std(axis=0)    # Shape: (10, 4, 4)\n",
    "\n",
    "    # Compute the threshold for values being within 3 standard deviations\n",
    "    lower_bound = mean - 3 * std\n",
    "    upper_bound = mean + 3 * std\n",
    "\n",
    "    # Only consider the first n values along the 10-axis (shape: n x 4 x 4)\n",
    "    n = 4\n",
    "    data_to_check = data[:, :n, :, :]  # Shape: (4000, n, 4, 4)\n",
    "    lower_bound_check = lower_bound[:n, :, :]  # Shape: (n, 4, 4)\n",
    "    upper_bound_check = upper_bound[:n, :, :]  # Shape: (n, 4, 4)\n",
    "\n",
    "    # Identify samples where all values in the first 3 indices along the 10-axis are within bounds\n",
    "    valid_mask = np.all((data_to_check >= lower_bound_check) & (data_to_check <= upper_bound_check), axis=(1, 2, 3))\n",
    "\n",
    "    # Apply the mask to filter the samples\n",
    "    filtered_data = data[valid_mask]\n",
    "\n",
    "    hists = filtered_data\n",
    "\n",
    "    # sliding window mean smoothing\n",
    "    # Compute the sliding window mean\n",
    "    k = 5\n",
    "    sliding_mean = np.array([\n",
    "        hists[j - k + 1 : j + 1].mean(axis=0)  # Mean of the last k elements\n",
    "        for j in range(k - 1, len(hists))\n",
    "    ])\n",
    "    \n",
    "    hists = sliding_mean\n",
    "\n",
    "    # generate more data by adding gaussian noise with std of the actual data std (changes for bin/pixel)\n",
    "    std = hists.std(axis=0)\n",
    "    hists = np.repeat(hists, 5, axis=0)\n",
    "    hists += np.random.normal(0, std * 0.5, hists.shape)\n",
    "\n",
    "    if i == 0:\n",
    "        zero_hists.append(hists)\n",
    "\n",
    "    else:\n",
    "        all_hists.append(hists)\n",
    "        labels += [zone_to_label[i]] * len(hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1600,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_hists = np.concatenate(zero_hists, axis=0)\n",
    "all_hists = np.concatenate(all_hists, axis=0)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# all_hists = all_hists.reshape(-1, height, width, depth)\n",
    "# # move depth to the front\n",
    "# all_hists = np.moveaxis(all_hists, -1, 1)\n",
    "\n",
    "# zero_hists = zero_hists.reshape(-1, height, width, depth)\n",
    "# zero_hists = np.moveaxis(zero_hists, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the mean and standard deviation for each position (24, 4, 4) across all samples\n",
    "# mean = all_hists.mean(axis=0)  # Shape: (24, 4, 4)\n",
    "# std = all_hists.std(axis=0)    # Shape: (24, 4, 4)\n",
    "\n",
    "# # Compute the threshold for values being within 3 standard deviations\n",
    "# lower_bound = mean - 3 * std\n",
    "# upper_bound = mean + 3 * std\n",
    "\n",
    "# # Only consider the first n values along the depth axis (shape: n x 4 x 4)\n",
    "# n = 4\n",
    "# data_to_check = all_hists[:, :n, :, :]  # Shape: (4000, n, 4, 4)\n",
    "# lower_bound_check = lower_bound[:n, :, :]  # Shape: (n, 4, 4)\n",
    "# upper_bound_check = upper_bound[:n, :, :]  # Shape: (n, 4, 4)\n",
    "\n",
    "# # Identify samples where all values in the first 3 indices along the depth axis are within bounds\n",
    "# valid_mask = np.all((data_to_check >= lower_bound_check) & (data_to_check <= upper_bound_check), axis=(1, 2, 3))\n",
    "\n",
    "# # Apply the mask to filter the samples\n",
    "# filtered_all_hists = all_hists[valid_mask]\n",
    "# filtered_labels = labels[valid_mask]\n",
    "\n",
    "# # Check the shapes of the original and filtered arrays\n",
    "# print(f\"Original shape: {all_hists.shape}\")\n",
    "# print(f\"Filtered shape: {filtered_all_hists.shape}\")\n",
    "# all_hists = filtered_all_hists\n",
    "# labels = filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop first bounce bins (first bounce in first 2 bins)\n",
    "# crop bins that are too far and noisy\n",
    "all_hists = all_hists[:, start_bin:end_bin, :, :]\n",
    "zero_hists = zero_hists[:, start_bin:end_bin, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate more data by adding gaussian noise\n",
    "\n",
    "# all_hists = np.repeat(all_hists, 5, axis=0)\n",
    "# labels = np.repeat(labels, 5, axis=0)\n",
    "\n",
    "# add noise\n",
    "# std = 3 is good for general training?\n",
    "# all_hists += np.random.normal(0, 1, all_hists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1604,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hists = torch.tensor(all_hists, dtype=torch.float32)\n",
    "zero_hists = torch.tensor(zero_hists, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 20 random zero hists to act as the zero mean\n",
    "num_samples_to_mean = 20\n",
    "\n",
    "random_zero_mean = torch.empty((all_hists.shape[0], all_hists.shape[1], height, width))\n",
    "\n",
    "for i in range(all_hists.shape[0]):\n",
    "    indices = torch.randint(0, zero_hists.shape[0], (num_samples_to_mean,))\n",
    "    random_zero_mean[i] = zero_hists[indices].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize to 0-1 values\n",
    "# all_hist_mins = all_hists.amin(dim=(1, 2, 3), keepdim=True)\n",
    "# all_hist_maxs = all_hists.amax(dim=(1, 2, 3), keepdim=True)\n",
    "# all_hist_ranges = all_hist_maxs - all_hist_mins\n",
    "# all_hist_ranges[all_hist_ranges == 0] = 1\n",
    "# all_hists = (all_hists - all_hist_mins) / all_hist_ranges\n",
    "\n",
    "# # normalize to 0-1 values\n",
    "# random_zero_mean_mins = random_zero_mean.amin(dim=(1, 2, 3), keepdim=True)\n",
    "# random_zero_mean_maxs = random_zero_mean.amax(dim=(1, 2, 3), keepdim=True)\n",
    "# random_zero_mean_ranges = random_zero_mean_maxs - random_zero_mean_mins\n",
    "# random_zero_mean_ranges[random_zero_mean_ranges == 0] = 1\n",
    "# random_zero_mean = (random_zero_mean - random_zero_mean_mins) / random_zero_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1607,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hists_rel = all_hists - random_zero_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean all pixels\n",
    "# all_hists_rel = all_hists_rel.mean(dim=(2, 3), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_mean = np.mean(zero_hists, axis=0)\n",
    "\n",
    "# # lower bound at 0\n",
    "# # all_hists = np.maximum(all_hists, 0)\n",
    "# # zero_mean = np.maximum(zero_mean, 0)\n",
    "# all_hists_rel = all_hists - zero_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of all_hists: torch.Size([13940, 12, 4, 4])\n",
      "shape of labels: (13940, 3)\n",
      "shape of all_hists_rel: torch.Size([13940, 12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# chunk sizes\n",
    "print(f'shape of all_hists: {all_hists.shape}')\n",
    "print(f'shape of labels: {labels.shape}')\n",
    "print(f'shape of all_hists_rel: {all_hists_rel.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1611,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all_hists_rel = np.concatenate([combined_all_hists_rel, all_hists_rel], axis=0)\n",
    "combined_labels = np.concatenate([combined_labels, labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of all_hists_rel: (85105, 12, 4, 4)\n",
      "shape of labels: (85105, 3)\n"
     ]
    }
   ],
   "source": [
    "# final dataset sizes\n",
    "print(f'shape of all_hists_rel: {combined_all_hists_rel.shape}')\n",
    "print(f'shape of labels: {combined_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "# all_hists_tensor = torch.tensor(all_hists, dtype=torch.float32)\n",
    "# all_hists_tensor = torch.tensor(all_hists_rel, dtype=torch.float32)\n",
    "all_hists_tensor = torch.tensor(combined_all_hists_rel, dtype=torch.float32)\n",
    "# labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(combined_labels, dtype=torch.float32)\n",
    "\n",
    "# all_hists_tensor = torch.sign(all_hists_tensor) * torch.log1p(torch.abs(all_hists_tensor))\n",
    "\n",
    "\n",
    "# epsilon = 0.1  # Smoothing factor\n",
    "# labels_tensor = (1 - epsilon) * labels_tensor + epsilon * 0.5  # Smooth towards uniform distribution\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(all_hists_tensor, labels_tensor)\n",
    "\n",
    "# Define the sizes for training, validation, and test sets\n",
    "train_size = int(0.5 * len(dataset))\n",
    "val_size = int(0.25 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size],\n",
    "                                                        generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x shape: torch.Size([32, 12, 4, 4])\n",
      "batch_y shape: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_loader:\n",
    "    print(f'batch_x shape: {batch_x.shape}')\n",
    "    print(f'batch_y shape: {batch_y.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CounterCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CounterCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=(end_bin - start_bin), out_channels=16, kernel_size=3, padding=1)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "#         # self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "#         self.fc1_bn = nn.BatchNorm1d(128)\n",
    "#         self.fc2 = nn.Linear(128, 3)  # Assuming 10 classes for the labels\n",
    "#         self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "#         self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(f'x shape at start: {x.shape}')\n",
    "#         x = self.relu(self.conv1(x))\n",
    "#         # print(f'x shape after conv1: {x.shape}')\n",
    "#         x = self.batchnorm1(x)\n",
    "#         # x = self.pool(x)\n",
    "#         # print(f'x shape after pool1: {x.shape}')\n",
    "#         x = self.relu(self.conv2(x))\n",
    "#         # print(f'x shape after conv2: {x.shape}')\n",
    "#         x = self.batchnorm2(x)\n",
    "#         # x = self.pool(x)\n",
    "#         # print(f'x shape after pool2: {x.shape}')\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         # print(f'x shape after flatten: {x.shape}')\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc1_bn(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = CounterCNN().to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CounterCNN(\n",
      "  (conv3d): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (batchnorm3d): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
      "  (fc1_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.7, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CounterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CounterCNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(in_channels=(end_bin - start_bin), out_channels=16, kernel_size=3, padding=1)\n",
    "        # self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=1)\n",
    "        # self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        out_channels = 4\n",
    "        self.conv3d = nn.Conv3d(in_channels=1, out_channels=out_channels, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.batchnorm3d = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels * (end_bin - start_bin) * height * width, 128)\n",
    "\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # Assuming 10 classes for the labels\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # # print(f'x shape at start: {x.shape}')\n",
    "        # x = self.relu(self.conv1(x))\n",
    "        # # print(f'x shape after conv1: {x.shape}')\n",
    "        # x = self.batchnorm1(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # print(f'x shape after pool1: {x.shape}')\n",
    "        # x = self.relu(self.conv2(x))\n",
    "        # # print(f'x shape after conv2: {x.shape}')\n",
    "        # x = self.batchnorm2(x)\n",
    "\n",
    "        x = self.relu(self.conv3d(x.unsqueeze(1)))\n",
    "        x = self.batchnorm3d(x)\n",
    "\n",
    "        # x = self.pool(x)\n",
    "        # print(f'x shape after pool2: {x.shape}')\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(f'x shape after flatten: {x.shape}')\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CounterCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CounterCNN(\n",
       "  (conv3d): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (batchnorm3d): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (fc1_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1620,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, counter=False, clipping=False, debug=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        if len(X) < batch_size:\n",
    "            continue\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # if counter:\n",
    "            # y = y.unsqueeze(1)\n",
    "        # print(f'pred shape: {pred.shape}, y shape: {y.shape}')\n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(f'loss: {loss.item()}')\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        if clipping:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Adjust max_norm as needed\n",
    "        \n",
    "        if debug:\n",
    "            # Inspect gradients for each layer\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:  # Only check if gradient is computed\n",
    "                    print(f\"Layer: {name} | Gradient mean: {param.grad.abs().mean().item()} | Gradient max: {param.grad.abs().max().item()}\")\n",
    "                else:\n",
    "                    print(f\"Layer: {name} has no gradient.\")\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= len(dataloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_counter(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            # y = y.unsqueeze_(1)\n",
    "            # print(X.shape)\n",
    "            # print(y.shape)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # final_pred = torch.round(torch.clamp(pred, min=0, max=1))\n",
    "            final_pred = torch.round(torch.sigmoid(pred))\n",
    "            \n",
    "            # print(final_pred.shape)\n",
    "            # print(\"true\")\n",
    "            # print(y)\n",
    "            # print(\"pred\")\n",
    "            # print(final_pred)\n",
    "            # print(\"diff\")\n",
    "            # print(final_pred - y)\n",
    "            exact_match = torch.all(final_pred == torch.round(y), dim=1)\n",
    "            correct += torch.sum(exact_match).item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_early_stopping(train_loader, val_loader, model, loss_fn, optimizer, \n",
    "    epochs=50, early_stopping=True, patience=5, threshold=0.15, counter=False, clipping=False, debug=False):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss = train(train_loader, model, loss_fn, optimizer, counter=counter, clipping=clipping, debug=debug)\n",
    "        if counter:\n",
    "            val_loss, correct = test_counter(val_loader, model, loss_fn)\n",
    "        else:\n",
    "            val_loss, correct = test(val_loader, model, loss_fn)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                if val_loss / best_val_loss > 1 + threshold:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {t+1}\")\n",
    "                        break\n",
    "        # print(f'patience_counter: {patience_counter}')\n",
    "    return best_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.148295  [   32/42552]\n",
      "loss: 0.522126  [ 3232/42552]\n",
      "loss: 0.415015  [ 6432/42552]\n",
      "loss: 0.480581  [ 9632/42552]\n",
      "loss: 0.557398  [12832/42552]\n",
      "loss: 0.538285  [16032/42552]\n",
      "loss: 0.396060  [19232/42552]\n",
      "loss: 0.586628  [22432/42552]\n",
      "loss: 0.475524  [25632/42552]\n",
      "loss: 0.276104  [28832/42552]\n",
      "loss: 0.646092  [32032/42552]\n",
      "loss: 0.450679  [35232/42552]\n",
      "loss: 0.418510  [38432/42552]\n",
      "loss: 0.482596  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 0.404847 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.544718  [   32/42552]\n",
      "loss: 0.427432  [ 3232/42552]\n",
      "loss: 0.558059  [ 6432/42552]\n",
      "loss: 0.628621  [ 9632/42552]\n",
      "loss: 0.516215  [12832/42552]\n",
      "loss: 0.485799  [16032/42552]\n",
      "loss: 0.483816  [19232/42552]\n",
      "loss: 0.494134  [22432/42552]\n",
      "loss: 0.463896  [25632/42552]\n",
      "loss: 0.497203  [28832/42552]\n",
      "loss: 0.554068  [32032/42552]\n",
      "loss: 0.439509  [35232/42552]\n",
      "loss: 0.462554  [38432/42552]\n",
      "loss: 0.305180  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 0.385199 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.456229  [   32/42552]\n",
      "loss: 0.453589  [ 3232/42552]\n",
      "loss: 0.416302  [ 6432/42552]\n",
      "loss: 0.467624  [ 9632/42552]\n",
      "loss: 0.435325  [12832/42552]\n",
      "loss: 0.412763  [16032/42552]\n",
      "loss: 0.350702  [19232/42552]\n",
      "loss: 0.493309  [22432/42552]\n",
      "loss: 0.351765  [25632/42552]\n",
      "loss: 0.472063  [28832/42552]\n",
      "loss: 0.410169  [32032/42552]\n",
      "loss: 0.372844  [35232/42552]\n",
      "loss: 0.435611  [38432/42552]\n",
      "loss: 0.473402  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 0.381019 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.441717  [   32/42552]\n",
      "loss: 0.593374  [ 3232/42552]\n",
      "loss: 0.529152  [ 6432/42552]\n",
      "loss: 0.355503  [ 9632/42552]\n",
      "loss: 0.449020  [12832/42552]\n",
      "loss: 0.420010  [16032/42552]\n",
      "loss: 0.426999  [19232/42552]\n",
      "loss: 0.323492  [22432/42552]\n",
      "loss: 0.350258  [25632/42552]\n",
      "loss: 0.484959  [28832/42552]\n",
      "loss: 0.505345  [32032/42552]\n",
      "loss: 0.333172  [35232/42552]\n",
      "loss: 0.419726  [38432/42552]\n",
      "loss: 0.393337  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 0.373305 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.380018  [   32/42552]\n",
      "loss: 0.346952  [ 3232/42552]\n",
      "loss: 0.463796  [ 6432/42552]\n",
      "loss: 0.434524  [ 9632/42552]\n",
      "loss: 0.280403  [12832/42552]\n",
      "loss: 0.293925  [16032/42552]\n",
      "loss: 0.387647  [19232/42552]\n",
      "loss: 0.410119  [22432/42552]\n",
      "loss: 0.465787  [25632/42552]\n",
      "loss: 0.486929  [28832/42552]\n",
      "loss: 0.508941  [32032/42552]\n",
      "loss: 0.444742  [35232/42552]\n",
      "loss: 0.417578  [38432/42552]\n",
      "loss: 0.374989  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 0.369235 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.392772  [   32/42552]\n",
      "loss: 0.345625  [ 3232/42552]\n",
      "loss: 0.550200  [ 6432/42552]\n",
      "loss: 0.377653  [ 9632/42552]\n",
      "loss: 0.296091  [12832/42552]\n",
      "loss: 0.468756  [16032/42552]\n",
      "loss: 0.428400  [19232/42552]\n",
      "loss: 0.287475  [22432/42552]\n",
      "loss: 0.399783  [25632/42552]\n",
      "loss: 0.382028  [28832/42552]\n",
      "loss: 0.349699  [32032/42552]\n",
      "loss: 0.352866  [35232/42552]\n",
      "loss: 0.423479  [38432/42552]\n",
      "loss: 0.411529  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 0.361613 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.284081  [   32/42552]\n",
      "loss: 0.476891  [ 3232/42552]\n",
      "loss: 0.394887  [ 6432/42552]\n",
      "loss: 0.519614  [ 9632/42552]\n",
      "loss: 0.346870  [12832/42552]\n",
      "loss: 0.439466  [16032/42552]\n",
      "loss: 0.421608  [19232/42552]\n",
      "loss: 0.463736  [22432/42552]\n",
      "loss: 0.411615  [25632/42552]\n",
      "loss: 0.239837  [28832/42552]\n",
      "loss: 0.378214  [32032/42552]\n",
      "loss: 0.344135  [35232/42552]\n",
      "loss: 0.300661  [38432/42552]\n",
      "loss: 0.429494  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 0.355543 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.455336  [   32/42552]\n",
      "loss: 0.404269  [ 3232/42552]\n",
      "loss: 0.398520  [ 6432/42552]\n",
      "loss: 0.506710  [ 9632/42552]\n",
      "loss: 0.360996  [12832/42552]\n",
      "loss: 0.319518  [16032/42552]\n",
      "loss: 0.396356  [19232/42552]\n",
      "loss: 0.512394  [22432/42552]\n",
      "loss: 0.402274  [25632/42552]\n",
      "loss: 0.489827  [28832/42552]\n",
      "loss: 0.397660  [32032/42552]\n",
      "loss: 0.455657  [35232/42552]\n",
      "loss: 0.286041  [38432/42552]\n",
      "loss: 0.464172  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.364382 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.358835  [   32/42552]\n",
      "loss: 0.366102  [ 3232/42552]\n",
      "loss: 0.365643  [ 6432/42552]\n",
      "loss: 0.451681  [ 9632/42552]\n",
      "loss: 0.339706  [12832/42552]\n",
      "loss: 0.435908  [16032/42552]\n",
      "loss: 0.385560  [19232/42552]\n",
      "loss: 0.401940  [22432/42552]\n",
      "loss: 0.569045  [25632/42552]\n",
      "loss: 0.281978  [28832/42552]\n",
      "loss: 0.472347  [32032/42552]\n",
      "loss: 0.363523  [35232/42552]\n",
      "loss: 0.476493  [38432/42552]\n",
      "loss: 0.410689  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 0.346944 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.467223  [   32/42552]\n",
      "loss: 0.463225  [ 3232/42552]\n",
      "loss: 0.295084  [ 6432/42552]\n",
      "loss: 0.242897  [ 9632/42552]\n",
      "loss: 0.356925  [12832/42552]\n",
      "loss: 0.461473  [16032/42552]\n",
      "loss: 0.461050  [19232/42552]\n",
      "loss: 0.292841  [22432/42552]\n",
      "loss: 0.359166  [25632/42552]\n",
      "loss: 0.275098  [28832/42552]\n",
      "loss: 0.418257  [32032/42552]\n",
      "loss: 0.336054  [35232/42552]\n",
      "loss: 0.408753  [38432/42552]\n",
      "loss: 0.420740  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 0.345513 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.418937  [   32/42552]\n",
      "loss: 0.546217  [ 3232/42552]\n",
      "loss: 0.454612  [ 6432/42552]\n",
      "loss: 0.324808  [ 9632/42552]\n",
      "loss: 0.304168  [12832/42552]\n",
      "loss: 0.375746  [16032/42552]\n",
      "loss: 0.474229  [19232/42552]\n",
      "loss: 0.373768  [22432/42552]\n",
      "loss: 0.529116  [25632/42552]\n",
      "loss: 0.380788  [28832/42552]\n",
      "loss: 0.523292  [32032/42552]\n",
      "loss: 0.390399  [35232/42552]\n",
      "loss: 0.465074  [38432/42552]\n",
      "loss: 0.350928  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 0.355651 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.366131  [   32/42552]\n",
      "loss: 0.321460  [ 3232/42552]\n",
      "loss: 0.344109  [ 6432/42552]\n",
      "loss: 0.401415  [ 9632/42552]\n",
      "loss: 0.382012  [12832/42552]\n",
      "loss: 0.411988  [16032/42552]\n",
      "loss: 0.382751  [19232/42552]\n",
      "loss: 0.477567  [22432/42552]\n",
      "loss: 0.416660  [25632/42552]\n",
      "loss: 0.375703  [28832/42552]\n",
      "loss: 0.300246  [32032/42552]\n",
      "loss: 0.286656  [35232/42552]\n",
      "loss: 0.440714  [38432/42552]\n",
      "loss: 0.378458  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 0.342457 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.449338  [   32/42552]\n",
      "loss: 0.289744  [ 3232/42552]\n",
      "loss: 0.366874  [ 6432/42552]\n",
      "loss: 0.436403  [ 9632/42552]\n",
      "loss: 0.392369  [12832/42552]\n",
      "loss: 0.395290  [16032/42552]\n",
      "loss: 0.320204  [19232/42552]\n",
      "loss: 0.422795  [22432/42552]\n",
      "loss: 0.388232  [25632/42552]\n",
      "loss: 0.417998  [28832/42552]\n",
      "loss: 0.419088  [32032/42552]\n",
      "loss: 0.401667  [35232/42552]\n",
      "loss: 0.308999  [38432/42552]\n",
      "loss: 0.317550  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 0.342774 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.413528  [   32/42552]\n",
      "loss: 0.228363  [ 3232/42552]\n",
      "loss: 0.378682  [ 6432/42552]\n",
      "loss: 0.445455  [ 9632/42552]\n",
      "loss: 0.321829  [12832/42552]\n",
      "loss: 0.352438  [16032/42552]\n",
      "loss: 0.416016  [19232/42552]\n",
      "loss: 0.317480  [22432/42552]\n",
      "loss: 0.373126  [25632/42552]\n",
      "loss: 0.243835  [28832/42552]\n",
      "loss: 0.325592  [32032/42552]\n",
      "loss: 0.369154  [35232/42552]\n",
      "loss: 0.343115  [38432/42552]\n",
      "loss: 0.305771  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 0.334034 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.221306  [   32/42552]\n",
      "loss: 0.264392  [ 3232/42552]\n",
      "loss: 0.599880  [ 6432/42552]\n",
      "loss: 0.335954  [ 9632/42552]\n",
      "loss: 0.373238  [12832/42552]\n",
      "loss: 0.335797  [16032/42552]\n",
      "loss: 0.403877  [19232/42552]\n",
      "loss: 0.317972  [22432/42552]\n",
      "loss: 0.329664  [25632/42552]\n",
      "loss: 0.442860  [28832/42552]\n",
      "loss: 0.342826  [32032/42552]\n",
      "loss: 0.322755  [35232/42552]\n",
      "loss: 0.336806  [38432/42552]\n",
      "loss: 0.466451  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.344650 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.266057  [   32/42552]\n",
      "loss: 0.427435  [ 3232/42552]\n",
      "loss: 0.413899  [ 6432/42552]\n",
      "loss: 0.301653  [ 9632/42552]\n",
      "loss: 0.463260  [12832/42552]\n",
      "loss: 0.280080  [16032/42552]\n",
      "loss: 0.372470  [19232/42552]\n",
      "loss: 0.298821  [22432/42552]\n",
      "loss: 0.356665  [25632/42552]\n",
      "loss: 0.400942  [28832/42552]\n",
      "loss: 0.300268  [32032/42552]\n",
      "loss: 0.395776  [35232/42552]\n",
      "loss: 0.281767  [38432/42552]\n",
      "loss: 0.272507  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.330877 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.393022  [   32/42552]\n",
      "loss: 0.301143  [ 3232/42552]\n",
      "loss: 0.335675  [ 6432/42552]\n",
      "loss: 0.403776  [ 9632/42552]\n",
      "loss: 0.317612  [12832/42552]\n",
      "loss: 0.362685  [16032/42552]\n",
      "loss: 0.479355  [19232/42552]\n",
      "loss: 0.295233  [22432/42552]\n",
      "loss: 0.358593  [25632/42552]\n",
      "loss: 0.462847  [28832/42552]\n",
      "loss: 0.397856  [32032/42552]\n",
      "loss: 0.271291  [35232/42552]\n",
      "loss: 0.324843  [38432/42552]\n",
      "loss: 0.324303  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 0.334031 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.481237  [   32/42552]\n",
      "loss: 0.510442  [ 3232/42552]\n",
      "loss: 0.362681  [ 6432/42552]\n",
      "loss: 0.455306  [ 9632/42552]\n",
      "loss: 0.256596  [12832/42552]\n",
      "loss: 0.308126  [16032/42552]\n",
      "loss: 0.514130  [19232/42552]\n",
      "loss: 0.443481  [22432/42552]\n",
      "loss: 0.368877  [25632/42552]\n",
      "loss: 0.371833  [28832/42552]\n",
      "loss: 0.278505  [32032/42552]\n",
      "loss: 0.428154  [35232/42552]\n",
      "loss: 0.276469  [38432/42552]\n",
      "loss: 0.280340  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.330298 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.349170  [   32/42552]\n",
      "loss: 0.341948  [ 3232/42552]\n",
      "loss: 0.365383  [ 6432/42552]\n",
      "loss: 0.468845  [ 9632/42552]\n",
      "loss: 0.461652  [12832/42552]\n",
      "loss: 0.357915  [16032/42552]\n",
      "loss: 0.375589  [19232/42552]\n",
      "loss: 0.333227  [22432/42552]\n",
      "loss: 0.431138  [25632/42552]\n",
      "loss: 0.479714  [28832/42552]\n",
      "loss: 0.405672  [32032/42552]\n",
      "loss: 0.297668  [35232/42552]\n",
      "loss: 0.320757  [38432/42552]\n",
      "loss: 0.349697  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 0.338358 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.412731  [   32/42552]\n",
      "loss: 0.361764  [ 3232/42552]\n",
      "loss: 0.461238  [ 6432/42552]\n",
      "loss: 0.356481  [ 9632/42552]\n",
      "loss: 0.388218  [12832/42552]\n",
      "loss: 0.375403  [16032/42552]\n",
      "loss: 0.537437  [19232/42552]\n",
      "loss: 0.436784  [22432/42552]\n",
      "loss: 0.293269  [25632/42552]\n",
      "loss: 0.231073  [28832/42552]\n",
      "loss: 0.342989  [32032/42552]\n",
      "loss: 0.267061  [35232/42552]\n",
      "loss: 0.362702  [38432/42552]\n",
      "loss: 0.384763  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 0.324326 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.443467  [   32/42552]\n",
      "loss: 0.510974  [ 3232/42552]\n",
      "loss: 0.345618  [ 6432/42552]\n",
      "loss: 0.314162  [ 9632/42552]\n",
      "loss: 0.275268  [12832/42552]\n",
      "loss: 0.483285  [16032/42552]\n",
      "loss: 0.373785  [19232/42552]\n",
      "loss: 0.439798  [22432/42552]\n",
      "loss: 0.414765  [25632/42552]\n",
      "loss: 0.446798  [28832/42552]\n",
      "loss: 0.336655  [32032/42552]\n",
      "loss: 0.308845  [35232/42552]\n",
      "loss: 0.350337  [38432/42552]\n",
      "loss: 0.312945  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 0.319468 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.399972  [   32/42552]\n",
      "loss: 0.236760  [ 3232/42552]\n",
      "loss: 0.379123  [ 6432/42552]\n",
      "loss: 0.262143  [ 9632/42552]\n",
      "loss: 0.298770  [12832/42552]\n",
      "loss: 0.394283  [16032/42552]\n",
      "loss: 0.384906  [19232/42552]\n",
      "loss: 0.549067  [22432/42552]\n",
      "loss: 0.339500  [25632/42552]\n",
      "loss: 0.381474  [28832/42552]\n",
      "loss: 0.414299  [32032/42552]\n",
      "loss: 0.453634  [35232/42552]\n",
      "loss: 0.466468  [38432/42552]\n",
      "loss: 0.360102  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.326489 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.343621  [   32/42552]\n",
      "loss: 0.355873  [ 3232/42552]\n",
      "loss: 0.177941  [ 6432/42552]\n",
      "loss: 0.298331  [ 9632/42552]\n",
      "loss: 0.253078  [12832/42552]\n",
      "loss: 0.233061  [16032/42552]\n",
      "loss: 0.367303  [19232/42552]\n",
      "loss: 0.324320  [22432/42552]\n",
      "loss: 0.260448  [25632/42552]\n",
      "loss: 0.316094  [28832/42552]\n",
      "loss: 0.378877  [32032/42552]\n",
      "loss: 0.499531  [35232/42552]\n",
      "loss: 0.390650  [38432/42552]\n",
      "loss: 0.421975  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.322280 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.261763  [   32/42552]\n",
      "loss: 0.272574  [ 3232/42552]\n",
      "loss: 0.322458  [ 6432/42552]\n",
      "loss: 0.629716  [ 9632/42552]\n",
      "loss: 0.288924  [12832/42552]\n",
      "loss: 0.284867  [16032/42552]\n",
      "loss: 0.459261  [19232/42552]\n",
      "loss: 0.328850  [22432/42552]\n",
      "loss: 0.384782  [25632/42552]\n",
      "loss: 0.407374  [28832/42552]\n",
      "loss: 0.440457  [32032/42552]\n",
      "loss: 0.338676  [35232/42552]\n",
      "loss: 0.462602  [38432/42552]\n",
      "loss: 0.405711  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 0.322201 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.319588  [   32/42552]\n",
      "loss: 0.390986  [ 3232/42552]\n",
      "loss: 0.448542  [ 6432/42552]\n",
      "loss: 0.387691  [ 9632/42552]\n",
      "loss: 0.359862  [12832/42552]\n",
      "loss: 0.365389  [16032/42552]\n",
      "loss: 0.249982  [19232/42552]\n",
      "loss: 0.325214  [22432/42552]\n",
      "loss: 0.264450  [25632/42552]\n",
      "loss: 0.266362  [28832/42552]\n",
      "loss: 0.332000  [32032/42552]\n",
      "loss: 0.353265  [35232/42552]\n",
      "loss: 0.430304  [38432/42552]\n",
      "loss: 0.351273  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.321821 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.353307  [   32/42552]\n",
      "loss: 0.433428  [ 3232/42552]\n",
      "loss: 0.349284  [ 6432/42552]\n",
      "loss: 0.259263  [ 9632/42552]\n",
      "loss: 0.403126  [12832/42552]\n",
      "loss: 0.327730  [16032/42552]\n",
      "loss: 0.269062  [19232/42552]\n",
      "loss: 0.401550  [22432/42552]\n",
      "loss: 0.413875  [25632/42552]\n",
      "loss: 0.404489  [28832/42552]\n",
      "loss: 0.335169  [32032/42552]\n",
      "loss: 0.348178  [35232/42552]\n",
      "loss: 0.281370  [38432/42552]\n",
      "loss: 0.344419  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.320982 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.331078  [   32/42552]\n",
      "loss: 0.314752  [ 3232/42552]\n",
      "loss: 0.390383  [ 6432/42552]\n",
      "loss: 0.392988  [ 9632/42552]\n",
      "loss: 0.344998  [12832/42552]\n",
      "loss: 0.417921  [16032/42552]\n",
      "loss: 0.287904  [19232/42552]\n",
      "loss: 0.247218  [22432/42552]\n",
      "loss: 0.527385  [25632/42552]\n",
      "loss: 0.305562  [28832/42552]\n",
      "loss: 0.443162  [32032/42552]\n",
      "loss: 0.287919  [35232/42552]\n",
      "loss: 0.306824  [38432/42552]\n",
      "loss: 0.412871  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.316585 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.341547  [   32/42552]\n",
      "loss: 0.372269  [ 3232/42552]\n",
      "loss: 0.585910  [ 6432/42552]\n",
      "loss: 0.297718  [ 9632/42552]\n",
      "loss: 0.252464  [12832/42552]\n",
      "loss: 0.257635  [16032/42552]\n",
      "loss: 0.418566  [19232/42552]\n",
      "loss: 0.458797  [22432/42552]\n",
      "loss: 0.355637  [25632/42552]\n",
      "loss: 0.317396  [28832/42552]\n",
      "loss: 0.273849  [32032/42552]\n",
      "loss: 0.297533  [35232/42552]\n",
      "loss: 0.274883  [38432/42552]\n",
      "loss: 0.394343  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.320231 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.386051  [   32/42552]\n",
      "loss: 0.386404  [ 3232/42552]\n",
      "loss: 0.248029  [ 6432/42552]\n",
      "loss: 0.296247  [ 9632/42552]\n",
      "loss: 0.360006  [12832/42552]\n",
      "loss: 0.447859  [16032/42552]\n",
      "loss: 0.249193  [19232/42552]\n",
      "loss: 0.399934  [22432/42552]\n",
      "loss: 0.337210  [25632/42552]\n",
      "loss: 0.403118  [28832/42552]\n",
      "loss: 0.308071  [32032/42552]\n",
      "loss: 0.499539  [35232/42552]\n",
      "loss: 0.514871  [38432/42552]\n",
      "loss: 0.430628  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.326052 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.340995  [   32/42552]\n",
      "loss: 0.325009  [ 3232/42552]\n",
      "loss: 0.363622  [ 6432/42552]\n",
      "loss: 0.375127  [ 9632/42552]\n",
      "loss: 0.291830  [12832/42552]\n",
      "loss: 0.277363  [16032/42552]\n",
      "loss: 0.580718  [19232/42552]\n",
      "loss: 0.403068  [22432/42552]\n",
      "loss: 0.429385  [25632/42552]\n",
      "loss: 0.323524  [28832/42552]\n",
      "loss: 0.268664  [32032/42552]\n",
      "loss: 0.396962  [35232/42552]\n",
      "loss: 0.362128  [38432/42552]\n",
      "loss: 0.355493  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 0.319110 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.405286  [   32/42552]\n",
      "loss: 0.443545  [ 3232/42552]\n",
      "loss: 0.317465  [ 6432/42552]\n",
      "loss: 0.405140  [ 9632/42552]\n",
      "loss: 0.307315  [12832/42552]\n",
      "loss: 0.353945  [16032/42552]\n",
      "loss: 0.289884  [19232/42552]\n",
      "loss: 0.425091  [22432/42552]\n",
      "loss: 0.257023  [25632/42552]\n",
      "loss: 0.420064  [28832/42552]\n",
      "loss: 0.333394  [32032/42552]\n",
      "loss: 0.642645  [35232/42552]\n",
      "loss: 0.345542  [38432/42552]\n",
      "loss: 0.363077  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.306622 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.304718  [   32/42552]\n",
      "loss: 0.420509  [ 3232/42552]\n",
      "loss: 0.460846  [ 6432/42552]\n",
      "loss: 0.312511  [ 9632/42552]\n",
      "loss: 0.420767  [12832/42552]\n",
      "loss: 0.248911  [16032/42552]\n",
      "loss: 0.454137  [19232/42552]\n",
      "loss: 0.355131  [22432/42552]\n",
      "loss: 0.370854  [25632/42552]\n",
      "loss: 0.470012  [28832/42552]\n",
      "loss: 0.266027  [32032/42552]\n",
      "loss: 0.222384  [35232/42552]\n",
      "loss: 0.347999  [38432/42552]\n",
      "loss: 0.386072  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.312270 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.338591  [   32/42552]\n",
      "loss: 0.373440  [ 3232/42552]\n",
      "loss: 0.413673  [ 6432/42552]\n",
      "loss: 0.360305  [ 9632/42552]\n",
      "loss: 0.312465  [12832/42552]\n",
      "loss: 0.223800  [16032/42552]\n",
      "loss: 0.354129  [19232/42552]\n",
      "loss: 0.330313  [22432/42552]\n",
      "loss: 0.233162  [25632/42552]\n",
      "loss: 0.541516  [28832/42552]\n",
      "loss: 0.300660  [32032/42552]\n",
      "loss: 0.369380  [35232/42552]\n",
      "loss: 0.284482  [38432/42552]\n",
      "loss: 0.383470  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.309164 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.348922  [   32/42552]\n",
      "loss: 0.526001  [ 3232/42552]\n",
      "loss: 0.324149  [ 6432/42552]\n",
      "loss: 0.228997  [ 9632/42552]\n",
      "loss: 0.399096  [12832/42552]\n",
      "loss: 0.330072  [16032/42552]\n",
      "loss: 0.326772  [19232/42552]\n",
      "loss: 0.318039  [22432/42552]\n",
      "loss: 0.364024  [25632/42552]\n",
      "loss: 0.388270  [28832/42552]\n",
      "loss: 0.506106  [32032/42552]\n",
      "loss: 0.418215  [35232/42552]\n",
      "loss: 0.275706  [38432/42552]\n",
      "loss: 0.484271  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.313332 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.291885  [   32/42552]\n",
      "loss: 0.339796  [ 3232/42552]\n",
      "loss: 0.336762  [ 6432/42552]\n",
      "loss: 0.340277  [ 9632/42552]\n",
      "loss: 0.261364  [12832/42552]\n",
      "loss: 0.348246  [16032/42552]\n",
      "loss: 0.432197  [19232/42552]\n",
      "loss: 0.432528  [22432/42552]\n",
      "loss: 0.360725  [25632/42552]\n",
      "loss: 0.446050  [28832/42552]\n",
      "loss: 0.326891  [32032/42552]\n",
      "loss: 0.300788  [35232/42552]\n",
      "loss: 0.306620  [38432/42552]\n",
      "loss: 0.378183  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.308672 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.482808  [   32/42552]\n",
      "loss: 0.260044  [ 3232/42552]\n",
      "loss: 0.353803  [ 6432/42552]\n",
      "loss: 0.218760  [ 9632/42552]\n",
      "loss: 0.441265  [12832/42552]\n",
      "loss: 0.401189  [16032/42552]\n",
      "loss: 0.310838  [19232/42552]\n",
      "loss: 0.252962  [22432/42552]\n",
      "loss: 0.275336  [25632/42552]\n",
      "loss: 0.349777  [28832/42552]\n",
      "loss: 0.295957  [32032/42552]\n",
      "loss: 0.244222  [35232/42552]\n",
      "loss: 0.573443  [38432/42552]\n",
      "loss: 0.307271  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.308465 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.338558  [   32/42552]\n",
      "loss: 0.207248  [ 3232/42552]\n",
      "loss: 0.314584  [ 6432/42552]\n",
      "loss: 0.283426  [ 9632/42552]\n",
      "loss: 0.237531  [12832/42552]\n",
      "loss: 0.246131  [16032/42552]\n",
      "loss: 0.341357  [19232/42552]\n",
      "loss: 0.478645  [22432/42552]\n",
      "loss: 0.219868  [25632/42552]\n",
      "loss: 0.414543  [28832/42552]\n",
      "loss: 0.240588  [32032/42552]\n",
      "loss: 0.343274  [35232/42552]\n",
      "loss: 0.264010  [38432/42552]\n",
      "loss: 0.419767  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.310584 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.523983  [   32/42552]\n",
      "loss: 0.272917  [ 3232/42552]\n",
      "loss: 0.496696  [ 6432/42552]\n",
      "loss: 0.371814  [ 9632/42552]\n",
      "loss: 0.310774  [12832/42552]\n",
      "loss: 0.385963  [16032/42552]\n",
      "loss: 0.405432  [19232/42552]\n",
      "loss: 0.370609  [22432/42552]\n",
      "loss: 0.133537  [25632/42552]\n",
      "loss: 0.375888  [28832/42552]\n",
      "loss: 0.298926  [32032/42552]\n",
      "loss: 0.355059  [35232/42552]\n",
      "loss: 0.283339  [38432/42552]\n",
      "loss: 0.443950  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.316342 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.425056  [   32/42552]\n",
      "loss: 0.195814  [ 3232/42552]\n",
      "loss: 0.335512  [ 6432/42552]\n",
      "loss: 0.245773  [ 9632/42552]\n",
      "loss: 0.294230  [12832/42552]\n",
      "loss: 0.413013  [16032/42552]\n",
      "loss: 0.435823  [19232/42552]\n",
      "loss: 0.346367  [22432/42552]\n",
      "loss: 0.273733  [25632/42552]\n",
      "loss: 0.357986  [28832/42552]\n",
      "loss: 0.260817  [32032/42552]\n",
      "loss: 0.357308  [35232/42552]\n",
      "loss: 0.309802  [38432/42552]\n",
      "loss: 0.401567  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.306464 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.277621  [   32/42552]\n",
      "loss: 0.311685  [ 3232/42552]\n",
      "loss: 0.264774  [ 6432/42552]\n",
      "loss: 0.291628  [ 9632/42552]\n",
      "loss: 0.302575  [12832/42552]\n",
      "loss: 0.336882  [16032/42552]\n",
      "loss: 0.342869  [19232/42552]\n",
      "loss: 0.323184  [22432/42552]\n",
      "loss: 0.230317  [25632/42552]\n",
      "loss: 0.360654  [28832/42552]\n",
      "loss: 0.416305  [32032/42552]\n",
      "loss: 0.268874  [35232/42552]\n",
      "loss: 0.429317  [38432/42552]\n",
      "loss: 0.415690  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.302148 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.210227  [   32/42552]\n",
      "loss: 0.418394  [ 3232/42552]\n",
      "loss: 0.512874  [ 6432/42552]\n",
      "loss: 0.372341  [ 9632/42552]\n",
      "loss: 0.449761  [12832/42552]\n",
      "loss: 0.364739  [16032/42552]\n",
      "loss: 0.229120  [19232/42552]\n",
      "loss: 0.326050  [22432/42552]\n",
      "loss: 0.284481  [25632/42552]\n",
      "loss: 0.444865  [28832/42552]\n",
      "loss: 0.290735  [32032/42552]\n",
      "loss: 0.304281  [35232/42552]\n",
      "loss: 0.485975  [38432/42552]\n",
      "loss: 0.281566  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.305707 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.248283  [   32/42552]\n",
      "loss: 0.287057  [ 3232/42552]\n",
      "loss: 0.370762  [ 6432/42552]\n",
      "loss: 0.227273  [ 9632/42552]\n",
      "loss: 0.410116  [12832/42552]\n",
      "loss: 0.272615  [16032/42552]\n",
      "loss: 0.397344  [19232/42552]\n",
      "loss: 0.309875  [22432/42552]\n",
      "loss: 0.297086  [25632/42552]\n",
      "loss: 0.332162  [28832/42552]\n",
      "loss: 0.379697  [32032/42552]\n",
      "loss: 0.357535  [35232/42552]\n",
      "loss: 0.316296  [38432/42552]\n",
      "loss: 0.332776  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.310910 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.353784  [   32/42552]\n",
      "loss: 0.349397  [ 3232/42552]\n",
      "loss: 0.270093  [ 6432/42552]\n",
      "loss: 0.329998  [ 9632/42552]\n",
      "loss: 0.187035  [12832/42552]\n",
      "loss: 0.325443  [16032/42552]\n",
      "loss: 0.383984  [19232/42552]\n",
      "loss: 0.297674  [22432/42552]\n",
      "loss: 0.284176  [25632/42552]\n",
      "loss: 0.282063  [28832/42552]\n",
      "loss: 0.372657  [32032/42552]\n",
      "loss: 0.395985  [35232/42552]\n",
      "loss: 0.404801  [38432/42552]\n",
      "loss: 0.365568  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.306449 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.401880  [   32/42552]\n",
      "loss: 0.463617  [ 3232/42552]\n",
      "loss: 0.215169  [ 6432/42552]\n",
      "loss: 0.312200  [ 9632/42552]\n",
      "loss: 0.331505  [12832/42552]\n",
      "loss: 0.252100  [16032/42552]\n",
      "loss: 0.298495  [19232/42552]\n",
      "loss: 0.339305  [22432/42552]\n",
      "loss: 0.280009  [25632/42552]\n",
      "loss: 0.384236  [28832/42552]\n",
      "loss: 0.332722  [32032/42552]\n",
      "loss: 0.267137  [35232/42552]\n",
      "loss: 0.313154  [38432/42552]\n",
      "loss: 0.245644  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.297445 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.261503  [   32/42552]\n",
      "loss: 0.208168  [ 3232/42552]\n",
      "loss: 0.375806  [ 6432/42552]\n",
      "loss: 0.322085  [ 9632/42552]\n",
      "loss: 0.212308  [12832/42552]\n",
      "loss: 0.285028  [16032/42552]\n",
      "loss: 0.342597  [19232/42552]\n",
      "loss: 0.330816  [22432/42552]\n",
      "loss: 0.476003  [25632/42552]\n",
      "loss: 0.351548  [28832/42552]\n",
      "loss: 0.335840  [32032/42552]\n",
      "loss: 0.475243  [35232/42552]\n",
      "loss: 0.370973  [38432/42552]\n",
      "loss: 0.238558  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.308501 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.337983  [   32/42552]\n",
      "loss: 0.294499  [ 3232/42552]\n",
      "loss: 0.289519  [ 6432/42552]\n",
      "loss: 0.413508  [ 9632/42552]\n",
      "loss: 0.457252  [12832/42552]\n",
      "loss: 0.328240  [16032/42552]\n",
      "loss: 0.354894  [19232/42552]\n",
      "loss: 0.363610  [22432/42552]\n",
      "loss: 0.395196  [25632/42552]\n",
      "loss: 0.357559  [28832/42552]\n",
      "loss: 0.416698  [32032/42552]\n",
      "loss: 0.451172  [35232/42552]\n",
      "loss: 0.265714  [38432/42552]\n",
      "loss: 0.285013  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.296419 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.179129  [   32/42552]\n",
      "loss: 0.228910  [ 3232/42552]\n",
      "loss: 0.345016  [ 6432/42552]\n",
      "loss: 0.345938  [ 9632/42552]\n",
      "loss: 0.245045  [12832/42552]\n",
      "loss: 0.352107  [16032/42552]\n",
      "loss: 0.291948  [19232/42552]\n",
      "loss: 0.245164  [22432/42552]\n",
      "loss: 0.576445  [25632/42552]\n",
      "loss: 0.262506  [28832/42552]\n",
      "loss: 0.318882  [32032/42552]\n",
      "loss: 0.269673  [35232/42552]\n",
      "loss: 0.296084  [38432/42552]\n",
      "loss: 0.342993  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.299483 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.254331  [   32/42552]\n",
      "loss: 0.355022  [ 3232/42552]\n",
      "loss: 0.282818  [ 6432/42552]\n",
      "loss: 0.280887  [ 9632/42552]\n",
      "loss: 0.161350  [12832/42552]\n",
      "loss: 0.311750  [16032/42552]\n",
      "loss: 0.328804  [19232/42552]\n",
      "loss: 0.387522  [22432/42552]\n",
      "loss: 0.297230  [25632/42552]\n",
      "loss: 0.369690  [28832/42552]\n",
      "loss: 0.404303  [32032/42552]\n",
      "loss: 0.459581  [35232/42552]\n",
      "loss: 0.573839  [38432/42552]\n",
      "loss: 0.252853  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.309822 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.246693  [   32/42552]\n",
      "loss: 0.400589  [ 3232/42552]\n",
      "loss: 0.346694  [ 6432/42552]\n",
      "loss: 0.248914  [ 9632/42552]\n",
      "loss: 0.262282  [12832/42552]\n",
      "loss: 0.400193  [16032/42552]\n",
      "loss: 0.295077  [19232/42552]\n",
      "loss: 0.352033  [22432/42552]\n",
      "loss: 0.439625  [25632/42552]\n",
      "loss: 0.321850  [28832/42552]\n",
      "loss: 0.376661  [32032/42552]\n",
      "loss: 0.388460  [35232/42552]\n",
      "loss: 0.261386  [38432/42552]\n",
      "loss: 0.307018  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.298346 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.361321  [   32/42552]\n",
      "loss: 0.250560  [ 3232/42552]\n",
      "loss: 0.314744  [ 6432/42552]\n",
      "loss: 0.229062  [ 9632/42552]\n",
      "loss: 0.309236  [12832/42552]\n",
      "loss: 0.342239  [16032/42552]\n",
      "loss: 0.183826  [19232/42552]\n",
      "loss: 0.411055  [22432/42552]\n",
      "loss: 0.333644  [25632/42552]\n",
      "loss: 0.269879  [28832/42552]\n",
      "loss: 0.394582  [32032/42552]\n",
      "loss: 0.372413  [35232/42552]\n",
      "loss: 0.223803  [38432/42552]\n",
      "loss: 0.255088  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.298943 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.255214  [   32/42552]\n",
      "loss: 0.208291  [ 3232/42552]\n",
      "loss: 0.292454  [ 6432/42552]\n",
      "loss: 0.340192  [ 9632/42552]\n",
      "loss: 0.371588  [12832/42552]\n",
      "loss: 0.232060  [16032/42552]\n",
      "loss: 0.342563  [19232/42552]\n",
      "loss: 0.359951  [22432/42552]\n",
      "loss: 0.360027  [25632/42552]\n",
      "loss: 0.308463  [28832/42552]\n",
      "loss: 0.486503  [32032/42552]\n",
      "loss: 0.240833  [35232/42552]\n",
      "loss: 0.552184  [38432/42552]\n",
      "loss: 0.313185  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.292261 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.340371  [   32/42552]\n",
      "loss: 0.334736  [ 3232/42552]\n",
      "loss: 0.206854  [ 6432/42552]\n",
      "loss: 0.263748  [ 9632/42552]\n",
      "loss: 0.255593  [12832/42552]\n",
      "loss: 0.383114  [16032/42552]\n",
      "loss: 0.298660  [19232/42552]\n",
      "loss: 0.309105  [22432/42552]\n",
      "loss: 0.262932  [25632/42552]\n",
      "loss: 0.319990  [28832/42552]\n",
      "loss: 0.482663  [32032/42552]\n",
      "loss: 0.393541  [35232/42552]\n",
      "loss: 0.248531  [38432/42552]\n",
      "loss: 0.385194  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.300454 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.372188  [   32/42552]\n",
      "loss: 0.376727  [ 3232/42552]\n",
      "loss: 0.445770  [ 6432/42552]\n",
      "loss: 0.275981  [ 9632/42552]\n",
      "loss: 0.363303  [12832/42552]\n",
      "loss: 0.349404  [16032/42552]\n",
      "loss: 0.258753  [19232/42552]\n",
      "loss: 0.513853  [22432/42552]\n",
      "loss: 0.176449  [25632/42552]\n",
      "loss: 0.289220  [28832/42552]\n",
      "loss: 0.298519  [32032/42552]\n",
      "loss: 0.314432  [35232/42552]\n",
      "loss: 0.240231  [38432/42552]\n",
      "loss: 0.380905  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.305622 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.632679  [   32/42552]\n",
      "loss: 0.469718  [ 3232/42552]\n",
      "loss: 0.236590  [ 6432/42552]\n",
      "loss: 0.465522  [ 9632/42552]\n",
      "loss: 0.328053  [12832/42552]\n",
      "loss: 0.366382  [16032/42552]\n",
      "loss: 0.265335  [19232/42552]\n",
      "loss: 0.326440  [22432/42552]\n",
      "loss: 0.221489  [25632/42552]\n",
      "loss: 0.280036  [28832/42552]\n",
      "loss: 0.244845  [32032/42552]\n",
      "loss: 0.346903  [35232/42552]\n",
      "loss: 0.237747  [38432/42552]\n",
      "loss: 0.411248  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.299274 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.497104  [   32/42552]\n",
      "loss: 0.405922  [ 3232/42552]\n",
      "loss: 0.253155  [ 6432/42552]\n",
      "loss: 0.336626  [ 9632/42552]\n",
      "loss: 0.309054  [12832/42552]\n",
      "loss: 0.272850  [16032/42552]\n",
      "loss: 0.366234  [19232/42552]\n",
      "loss: 0.296362  [22432/42552]\n",
      "loss: 0.490202  [25632/42552]\n",
      "loss: 0.313132  [28832/42552]\n",
      "loss: 0.330132  [32032/42552]\n",
      "loss: 0.451302  [35232/42552]\n",
      "loss: 0.208272  [38432/42552]\n",
      "loss: 0.250940  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.294294 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.211312  [   32/42552]\n",
      "loss: 0.251062  [ 3232/42552]\n",
      "loss: 0.395073  [ 6432/42552]\n",
      "loss: 0.337863  [ 9632/42552]\n",
      "loss: 0.259367  [12832/42552]\n",
      "loss: 0.257067  [16032/42552]\n",
      "loss: 0.300442  [19232/42552]\n",
      "loss: 0.463320  [22432/42552]\n",
      "loss: 0.280298  [25632/42552]\n",
      "loss: 0.245368  [28832/42552]\n",
      "loss: 0.383537  [32032/42552]\n",
      "loss: 0.270000  [35232/42552]\n",
      "loss: 0.305542  [38432/42552]\n",
      "loss: 0.311289  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.299072 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.220787  [   32/42552]\n",
      "loss: 0.222792  [ 3232/42552]\n",
      "loss: 0.331272  [ 6432/42552]\n",
      "loss: 0.364208  [ 9632/42552]\n",
      "loss: 0.415487  [12832/42552]\n",
      "loss: 0.304337  [16032/42552]\n",
      "loss: 0.294520  [19232/42552]\n",
      "loss: 0.481358  [22432/42552]\n",
      "loss: 0.398166  [25632/42552]\n",
      "loss: 0.307909  [28832/42552]\n",
      "loss: 0.346442  [32032/42552]\n",
      "loss: 0.326026  [35232/42552]\n",
      "loss: 0.279344  [38432/42552]\n",
      "loss: 0.516966  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.295226 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.281829  [   32/42552]\n",
      "loss: 0.258571  [ 3232/42552]\n",
      "loss: 0.377593  [ 6432/42552]\n",
      "loss: 0.332571  [ 9632/42552]\n",
      "loss: 0.319367  [12832/42552]\n",
      "loss: 0.315525  [16032/42552]\n",
      "loss: 0.425309  [19232/42552]\n",
      "loss: 0.265752  [22432/42552]\n",
      "loss: 0.308291  [25632/42552]\n",
      "loss: 0.399285  [28832/42552]\n",
      "loss: 0.341485  [32032/42552]\n",
      "loss: 0.399075  [35232/42552]\n",
      "loss: 0.392994  [38432/42552]\n",
      "loss: 0.286980  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.305075 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.336078  [   32/42552]\n",
      "loss: 0.419311  [ 3232/42552]\n",
      "loss: 0.264250  [ 6432/42552]\n",
      "loss: 0.294108  [ 9632/42552]\n",
      "loss: 0.294344  [12832/42552]\n",
      "loss: 0.271580  [16032/42552]\n",
      "loss: 0.248460  [19232/42552]\n",
      "loss: 0.261564  [22432/42552]\n",
      "loss: 0.381505  [25632/42552]\n",
      "loss: 0.310963  [28832/42552]\n",
      "loss: 0.294969  [32032/42552]\n",
      "loss: 0.344084  [35232/42552]\n",
      "loss: 0.234886  [38432/42552]\n",
      "loss: 0.287279  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.289817 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.272846  [   32/42552]\n",
      "loss: 0.391983  [ 3232/42552]\n",
      "loss: 0.590785  [ 6432/42552]\n",
      "loss: 0.317285  [ 9632/42552]\n",
      "loss: 0.347384  [12832/42552]\n",
      "loss: 0.286528  [16032/42552]\n",
      "loss: 0.308103  [19232/42552]\n",
      "loss: 0.411427  [22432/42552]\n",
      "loss: 0.288381  [25632/42552]\n",
      "loss: 0.287457  [28832/42552]\n",
      "loss: 0.341225  [32032/42552]\n",
      "loss: 0.380664  [35232/42552]\n",
      "loss: 0.418731  [38432/42552]\n",
      "loss: 0.453750  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.288154 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.231055  [   32/42552]\n",
      "loss: 0.268271  [ 3232/42552]\n",
      "loss: 0.336973  [ 6432/42552]\n",
      "loss: 0.361781  [ 9632/42552]\n",
      "loss: 0.356720  [12832/42552]\n",
      "loss: 0.460961  [16032/42552]\n",
      "loss: 0.316025  [19232/42552]\n",
      "loss: 0.307472  [22432/42552]\n",
      "loss: 0.262975  [25632/42552]\n",
      "loss: 0.390745  [28832/42552]\n",
      "loss: 0.390529  [32032/42552]\n",
      "loss: 0.332349  [35232/42552]\n",
      "loss: 0.282766  [38432/42552]\n",
      "loss: 0.364378  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.291920 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.318785  [   32/42552]\n",
      "loss: 0.375537  [ 3232/42552]\n",
      "loss: 0.433009  [ 6432/42552]\n",
      "loss: 0.397644  [ 9632/42552]\n",
      "loss: 0.393461  [12832/42552]\n",
      "loss: 0.272606  [16032/42552]\n",
      "loss: 0.306508  [19232/42552]\n",
      "loss: 0.361043  [22432/42552]\n",
      "loss: 0.315937  [25632/42552]\n",
      "loss: 0.319642  [28832/42552]\n",
      "loss: 0.274076  [32032/42552]\n",
      "loss: 0.255551  [35232/42552]\n",
      "loss: 0.321401  [38432/42552]\n",
      "loss: 0.332627  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.291765 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.358426  [   32/42552]\n",
      "loss: 0.408980  [ 3232/42552]\n",
      "loss: 0.335421  [ 6432/42552]\n",
      "loss: 0.360860  [ 9632/42552]\n",
      "loss: 0.390225  [12832/42552]\n",
      "loss: 0.396136  [16032/42552]\n",
      "loss: 0.224954  [19232/42552]\n",
      "loss: 0.254361  [22432/42552]\n",
      "loss: 0.467259  [25632/42552]\n",
      "loss: 0.285664  [28832/42552]\n",
      "loss: 0.326680  [32032/42552]\n",
      "loss: 0.349352  [35232/42552]\n",
      "loss: 0.366491  [38432/42552]\n",
      "loss: 0.368960  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.287899 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.276625  [   32/42552]\n",
      "loss: 0.334652  [ 3232/42552]\n",
      "loss: 0.393452  [ 6432/42552]\n",
      "loss: 0.278265  [ 9632/42552]\n",
      "loss: 0.334580  [12832/42552]\n",
      "loss: 0.530158  [16032/42552]\n",
      "loss: 0.275156  [19232/42552]\n",
      "loss: 0.363632  [22432/42552]\n",
      "loss: 0.180812  [25632/42552]\n",
      "loss: 0.301681  [28832/42552]\n",
      "loss: 0.418927  [32032/42552]\n",
      "loss: 0.373669  [35232/42552]\n",
      "loss: 0.304249  [38432/42552]\n",
      "loss: 0.458372  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.292567 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.168732  [   32/42552]\n",
      "loss: 0.272220  [ 3232/42552]\n",
      "loss: 0.232310  [ 6432/42552]\n",
      "loss: 0.288357  [ 9632/42552]\n",
      "loss: 0.494406  [12832/42552]\n",
      "loss: 0.456551  [16032/42552]\n",
      "loss: 0.283948  [19232/42552]\n",
      "loss: 0.303391  [22432/42552]\n",
      "loss: 0.414008  [25632/42552]\n",
      "loss: 0.425584  [28832/42552]\n",
      "loss: 0.350942  [32032/42552]\n",
      "loss: 0.443413  [35232/42552]\n",
      "loss: 0.393215  [38432/42552]\n",
      "loss: 0.311334  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.288392 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.311521  [   32/42552]\n",
      "loss: 0.302956  [ 3232/42552]\n",
      "loss: 0.292478  [ 6432/42552]\n",
      "loss: 0.279636  [ 9632/42552]\n",
      "loss: 0.411255  [12832/42552]\n",
      "loss: 0.392742  [16032/42552]\n",
      "loss: 0.423161  [19232/42552]\n",
      "loss: 0.354343  [22432/42552]\n",
      "loss: 0.368323  [25632/42552]\n",
      "loss: 0.269473  [28832/42552]\n",
      "loss: 0.236194  [32032/42552]\n",
      "loss: 0.400184  [35232/42552]\n",
      "loss: 0.356467  [38432/42552]\n",
      "loss: 0.348709  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.286188 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.306757  [   32/42552]\n",
      "loss: 0.385897  [ 3232/42552]\n",
      "loss: 0.302354  [ 6432/42552]\n",
      "loss: 0.284145  [ 9632/42552]\n",
      "loss: 0.370240  [12832/42552]\n",
      "loss: 0.262930  [16032/42552]\n",
      "loss: 0.296025  [19232/42552]\n",
      "loss: 0.259325  [22432/42552]\n",
      "loss: 0.344496  [25632/42552]\n",
      "loss: 0.313653  [28832/42552]\n",
      "loss: 0.282284  [32032/42552]\n",
      "loss: 0.334032  [35232/42552]\n",
      "loss: 0.352027  [38432/42552]\n",
      "loss: 0.350986  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.288479 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.257460  [   32/42552]\n",
      "loss: 0.254416  [ 3232/42552]\n",
      "loss: 0.296435  [ 6432/42552]\n",
      "loss: 0.446780  [ 9632/42552]\n",
      "loss: 0.425316  [12832/42552]\n",
      "loss: 0.307336  [16032/42552]\n",
      "loss: 0.399167  [19232/42552]\n",
      "loss: 0.366231  [22432/42552]\n",
      "loss: 0.373663  [25632/42552]\n",
      "loss: 0.232152  [28832/42552]\n",
      "loss: 0.241761  [32032/42552]\n",
      "loss: 0.263056  [35232/42552]\n",
      "loss: 0.438479  [38432/42552]\n",
      "loss: 0.327725  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.291711 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.230645  [   32/42552]\n",
      "loss: 0.339002  [ 3232/42552]\n",
      "loss: 0.304439  [ 6432/42552]\n",
      "loss: 0.387030  [ 9632/42552]\n",
      "loss: 0.229993  [12832/42552]\n",
      "loss: 0.286311  [16032/42552]\n",
      "loss: 0.313763  [19232/42552]\n",
      "loss: 0.318956  [22432/42552]\n",
      "loss: 0.379502  [25632/42552]\n",
      "loss: 0.304621  [28832/42552]\n",
      "loss: 0.323672  [32032/42552]\n",
      "loss: 0.229782  [35232/42552]\n",
      "loss: 0.346903  [38432/42552]\n",
      "loss: 0.402518  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.289683 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.271559  [   32/42552]\n",
      "loss: 0.403777  [ 3232/42552]\n",
      "loss: 0.373427  [ 6432/42552]\n",
      "loss: 0.287631  [ 9632/42552]\n",
      "loss: 0.245457  [12832/42552]\n",
      "loss: 0.166378  [16032/42552]\n",
      "loss: 0.389560  [19232/42552]\n",
      "loss: 0.325719  [22432/42552]\n",
      "loss: 0.255566  [25632/42552]\n",
      "loss: 0.214310  [28832/42552]\n",
      "loss: 0.406918  [32032/42552]\n",
      "loss: 0.329228  [35232/42552]\n",
      "loss: 0.262206  [38432/42552]\n",
      "loss: 0.430605  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.290731 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.338841  [   32/42552]\n",
      "loss: 0.316292  [ 3232/42552]\n",
      "loss: 0.403804  [ 6432/42552]\n",
      "loss: 0.262276  [ 9632/42552]\n",
      "loss: 0.367060  [12832/42552]\n",
      "loss: 0.417379  [16032/42552]\n",
      "loss: 0.377102  [19232/42552]\n",
      "loss: 0.356943  [22432/42552]\n",
      "loss: 0.307378  [25632/42552]\n",
      "loss: 0.316159  [28832/42552]\n",
      "loss: 0.361399  [32032/42552]\n",
      "loss: 0.317867  [35232/42552]\n",
      "loss: 0.318731  [38432/42552]\n",
      "loss: 0.347340  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.285520 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.312323  [   32/42552]\n",
      "loss: 0.278610  [ 3232/42552]\n",
      "loss: 0.158895  [ 6432/42552]\n",
      "loss: 0.313220  [ 9632/42552]\n",
      "loss: 0.477574  [12832/42552]\n",
      "loss: 0.313472  [16032/42552]\n",
      "loss: 0.220452  [19232/42552]\n",
      "loss: 0.371589  [22432/42552]\n",
      "loss: 0.368586  [25632/42552]\n",
      "loss: 0.599806  [28832/42552]\n",
      "loss: 0.220570  [32032/42552]\n",
      "loss: 0.444935  [35232/42552]\n",
      "loss: 0.350185  [38432/42552]\n",
      "loss: 0.301348  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.291876 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.334574  [   32/42552]\n",
      "loss: 0.407414  [ 3232/42552]\n",
      "loss: 0.160968  [ 6432/42552]\n",
      "loss: 0.275838  [ 9632/42552]\n",
      "loss: 0.236802  [12832/42552]\n",
      "loss: 0.347004  [16032/42552]\n",
      "loss: 0.275189  [19232/42552]\n",
      "loss: 0.308569  [22432/42552]\n",
      "loss: 0.323121  [25632/42552]\n",
      "loss: 0.330149  [28832/42552]\n",
      "loss: 0.370378  [32032/42552]\n",
      "loss: 0.286165  [35232/42552]\n",
      "loss: 0.223836  [38432/42552]\n",
      "loss: 0.433612  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.287764 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.454270  [   32/42552]\n",
      "loss: 0.313380  [ 3232/42552]\n",
      "loss: 0.227506  [ 6432/42552]\n",
      "loss: 0.320545  [ 9632/42552]\n",
      "loss: 0.269554  [12832/42552]\n",
      "loss: 0.404180  [16032/42552]\n",
      "loss: 0.443342  [19232/42552]\n",
      "loss: 0.369075  [22432/42552]\n",
      "loss: 0.467217  [25632/42552]\n",
      "loss: 0.364110  [28832/42552]\n",
      "loss: 0.300841  [32032/42552]\n",
      "loss: 0.315810  [35232/42552]\n",
      "loss: 0.207364  [38432/42552]\n",
      "loss: 0.312912  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.298926 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.211516  [   32/42552]\n",
      "loss: 0.289847  [ 3232/42552]\n",
      "loss: 0.299250  [ 6432/42552]\n",
      "loss: 0.275378  [ 9632/42552]\n",
      "loss: 0.283233  [12832/42552]\n",
      "loss: 0.204916  [16032/42552]\n",
      "loss: 0.372802  [19232/42552]\n",
      "loss: 0.314369  [22432/42552]\n",
      "loss: 0.275099  [25632/42552]\n",
      "loss: 0.273688  [28832/42552]\n",
      "loss: 0.409216  [32032/42552]\n",
      "loss: 0.392885  [35232/42552]\n",
      "loss: 0.256463  [38432/42552]\n",
      "loss: 0.520697  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.283346 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.291038  [   32/42552]\n",
      "loss: 0.233138  [ 3232/42552]\n",
      "loss: 0.290991  [ 6432/42552]\n",
      "loss: 0.222225  [ 9632/42552]\n",
      "loss: 0.330606  [12832/42552]\n",
      "loss: 0.197113  [16032/42552]\n",
      "loss: 0.228148  [19232/42552]\n",
      "loss: 0.333486  [22432/42552]\n",
      "loss: 0.199379  [25632/42552]\n",
      "loss: 0.366025  [28832/42552]\n",
      "loss: 0.249281  [32032/42552]\n",
      "loss: 0.326859  [35232/42552]\n",
      "loss: 0.263420  [38432/42552]\n",
      "loss: 0.440031  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.283524 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.281609  [   32/42552]\n",
      "loss: 0.336181  [ 3232/42552]\n",
      "loss: 0.362316  [ 6432/42552]\n",
      "loss: 0.496203  [ 9632/42552]\n",
      "loss: 0.292108  [12832/42552]\n",
      "loss: 0.327867  [16032/42552]\n",
      "loss: 0.327382  [19232/42552]\n",
      "loss: 0.350713  [22432/42552]\n",
      "loss: 0.340146  [25632/42552]\n",
      "loss: 0.328891  [28832/42552]\n",
      "loss: 0.306569  [32032/42552]\n",
      "loss: 0.278459  [35232/42552]\n",
      "loss: 0.375616  [38432/42552]\n",
      "loss: 0.245290  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.284041 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.284786  [   32/42552]\n",
      "loss: 0.343189  [ 3232/42552]\n",
      "loss: 0.189188  [ 6432/42552]\n",
      "loss: 0.190991  [ 9632/42552]\n",
      "loss: 0.430069  [12832/42552]\n",
      "loss: 0.289574  [16032/42552]\n",
      "loss: 0.302368  [19232/42552]\n",
      "loss: 0.300976  [22432/42552]\n",
      "loss: 0.307519  [25632/42552]\n",
      "loss: 0.333260  [28832/42552]\n",
      "loss: 0.296201  [32032/42552]\n",
      "loss: 0.276502  [35232/42552]\n",
      "loss: 0.381228  [38432/42552]\n",
      "loss: 0.245843  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.283845 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.264520  [   32/42552]\n",
      "loss: 0.212251  [ 3232/42552]\n",
      "loss: 0.357879  [ 6432/42552]\n",
      "loss: 0.284495  [ 9632/42552]\n",
      "loss: 0.211149  [12832/42552]\n",
      "loss: 0.260923  [16032/42552]\n",
      "loss: 0.311755  [19232/42552]\n",
      "loss: 0.264160  [22432/42552]\n",
      "loss: 0.558562  [25632/42552]\n",
      "loss: 0.383738  [28832/42552]\n",
      "loss: 0.266943  [32032/42552]\n",
      "loss: 0.349670  [35232/42552]\n",
      "loss: 0.227880  [38432/42552]\n",
      "loss: 0.225739  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.287628 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.405780  [   32/42552]\n",
      "loss: 0.291474  [ 3232/42552]\n",
      "loss: 0.310556  [ 6432/42552]\n",
      "loss: 0.353558  [ 9632/42552]\n",
      "loss: 0.274271  [12832/42552]\n",
      "loss: 0.420887  [16032/42552]\n",
      "loss: 0.243860  [19232/42552]\n",
      "loss: 0.334314  [22432/42552]\n",
      "loss: 0.395775  [25632/42552]\n",
      "loss: 0.248072  [28832/42552]\n",
      "loss: 0.297530  [32032/42552]\n",
      "loss: 0.271291  [35232/42552]\n",
      "loss: 0.259882  [38432/42552]\n",
      "loss: 0.401005  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.283190 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.353051  [   32/42552]\n",
      "loss: 0.250776  [ 3232/42552]\n",
      "loss: 0.465550  [ 6432/42552]\n",
      "loss: 0.344713  [ 9632/42552]\n",
      "loss: 0.396619  [12832/42552]\n",
      "loss: 0.222598  [16032/42552]\n",
      "loss: 0.335670  [19232/42552]\n",
      "loss: 0.428803  [22432/42552]\n",
      "loss: 0.346911  [25632/42552]\n",
      "loss: 0.330493  [28832/42552]\n",
      "loss: 0.297844  [32032/42552]\n",
      "loss: 0.243154  [35232/42552]\n",
      "loss: 0.282221  [38432/42552]\n",
      "loss: 0.358129  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.278873 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.344301  [   32/42552]\n",
      "loss: 0.400154  [ 3232/42552]\n",
      "loss: 0.309739  [ 6432/42552]\n",
      "loss: 0.353907  [ 9632/42552]\n",
      "loss: 0.248623  [12832/42552]\n",
      "loss: 0.254941  [16032/42552]\n",
      "loss: 0.220632  [19232/42552]\n",
      "loss: 0.321079  [22432/42552]\n",
      "loss: 0.368054  [25632/42552]\n",
      "loss: 0.277054  [28832/42552]\n",
      "loss: 0.239849  [32032/42552]\n",
      "loss: 0.225798  [35232/42552]\n",
      "loss: 0.279872  [38432/42552]\n",
      "loss: 0.280034  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.283740 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.597090  [   32/42552]\n",
      "loss: 0.364060  [ 3232/42552]\n",
      "loss: 0.390758  [ 6432/42552]\n",
      "loss: 0.162691  [ 9632/42552]\n",
      "loss: 0.297162  [12832/42552]\n",
      "loss: 0.270049  [16032/42552]\n",
      "loss: 0.396528  [19232/42552]\n",
      "loss: 0.492878  [22432/42552]\n",
      "loss: 0.289354  [25632/42552]\n",
      "loss: 0.334229  [28832/42552]\n",
      "loss: 0.357600  [32032/42552]\n",
      "loss: 0.368771  [35232/42552]\n",
      "loss: 0.249282  [38432/42552]\n",
      "loss: 0.249053  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.279719 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.247589  [   32/42552]\n",
      "loss: 0.311714  [ 3232/42552]\n",
      "loss: 0.281248  [ 6432/42552]\n",
      "loss: 0.240047  [ 9632/42552]\n",
      "loss: 0.289349  [12832/42552]\n",
      "loss: 0.253509  [16032/42552]\n",
      "loss: 0.223037  [19232/42552]\n",
      "loss: 0.233435  [22432/42552]\n",
      "loss: 0.296238  [25632/42552]\n",
      "loss: 0.359508  [28832/42552]\n",
      "loss: 0.318034  [32032/42552]\n",
      "loss: 0.317235  [35232/42552]\n",
      "loss: 0.244152  [38432/42552]\n",
      "loss: 0.382988  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.273298 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.307295  [   32/42552]\n",
      "loss: 0.471585  [ 3232/42552]\n",
      "loss: 0.259545  [ 6432/42552]\n",
      "loss: 0.321195  [ 9632/42552]\n",
      "loss: 0.226283  [12832/42552]\n",
      "loss: 0.410358  [16032/42552]\n",
      "loss: 0.388187  [19232/42552]\n",
      "loss: 0.400316  [22432/42552]\n",
      "loss: 0.162555  [25632/42552]\n",
      "loss: 0.344201  [28832/42552]\n",
      "loss: 0.378133  [32032/42552]\n",
      "loss: 0.257002  [35232/42552]\n",
      "loss: 0.231522  [38432/42552]\n",
      "loss: 0.202557  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.287355 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.356235  [   32/42552]\n",
      "loss: 0.532875  [ 3232/42552]\n",
      "loss: 0.288459  [ 6432/42552]\n",
      "loss: 0.387358  [ 9632/42552]\n",
      "loss: 0.347442  [12832/42552]\n",
      "loss: 0.267691  [16032/42552]\n",
      "loss: 0.227729  [19232/42552]\n",
      "loss: 0.413256  [22432/42552]\n",
      "loss: 0.301277  [25632/42552]\n",
      "loss: 0.320584  [28832/42552]\n",
      "loss: 0.356905  [32032/42552]\n",
      "loss: 0.418979  [35232/42552]\n",
      "loss: 0.271458  [38432/42552]\n",
      "loss: 0.259574  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.286641 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.296494  [   32/42552]\n",
      "loss: 0.489845  [ 3232/42552]\n",
      "loss: 0.257383  [ 6432/42552]\n",
      "loss: 0.258996  [ 9632/42552]\n",
      "loss: 0.280243  [12832/42552]\n",
      "loss: 0.323884  [16032/42552]\n",
      "loss: 0.295284  [19232/42552]\n",
      "loss: 0.436823  [22432/42552]\n",
      "loss: 0.240242  [25632/42552]\n",
      "loss: 0.293565  [28832/42552]\n",
      "loss: 0.294536  [32032/42552]\n",
      "loss: 0.220702  [35232/42552]\n",
      "loss: 0.291371  [38432/42552]\n",
      "loss: 0.316351  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.280861 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.311047  [   32/42552]\n",
      "loss: 0.252743  [ 3232/42552]\n",
      "loss: 0.325172  [ 6432/42552]\n",
      "loss: 0.366878  [ 9632/42552]\n",
      "loss: 0.460036  [12832/42552]\n",
      "loss: 0.170078  [16032/42552]\n",
      "loss: 0.275475  [19232/42552]\n",
      "loss: 0.299874  [22432/42552]\n",
      "loss: 0.374872  [25632/42552]\n",
      "loss: 0.425271  [28832/42552]\n",
      "loss: 0.302386  [32032/42552]\n",
      "loss: 0.256081  [35232/42552]\n",
      "loss: 0.360026  [38432/42552]\n",
      "loss: 0.299658  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.275498 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.306027  [   32/42552]\n",
      "loss: 0.196629  [ 3232/42552]\n",
      "loss: 0.365971  [ 6432/42552]\n",
      "loss: 0.290513  [ 9632/42552]\n",
      "loss: 0.353137  [12832/42552]\n",
      "loss: 0.378141  [16032/42552]\n",
      "loss: 0.376367  [19232/42552]\n",
      "loss: 0.264687  [22432/42552]\n",
      "loss: 0.328732  [25632/42552]\n",
      "loss: 0.506585  [28832/42552]\n",
      "loss: 0.482245  [32032/42552]\n",
      "loss: 0.394312  [35232/42552]\n",
      "loss: 0.379678  [38432/42552]\n",
      "loss: 0.280915  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.280104 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.360654  [   32/42552]\n",
      "loss: 0.290009  [ 3232/42552]\n",
      "loss: 0.395355  [ 6432/42552]\n",
      "loss: 0.184271  [ 9632/42552]\n",
      "loss: 0.412718  [12832/42552]\n",
      "loss: 0.330110  [16032/42552]\n",
      "loss: 0.235420  [19232/42552]\n",
      "loss: 0.318851  [22432/42552]\n",
      "loss: 0.280189  [25632/42552]\n",
      "loss: 0.369577  [28832/42552]\n",
      "loss: 0.277364  [32032/42552]\n",
      "loss: 0.438478  [35232/42552]\n",
      "loss: 0.336278  [38432/42552]\n",
      "loss: 0.296993  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.282926 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.180860  [   32/42552]\n",
      "loss: 0.265679  [ 3232/42552]\n",
      "loss: 0.328282  [ 6432/42552]\n",
      "loss: 0.285927  [ 9632/42552]\n",
      "loss: 0.309668  [12832/42552]\n",
      "loss: 0.236601  [16032/42552]\n",
      "loss: 0.170397  [19232/42552]\n",
      "loss: 0.307086  [22432/42552]\n",
      "loss: 0.368627  [25632/42552]\n",
      "loss: 0.272562  [28832/42552]\n",
      "loss: 0.374393  [32032/42552]\n",
      "loss: 0.217271  [35232/42552]\n",
      "loss: 0.299705  [38432/42552]\n",
      "loss: 0.386804  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.282463 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.262031  [   32/42552]\n",
      "loss: 0.292971  [ 3232/42552]\n",
      "loss: 0.318308  [ 6432/42552]\n",
      "loss: 0.414711  [ 9632/42552]\n",
      "loss: 0.406671  [12832/42552]\n",
      "loss: 0.188462  [16032/42552]\n",
      "loss: 0.185118  [19232/42552]\n",
      "loss: 0.186378  [22432/42552]\n",
      "loss: 0.262949  [25632/42552]\n",
      "loss: 0.203742  [28832/42552]\n",
      "loss: 0.232300  [32032/42552]\n",
      "loss: 0.296943  [35232/42552]\n",
      "loss: 0.547461  [38432/42552]\n",
      "loss: 0.268221  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.289983 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.401623  [   32/42552]\n",
      "loss: 0.445963  [ 3232/42552]\n",
      "loss: 0.377285  [ 6432/42552]\n",
      "loss: 0.376529  [ 9632/42552]\n",
      "loss: 0.327434  [12832/42552]\n",
      "loss: 0.334205  [16032/42552]\n",
      "loss: 0.386786  [19232/42552]\n",
      "loss: 0.261911  [22432/42552]\n",
      "loss: 0.309353  [25632/42552]\n",
      "loss: 0.298320  [28832/42552]\n",
      "loss: 0.490318  [32032/42552]\n",
      "loss: 0.372797  [35232/42552]\n",
      "loss: 0.226780  [38432/42552]\n",
      "loss: 0.241632  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.274513 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.276891  [   32/42552]\n",
      "loss: 0.427435  [ 3232/42552]\n",
      "loss: 0.335349  [ 6432/42552]\n",
      "loss: 0.385759  [ 9632/42552]\n",
      "loss: 0.231763  [12832/42552]\n",
      "loss: 0.357544  [16032/42552]\n",
      "loss: 0.258882  [19232/42552]\n",
      "loss: 0.227844  [22432/42552]\n",
      "loss: 0.229291  [25632/42552]\n",
      "loss: 0.414714  [28832/42552]\n",
      "loss: 0.337409  [32032/42552]\n",
      "loss: 0.289278  [35232/42552]\n",
      "loss: 0.224362  [38432/42552]\n",
      "loss: 0.318944  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.276419 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.217849  [   32/42552]\n",
      "loss: 0.502813  [ 3232/42552]\n",
      "loss: 0.193402  [ 6432/42552]\n",
      "loss: 0.213385  [ 9632/42552]\n",
      "loss: 0.268465  [12832/42552]\n",
      "loss: 0.379263  [16032/42552]\n",
      "loss: 0.295111  [19232/42552]\n",
      "loss: 0.257007  [22432/42552]\n",
      "loss: 0.203042  [25632/42552]\n",
      "loss: 0.199817  [28832/42552]\n",
      "loss: 0.258651  [32032/42552]\n",
      "loss: 0.245457  [35232/42552]\n",
      "loss: 0.255832  [38432/42552]\n",
      "loss: 0.291913  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.281121 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.367799  [   32/42552]\n",
      "loss: 0.217179  [ 3232/42552]\n",
      "loss: 0.290308  [ 6432/42552]\n",
      "loss: 0.290329  [ 9632/42552]\n",
      "loss: 0.307380  [12832/42552]\n",
      "loss: 0.337595  [16032/42552]\n",
      "loss: 0.472392  [19232/42552]\n",
      "loss: 0.391922  [22432/42552]\n",
      "loss: 0.276757  [25632/42552]\n",
      "loss: 0.258476  [28832/42552]\n",
      "loss: 0.357192  [32032/42552]\n",
      "loss: 0.367050  [35232/42552]\n",
      "loss: 0.342129  [38432/42552]\n",
      "loss: 0.337683  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.272456 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.485183  [   32/42552]\n",
      "loss: 0.356205  [ 3232/42552]\n",
      "loss: 0.271778  [ 6432/42552]\n",
      "loss: 0.169937  [ 9632/42552]\n",
      "loss: 0.450748  [12832/42552]\n",
      "loss: 0.311290  [16032/42552]\n",
      "loss: 0.381836  [19232/42552]\n",
      "loss: 0.262729  [22432/42552]\n",
      "loss: 0.451098  [25632/42552]\n",
      "loss: 0.265362  [28832/42552]\n",
      "loss: 0.480758  [32032/42552]\n",
      "loss: 0.441566  [35232/42552]\n",
      "loss: 0.380228  [38432/42552]\n",
      "loss: 0.191726  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.276215 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.346500  [   32/42552]\n",
      "loss: 0.272124  [ 3232/42552]\n",
      "loss: 0.397656  [ 6432/42552]\n",
      "loss: 0.246423  [ 9632/42552]\n",
      "loss: 0.194749  [12832/42552]\n",
      "loss: 0.268911  [16032/42552]\n",
      "loss: 0.473307  [19232/42552]\n",
      "loss: 0.284935  [22432/42552]\n",
      "loss: 0.344689  [25632/42552]\n",
      "loss: 0.378944  [28832/42552]\n",
      "loss: 0.488589  [32032/42552]\n",
      "loss: 0.244099  [35232/42552]\n",
      "loss: 0.416839  [38432/42552]\n",
      "loss: 0.318216  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.275524 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.252605  [   32/42552]\n",
      "loss: 0.491931  [ 3232/42552]\n",
      "loss: 0.296739  [ 6432/42552]\n",
      "loss: 0.352680  [ 9632/42552]\n",
      "loss: 0.304225  [12832/42552]\n",
      "loss: 0.414068  [16032/42552]\n",
      "loss: 0.275414  [19232/42552]\n",
      "loss: 0.311943  [22432/42552]\n",
      "loss: 0.314654  [25632/42552]\n",
      "loss: 0.238715  [28832/42552]\n",
      "loss: 0.315871  [32032/42552]\n",
      "loss: 0.221805  [35232/42552]\n",
      "loss: 0.314961  [38432/42552]\n",
      "loss: 0.329144  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.280452 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.256272  [   32/42552]\n",
      "loss: 0.271655  [ 3232/42552]\n",
      "loss: 0.386661  [ 6432/42552]\n",
      "loss: 0.313473  [ 9632/42552]\n",
      "loss: 0.233217  [12832/42552]\n",
      "loss: 0.206410  [16032/42552]\n",
      "loss: 0.298516  [19232/42552]\n",
      "loss: 0.304128  [22432/42552]\n",
      "loss: 0.290748  [25632/42552]\n",
      "loss: 0.211511  [28832/42552]\n",
      "loss: 0.328971  [32032/42552]\n",
      "loss: 0.409677  [35232/42552]\n",
      "loss: 0.310495  [38432/42552]\n",
      "loss: 0.257978  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.268961 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.355341  [   32/42552]\n",
      "loss: 0.296288  [ 3232/42552]\n",
      "loss: 0.312594  [ 6432/42552]\n",
      "loss: 0.325614  [ 9632/42552]\n",
      "loss: 0.313003  [12832/42552]\n",
      "loss: 0.258612  [16032/42552]\n",
      "loss: 0.428622  [19232/42552]\n",
      "loss: 0.275104  [22432/42552]\n",
      "loss: 0.278372  [25632/42552]\n",
      "loss: 0.358091  [28832/42552]\n",
      "loss: 0.220958  [32032/42552]\n",
      "loss: 0.250307  [35232/42552]\n",
      "loss: 0.281914  [38432/42552]\n",
      "loss: 0.281597  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.274893 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.305106  [   32/42552]\n",
      "loss: 0.276179  [ 3232/42552]\n",
      "loss: 0.331866  [ 6432/42552]\n",
      "loss: 0.259452  [ 9632/42552]\n",
      "loss: 0.286550  [12832/42552]\n",
      "loss: 0.443658  [16032/42552]\n",
      "loss: 0.226450  [19232/42552]\n",
      "loss: 0.340810  [22432/42552]\n",
      "loss: 0.254406  [25632/42552]\n",
      "loss: 0.162370  [28832/42552]\n",
      "loss: 0.277267  [32032/42552]\n",
      "loss: 0.336877  [35232/42552]\n",
      "loss: 0.342283  [38432/42552]\n",
      "loss: 0.253255  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.275565 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.273161  [   32/42552]\n",
      "loss: 0.350980  [ 3232/42552]\n",
      "loss: 0.294898  [ 6432/42552]\n",
      "loss: 0.212989  [ 9632/42552]\n",
      "loss: 0.344818  [12832/42552]\n",
      "loss: 0.341637  [16032/42552]\n",
      "loss: 0.403377  [19232/42552]\n",
      "loss: 0.223747  [22432/42552]\n",
      "loss: 0.403717  [25632/42552]\n",
      "loss: 0.221333  [28832/42552]\n",
      "loss: 0.332961  [32032/42552]\n",
      "loss: 0.372854  [35232/42552]\n",
      "loss: 0.354775  [38432/42552]\n",
      "loss: 0.393674  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.279442 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.306303  [   32/42552]\n",
      "loss: 0.388464  [ 3232/42552]\n",
      "loss: 0.326060  [ 6432/42552]\n",
      "loss: 0.176964  [ 9632/42552]\n",
      "loss: 0.242466  [12832/42552]\n",
      "loss: 0.269629  [16032/42552]\n",
      "loss: 0.316552  [19232/42552]\n",
      "loss: 0.311672  [22432/42552]\n",
      "loss: 0.325473  [25632/42552]\n",
      "loss: 0.337280  [28832/42552]\n",
      "loss: 0.357325  [32032/42552]\n",
      "loss: 0.253719  [35232/42552]\n",
      "loss: 0.308490  [38432/42552]\n",
      "loss: 0.303018  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.278149 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.370141  [   32/42552]\n",
      "loss: 0.263855  [ 3232/42552]\n",
      "loss: 0.249728  [ 6432/42552]\n",
      "loss: 0.346218  [ 9632/42552]\n",
      "loss: 0.337992  [12832/42552]\n",
      "loss: 0.375370  [16032/42552]\n",
      "loss: 0.384961  [19232/42552]\n",
      "loss: 0.251313  [22432/42552]\n",
      "loss: 0.428430  [25632/42552]\n",
      "loss: 0.371848  [28832/42552]\n",
      "loss: 0.355355  [32032/42552]\n",
      "loss: 0.243663  [35232/42552]\n",
      "loss: 0.352360  [38432/42552]\n",
      "loss: 0.332278  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.274940 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.398572  [   32/42552]\n",
      "loss: 0.212723  [ 3232/42552]\n",
      "loss: 0.368676  [ 6432/42552]\n",
      "loss: 0.233810  [ 9632/42552]\n",
      "loss: 0.324115  [12832/42552]\n",
      "loss: 0.237681  [16032/42552]\n",
      "loss: 0.268392  [19232/42552]\n",
      "loss: 0.223151  [22432/42552]\n",
      "loss: 0.372735  [25632/42552]\n",
      "loss: 0.288521  [28832/42552]\n",
      "loss: 0.293124  [32032/42552]\n",
      "loss: 0.318328  [35232/42552]\n",
      "loss: 0.291524  [38432/42552]\n",
      "loss: 0.366393  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.270900 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.201692  [   32/42552]\n",
      "loss: 0.394544  [ 3232/42552]\n",
      "loss: 0.246638  [ 6432/42552]\n",
      "loss: 0.273470  [ 9632/42552]\n",
      "loss: 0.397829  [12832/42552]\n",
      "loss: 0.303736  [16032/42552]\n",
      "loss: 0.345628  [19232/42552]\n",
      "loss: 0.289816  [22432/42552]\n",
      "loss: 0.325967  [25632/42552]\n",
      "loss: 0.360334  [28832/42552]\n",
      "loss: 0.500792  [32032/42552]\n",
      "loss: 0.351026  [35232/42552]\n",
      "loss: 0.242201  [38432/42552]\n",
      "loss: 0.247880  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.278118 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.222459  [   32/42552]\n",
      "loss: 0.350274  [ 3232/42552]\n",
      "loss: 0.398244  [ 6432/42552]\n",
      "loss: 0.333873  [ 9632/42552]\n",
      "loss: 0.354143  [12832/42552]\n",
      "loss: 0.337235  [16032/42552]\n",
      "loss: 0.227752  [19232/42552]\n",
      "loss: 0.240921  [22432/42552]\n",
      "loss: 0.168202  [25632/42552]\n",
      "loss: 0.290174  [28832/42552]\n",
      "loss: 0.394987  [32032/42552]\n",
      "loss: 0.311226  [35232/42552]\n",
      "loss: 0.238967  [38432/42552]\n",
      "loss: 0.368828  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.272214 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.299872  [   32/42552]\n",
      "loss: 0.197954  [ 3232/42552]\n",
      "loss: 0.287022  [ 6432/42552]\n",
      "loss: 0.275437  [ 9632/42552]\n",
      "loss: 0.505551  [12832/42552]\n",
      "loss: 0.327062  [16032/42552]\n",
      "loss: 0.346642  [19232/42552]\n",
      "loss: 0.425006  [22432/42552]\n",
      "loss: 0.360554  [25632/42552]\n",
      "loss: 0.349719  [28832/42552]\n",
      "loss: 0.418927  [32032/42552]\n",
      "loss: 0.325813  [35232/42552]\n",
      "loss: 0.201358  [38432/42552]\n",
      "loss: 0.290866  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.270637 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.314735  [   32/42552]\n",
      "loss: 0.272316  [ 3232/42552]\n",
      "loss: 0.294368  [ 6432/42552]\n",
      "loss: 0.297611  [ 9632/42552]\n",
      "loss: 0.194550  [12832/42552]\n",
      "loss: 0.397385  [16032/42552]\n",
      "loss: 0.310052  [19232/42552]\n",
      "loss: 0.433507  [22432/42552]\n",
      "loss: 0.257904  [25632/42552]\n",
      "loss: 0.438308  [28832/42552]\n",
      "loss: 0.362737  [32032/42552]\n",
      "loss: 0.364763  [35232/42552]\n",
      "loss: 0.329688  [38432/42552]\n",
      "loss: 0.375998  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.265287 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.266641  [   32/42552]\n",
      "loss: 0.327312  [ 3232/42552]\n",
      "loss: 0.292207  [ 6432/42552]\n",
      "loss: 0.341140  [ 9632/42552]\n",
      "loss: 0.326533  [12832/42552]\n",
      "loss: 0.256984  [16032/42552]\n",
      "loss: 0.270681  [19232/42552]\n",
      "loss: 0.382482  [22432/42552]\n",
      "loss: 0.285034  [25632/42552]\n",
      "loss: 0.237039  [28832/42552]\n",
      "loss: 0.259538  [32032/42552]\n",
      "loss: 0.213661  [35232/42552]\n",
      "loss: 0.316445  [38432/42552]\n",
      "loss: 0.305895  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.271111 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.435879  [   32/42552]\n",
      "loss: 0.371858  [ 3232/42552]\n",
      "loss: 0.255612  [ 6432/42552]\n",
      "loss: 0.247563  [ 9632/42552]\n",
      "loss: 0.243540  [12832/42552]\n",
      "loss: 0.315702  [16032/42552]\n",
      "loss: 0.303739  [19232/42552]\n",
      "loss: 0.388406  [22432/42552]\n",
      "loss: 0.249732  [25632/42552]\n",
      "loss: 0.391188  [28832/42552]\n",
      "loss: 0.283951  [32032/42552]\n",
      "loss: 0.317525  [35232/42552]\n",
      "loss: 0.177299  [38432/42552]\n",
      "loss: 0.452193  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.271074 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.253419  [   32/42552]\n",
      "loss: 0.435441  [ 3232/42552]\n",
      "loss: 0.221835  [ 6432/42552]\n",
      "loss: 0.334759  [ 9632/42552]\n",
      "loss: 0.333279  [12832/42552]\n",
      "loss: 0.287278  [16032/42552]\n",
      "loss: 0.356125  [19232/42552]\n",
      "loss: 0.392230  [22432/42552]\n",
      "loss: 0.249285  [25632/42552]\n",
      "loss: 0.241835  [28832/42552]\n",
      "loss: 0.220163  [32032/42552]\n",
      "loss: 0.367927  [35232/42552]\n",
      "loss: 0.192175  [38432/42552]\n",
      "loss: 0.244140  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.269226 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.335017  [   32/42552]\n",
      "loss: 0.335322  [ 3232/42552]\n",
      "loss: 0.252984  [ 6432/42552]\n",
      "loss: 0.338252  [ 9632/42552]\n",
      "loss: 0.398258  [12832/42552]\n",
      "loss: 0.207860  [16032/42552]\n",
      "loss: 0.251818  [19232/42552]\n",
      "loss: 0.192871  [22432/42552]\n",
      "loss: 0.409522  [25632/42552]\n",
      "loss: 0.306721  [28832/42552]\n",
      "loss: 0.358096  [32032/42552]\n",
      "loss: 0.282102  [35232/42552]\n",
      "loss: 0.274105  [38432/42552]\n",
      "loss: 0.407487  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.264133 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.369841  [   32/42552]\n",
      "loss: 0.238979  [ 3232/42552]\n",
      "loss: 0.368576  [ 6432/42552]\n",
      "loss: 0.330672  [ 9632/42552]\n",
      "loss: 0.240451  [12832/42552]\n",
      "loss: 0.393025  [16032/42552]\n",
      "loss: 0.372025  [19232/42552]\n",
      "loss: 0.340907  [22432/42552]\n",
      "loss: 0.337253  [25632/42552]\n",
      "loss: 0.361489  [28832/42552]\n",
      "loss: 0.358575  [32032/42552]\n",
      "loss: 0.258683  [35232/42552]\n",
      "loss: 0.232853  [38432/42552]\n",
      "loss: 0.574904  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.267979 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.415459  [   32/42552]\n",
      "loss: 0.501838  [ 3232/42552]\n",
      "loss: 0.308167  [ 6432/42552]\n",
      "loss: 0.295717  [ 9632/42552]\n",
      "loss: 0.418382  [12832/42552]\n",
      "loss: 0.220118  [16032/42552]\n",
      "loss: 0.208424  [19232/42552]\n",
      "loss: 0.304670  [22432/42552]\n",
      "loss: 0.328641  [25632/42552]\n",
      "loss: 0.292129  [28832/42552]\n",
      "loss: 0.350783  [32032/42552]\n",
      "loss: 0.351981  [35232/42552]\n",
      "loss: 0.286212  [38432/42552]\n",
      "loss: 0.301529  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.277167 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.328096  [   32/42552]\n",
      "loss: 0.245466  [ 3232/42552]\n",
      "loss: 0.184861  [ 6432/42552]\n",
      "loss: 0.484665  [ 9632/42552]\n",
      "loss: 0.136586  [12832/42552]\n",
      "loss: 0.326611  [16032/42552]\n",
      "loss: 0.238473  [19232/42552]\n",
      "loss: 0.192341  [22432/42552]\n",
      "loss: 0.258470  [25632/42552]\n",
      "loss: 0.514207  [28832/42552]\n",
      "loss: 0.306910  [32032/42552]\n",
      "loss: 0.309078  [35232/42552]\n",
      "loss: 0.216673  [38432/42552]\n",
      "loss: 0.407641  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.269661 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.255483  [   32/42552]\n",
      "loss: 0.328907  [ 3232/42552]\n",
      "loss: 0.374507  [ 6432/42552]\n",
      "loss: 0.356886  [ 9632/42552]\n",
      "loss: 0.265694  [12832/42552]\n",
      "loss: 0.290264  [16032/42552]\n",
      "loss: 0.346272  [19232/42552]\n",
      "loss: 0.404858  [22432/42552]\n",
      "loss: 0.261795  [25632/42552]\n",
      "loss: 0.275988  [28832/42552]\n",
      "loss: 0.280214  [32032/42552]\n",
      "loss: 0.288753  [35232/42552]\n",
      "loss: 0.453443  [38432/42552]\n",
      "loss: 0.401524  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.267502 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.196132  [   32/42552]\n",
      "loss: 0.268642  [ 3232/42552]\n",
      "loss: 0.240939  [ 6432/42552]\n",
      "loss: 0.290735  [ 9632/42552]\n",
      "loss: 0.325592  [12832/42552]\n",
      "loss: 0.210384  [16032/42552]\n",
      "loss: 0.285365  [19232/42552]\n",
      "loss: 0.253295  [22432/42552]\n",
      "loss: 0.279755  [25632/42552]\n",
      "loss: 0.407782  [28832/42552]\n",
      "loss: 0.283058  [32032/42552]\n",
      "loss: 0.341140  [35232/42552]\n",
      "loss: 0.291532  [38432/42552]\n",
      "loss: 0.286071  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.268845 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.354821  [   32/42552]\n",
      "loss: 0.259866  [ 3232/42552]\n",
      "loss: 0.236360  [ 6432/42552]\n",
      "loss: 0.359868  [ 9632/42552]\n",
      "loss: 0.413837  [12832/42552]\n",
      "loss: 0.310371  [16032/42552]\n",
      "loss: 0.252565  [19232/42552]\n",
      "loss: 0.248636  [22432/42552]\n",
      "loss: 0.379306  [25632/42552]\n",
      "loss: 0.497879  [28832/42552]\n",
      "loss: 0.362085  [32032/42552]\n",
      "loss: 0.307384  [35232/42552]\n",
      "loss: 0.228150  [38432/42552]\n",
      "loss: 0.284005  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.269610 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.297855  [   32/42552]\n",
      "loss: 0.208148  [ 3232/42552]\n",
      "loss: 0.301675  [ 6432/42552]\n",
      "loss: 0.236924  [ 9632/42552]\n",
      "loss: 0.510132  [12832/42552]\n",
      "loss: 0.411926  [16032/42552]\n",
      "loss: 0.236238  [19232/42552]\n",
      "loss: 0.335230  [22432/42552]\n",
      "loss: 0.518518  [25632/42552]\n",
      "loss: 0.259695  [28832/42552]\n",
      "loss: 0.319503  [32032/42552]\n",
      "loss: 0.287402  [35232/42552]\n",
      "loss: 0.219898  [38432/42552]\n",
      "loss: 0.507119  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.274671 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.269046  [   32/42552]\n",
      "loss: 0.229957  [ 3232/42552]\n",
      "loss: 0.425952  [ 6432/42552]\n",
      "loss: 0.337700  [ 9632/42552]\n",
      "loss: 0.367933  [12832/42552]\n",
      "loss: 0.383822  [16032/42552]\n",
      "loss: 0.333229  [19232/42552]\n",
      "loss: 0.390323  [22432/42552]\n",
      "loss: 0.348467  [25632/42552]\n",
      "loss: 0.376033  [28832/42552]\n",
      "loss: 0.337818  [32032/42552]\n",
      "loss: 0.313358  [35232/42552]\n",
      "loss: 0.269759  [38432/42552]\n",
      "loss: 0.228481  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.268506 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.315796  [   32/42552]\n",
      "loss: 0.318817  [ 3232/42552]\n",
      "loss: 0.280809  [ 6432/42552]\n",
      "loss: 0.432129  [ 9632/42552]\n",
      "loss: 0.286643  [12832/42552]\n",
      "loss: 0.321452  [16032/42552]\n",
      "loss: 0.337182  [19232/42552]\n",
      "loss: 0.349506  [22432/42552]\n",
      "loss: 0.261263  [25632/42552]\n",
      "loss: 0.286189  [28832/42552]\n",
      "loss: 0.315811  [32032/42552]\n",
      "loss: 0.287429  [35232/42552]\n",
      "loss: 0.220619  [38432/42552]\n",
      "loss: 0.303624  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.265172 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.240088  [   32/42552]\n",
      "loss: 0.271265  [ 3232/42552]\n",
      "loss: 0.382172  [ 6432/42552]\n",
      "loss: 0.330220  [ 9632/42552]\n",
      "loss: 0.291052  [12832/42552]\n",
      "loss: 0.150698  [16032/42552]\n",
      "loss: 0.307832  [19232/42552]\n",
      "loss: 0.342169  [22432/42552]\n",
      "loss: 0.373060  [25632/42552]\n",
      "loss: 0.249247  [28832/42552]\n",
      "loss: 0.340670  [32032/42552]\n",
      "loss: 0.299611  [35232/42552]\n",
      "loss: 0.216346  [38432/42552]\n",
      "loss: 0.311784  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.279354 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.321980  [   32/42552]\n",
      "loss: 0.368081  [ 3232/42552]\n",
      "loss: 0.543056  [ 6432/42552]\n",
      "loss: 0.274781  [ 9632/42552]\n",
      "loss: 0.329491  [12832/42552]\n",
      "loss: 0.298052  [16032/42552]\n",
      "loss: 0.292462  [19232/42552]\n",
      "loss: 0.387253  [22432/42552]\n",
      "loss: 0.368697  [25632/42552]\n",
      "loss: 0.323731  [28832/42552]\n",
      "loss: 0.290374  [32032/42552]\n",
      "loss: 0.287847  [35232/42552]\n",
      "loss: 0.237371  [38432/42552]\n",
      "loss: 0.327272  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.275421 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.270764  [   32/42552]\n",
      "loss: 0.347155  [ 3232/42552]\n",
      "loss: 0.298783  [ 6432/42552]\n",
      "loss: 0.299617  [ 9632/42552]\n",
      "loss: 0.304970  [12832/42552]\n",
      "loss: 0.373315  [16032/42552]\n",
      "loss: 0.299669  [19232/42552]\n",
      "loss: 0.414556  [22432/42552]\n",
      "loss: 0.596922  [25632/42552]\n",
      "loss: 0.280587  [28832/42552]\n",
      "loss: 0.277415  [32032/42552]\n",
      "loss: 0.244843  [35232/42552]\n",
      "loss: 0.343450  [38432/42552]\n",
      "loss: 0.228195  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.267554 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.354815  [   32/42552]\n",
      "loss: 0.567923  [ 3232/42552]\n",
      "loss: 0.284844  [ 6432/42552]\n",
      "loss: 0.216632  [ 9632/42552]\n",
      "loss: 0.261505  [12832/42552]\n",
      "loss: 0.295696  [16032/42552]\n",
      "loss: 0.429899  [19232/42552]\n",
      "loss: 0.226494  [22432/42552]\n",
      "loss: 0.390463  [25632/42552]\n",
      "loss: 0.214114  [28832/42552]\n",
      "loss: 0.473271  [32032/42552]\n",
      "loss: 0.389576  [35232/42552]\n",
      "loss: 0.335640  [38432/42552]\n",
      "loss: 0.347705  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.272611 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.524267  [   32/42552]\n",
      "loss: 0.312219  [ 3232/42552]\n",
      "loss: 0.289705  [ 6432/42552]\n",
      "loss: 0.276849  [ 9632/42552]\n",
      "loss: 0.352031  [12832/42552]\n",
      "loss: 0.300408  [16032/42552]\n",
      "loss: 0.242400  [19232/42552]\n",
      "loss: 0.414393  [22432/42552]\n",
      "loss: 0.451673  [25632/42552]\n",
      "loss: 0.499818  [28832/42552]\n",
      "loss: 0.400982  [32032/42552]\n",
      "loss: 0.324697  [35232/42552]\n",
      "loss: 0.380892  [38432/42552]\n",
      "loss: 0.390094  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.266115 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.342209  [   32/42552]\n",
      "loss: 0.252239  [ 3232/42552]\n",
      "loss: 0.353349  [ 6432/42552]\n",
      "loss: 0.316345  [ 9632/42552]\n",
      "loss: 0.244965  [12832/42552]\n",
      "loss: 0.517915  [16032/42552]\n",
      "loss: 0.384454  [19232/42552]\n",
      "loss: 0.359806  [22432/42552]\n",
      "loss: 0.258420  [25632/42552]\n",
      "loss: 0.225420  [28832/42552]\n",
      "loss: 0.218573  [32032/42552]\n",
      "loss: 0.380790  [35232/42552]\n",
      "loss: 0.311183  [38432/42552]\n",
      "loss: 0.213255  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.262196 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.189137  [   32/42552]\n",
      "loss: 0.231765  [ 3232/42552]\n",
      "loss: 0.193410  [ 6432/42552]\n",
      "loss: 0.245673  [ 9632/42552]\n",
      "loss: 0.291450  [12832/42552]\n",
      "loss: 0.343466  [16032/42552]\n",
      "loss: 0.321634  [19232/42552]\n",
      "loss: 0.242639  [22432/42552]\n",
      "loss: 0.326947  [25632/42552]\n",
      "loss: 0.340853  [28832/42552]\n",
      "loss: 0.268892  [32032/42552]\n",
      "loss: 0.183611  [35232/42552]\n",
      "loss: 0.219198  [38432/42552]\n",
      "loss: 0.235676  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.259146 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.294231  [   32/42552]\n",
      "loss: 0.357914  [ 3232/42552]\n",
      "loss: 0.163263  [ 6432/42552]\n",
      "loss: 0.360580  [ 9632/42552]\n",
      "loss: 0.300524  [12832/42552]\n",
      "loss: 0.359495  [16032/42552]\n",
      "loss: 0.361983  [19232/42552]\n",
      "loss: 0.451550  [22432/42552]\n",
      "loss: 0.338198  [25632/42552]\n",
      "loss: 0.320175  [28832/42552]\n",
      "loss: 0.175965  [32032/42552]\n",
      "loss: 0.541819  [35232/42552]\n",
      "loss: 0.329545  [38432/42552]\n",
      "loss: 0.184272  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.266710 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.298030  [   32/42552]\n",
      "loss: 0.298560  [ 3232/42552]\n",
      "loss: 0.245724  [ 6432/42552]\n",
      "loss: 0.260676  [ 9632/42552]\n",
      "loss: 0.262305  [12832/42552]\n",
      "loss: 0.327586  [16032/42552]\n",
      "loss: 0.247126  [19232/42552]\n",
      "loss: 0.439229  [22432/42552]\n",
      "loss: 0.228369  [25632/42552]\n",
      "loss: 0.288732  [28832/42552]\n",
      "loss: 0.277973  [32032/42552]\n",
      "loss: 0.324736  [35232/42552]\n",
      "loss: 0.300527  [38432/42552]\n",
      "loss: 0.387100  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.261543 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.241237  [   32/42552]\n",
      "loss: 0.336109  [ 3232/42552]\n",
      "loss: 0.303294  [ 6432/42552]\n",
      "loss: 0.301845  [ 9632/42552]\n",
      "loss: 0.284876  [12832/42552]\n",
      "loss: 0.165336  [16032/42552]\n",
      "loss: 0.289082  [19232/42552]\n",
      "loss: 0.304811  [22432/42552]\n",
      "loss: 0.262864  [25632/42552]\n",
      "loss: 0.226264  [28832/42552]\n",
      "loss: 0.221957  [32032/42552]\n",
      "loss: 0.320777  [35232/42552]\n",
      "loss: 0.392754  [38432/42552]\n",
      "loss: 0.250809  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.272815 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.245809  [   32/42552]\n",
      "loss: 0.363973  [ 3232/42552]\n",
      "loss: 0.342275  [ 6432/42552]\n",
      "loss: 0.284790  [ 9632/42552]\n",
      "loss: 0.312373  [12832/42552]\n",
      "loss: 0.218280  [16032/42552]\n",
      "loss: 0.334733  [19232/42552]\n",
      "loss: 0.398988  [22432/42552]\n",
      "loss: 0.316387  [25632/42552]\n",
      "loss: 0.499055  [28832/42552]\n",
      "loss: 0.486210  [32032/42552]\n",
      "loss: 0.395946  [35232/42552]\n",
      "loss: 0.269494  [38432/42552]\n",
      "loss: 0.369608  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.276119 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.383721  [   32/42552]\n",
      "loss: 0.416765  [ 3232/42552]\n",
      "loss: 0.431574  [ 6432/42552]\n",
      "loss: 0.263563  [ 9632/42552]\n",
      "loss: 0.221058  [12832/42552]\n",
      "loss: 0.253886  [16032/42552]\n",
      "loss: 0.225210  [19232/42552]\n",
      "loss: 0.500879  [22432/42552]\n",
      "loss: 0.366325  [25632/42552]\n",
      "loss: 0.264372  [28832/42552]\n",
      "loss: 0.290440  [32032/42552]\n",
      "loss: 0.370659  [35232/42552]\n",
      "loss: 0.228783  [38432/42552]\n",
      "loss: 0.482609  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.269598 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.275267  [   32/42552]\n",
      "loss: 0.346847  [ 3232/42552]\n",
      "loss: 0.381773  [ 6432/42552]\n",
      "loss: 0.257596  [ 9632/42552]\n",
      "loss: 0.242142  [12832/42552]\n",
      "loss: 0.238736  [16032/42552]\n",
      "loss: 0.421064  [19232/42552]\n",
      "loss: 0.415783  [22432/42552]\n",
      "loss: 0.205707  [25632/42552]\n",
      "loss: 0.308345  [28832/42552]\n",
      "loss: 0.266482  [32032/42552]\n",
      "loss: 0.171038  [35232/42552]\n",
      "loss: 0.265412  [38432/42552]\n",
      "loss: 0.251299  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.264587 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.261526  [   32/42552]\n",
      "loss: 0.273731  [ 3232/42552]\n",
      "loss: 0.220991  [ 6432/42552]\n",
      "loss: 0.235348  [ 9632/42552]\n",
      "loss: 0.310217  [12832/42552]\n",
      "loss: 0.328116  [16032/42552]\n",
      "loss: 0.260607  [19232/42552]\n",
      "loss: 0.373081  [22432/42552]\n",
      "loss: 0.206029  [25632/42552]\n",
      "loss: 0.232588  [28832/42552]\n",
      "loss: 0.270584  [32032/42552]\n",
      "loss: 0.212835  [35232/42552]\n",
      "loss: 0.438868  [38432/42552]\n",
      "loss: 0.796491  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.268326 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.202282  [   32/42552]\n",
      "loss: 0.254613  [ 3232/42552]\n",
      "loss: 0.169608  [ 6432/42552]\n",
      "loss: 0.255654  [ 9632/42552]\n",
      "loss: 0.245352  [12832/42552]\n",
      "loss: 0.439127  [16032/42552]\n",
      "loss: 0.173305  [19232/42552]\n",
      "loss: 0.208460  [22432/42552]\n",
      "loss: 0.297399  [25632/42552]\n",
      "loss: 0.346286  [28832/42552]\n",
      "loss: 0.224133  [32032/42552]\n",
      "loss: 0.252793  [35232/42552]\n",
      "loss: 0.254705  [38432/42552]\n",
      "loss: 0.227309  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.262527 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.257289  [   32/42552]\n",
      "loss: 0.265565  [ 3232/42552]\n",
      "loss: 0.231418  [ 6432/42552]\n",
      "loss: 0.297119  [ 9632/42552]\n",
      "loss: 0.219983  [12832/42552]\n",
      "loss: 0.395542  [16032/42552]\n",
      "loss: 0.234151  [19232/42552]\n",
      "loss: 0.265057  [22432/42552]\n",
      "loss: 0.325023  [25632/42552]\n",
      "loss: 0.202458  [28832/42552]\n",
      "loss: 0.325902  [32032/42552]\n",
      "loss: 0.284741  [35232/42552]\n",
      "loss: 0.210572  [38432/42552]\n",
      "loss: 0.225023  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.267000 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.186517  [   32/42552]\n",
      "loss: 0.361361  [ 3232/42552]\n",
      "loss: 0.296050  [ 6432/42552]\n",
      "loss: 0.297427  [ 9632/42552]\n",
      "loss: 0.351658  [12832/42552]\n",
      "loss: 0.305260  [16032/42552]\n",
      "loss: 0.378237  [19232/42552]\n",
      "loss: 0.554218  [22432/42552]\n",
      "loss: 0.271171  [25632/42552]\n",
      "loss: 0.280292  [28832/42552]\n",
      "loss: 0.436554  [32032/42552]\n",
      "loss: 0.230553  [35232/42552]\n",
      "loss: 0.405678  [38432/42552]\n",
      "loss: 0.319208  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.263057 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.329288  [   32/42552]\n",
      "loss: 0.251239  [ 3232/42552]\n",
      "loss: 0.256137  [ 6432/42552]\n",
      "loss: 0.246306  [ 9632/42552]\n",
      "loss: 0.199643  [12832/42552]\n",
      "loss: 0.320756  [16032/42552]\n",
      "loss: 0.265137  [19232/42552]\n",
      "loss: 0.358965  [22432/42552]\n",
      "loss: 0.349965  [25632/42552]\n",
      "loss: 0.575488  [28832/42552]\n",
      "loss: 0.535304  [32032/42552]\n",
      "loss: 0.206098  [35232/42552]\n",
      "loss: 0.217605  [38432/42552]\n",
      "loss: 0.210713  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.254721 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.317343  [   32/42552]\n",
      "loss: 0.304998  [ 3232/42552]\n",
      "loss: 0.274368  [ 6432/42552]\n",
      "loss: 0.372178  [ 9632/42552]\n",
      "loss: 0.211721  [12832/42552]\n",
      "loss: 0.384815  [16032/42552]\n",
      "loss: 0.318248  [19232/42552]\n",
      "loss: 0.293591  [22432/42552]\n",
      "loss: 0.397746  [25632/42552]\n",
      "loss: 0.354561  [28832/42552]\n",
      "loss: 0.269467  [32032/42552]\n",
      "loss: 0.375794  [35232/42552]\n",
      "loss: 0.344269  [38432/42552]\n",
      "loss: 0.280442  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.269477 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.275416  [   32/42552]\n",
      "loss: 0.255052  [ 3232/42552]\n",
      "loss: 0.339775  [ 6432/42552]\n",
      "loss: 0.282124  [ 9632/42552]\n",
      "loss: 0.227068  [12832/42552]\n",
      "loss: 0.339015  [16032/42552]\n",
      "loss: 0.391685  [19232/42552]\n",
      "loss: 0.296619  [22432/42552]\n",
      "loss: 0.226970  [25632/42552]\n",
      "loss: 0.459688  [28832/42552]\n",
      "loss: 0.243142  [32032/42552]\n",
      "loss: 0.357162  [35232/42552]\n",
      "loss: 0.270294  [38432/42552]\n",
      "loss: 0.355572  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.265238 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.269251  [   32/42552]\n",
      "loss: 0.254605  [ 3232/42552]\n",
      "loss: 0.452406  [ 6432/42552]\n",
      "loss: 0.378069  [ 9632/42552]\n",
      "loss: 0.326971  [12832/42552]\n",
      "loss: 0.228408  [16032/42552]\n",
      "loss: 0.270024  [19232/42552]\n",
      "loss: 0.328200  [22432/42552]\n",
      "loss: 0.287797  [25632/42552]\n",
      "loss: 0.355078  [28832/42552]\n",
      "loss: 0.360530  [32032/42552]\n",
      "loss: 0.281377  [35232/42552]\n",
      "loss: 0.237767  [38432/42552]\n",
      "loss: 0.307747  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.259844 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.253161  [   32/42552]\n",
      "loss: 0.271137  [ 3232/42552]\n",
      "loss: 0.390390  [ 6432/42552]\n",
      "loss: 0.292152  [ 9632/42552]\n",
      "loss: 0.270144  [12832/42552]\n",
      "loss: 0.400436  [16032/42552]\n",
      "loss: 0.271951  [19232/42552]\n",
      "loss: 0.197605  [22432/42552]\n",
      "loss: 0.226596  [25632/42552]\n",
      "loss: 0.432811  [28832/42552]\n",
      "loss: 0.324850  [32032/42552]\n",
      "loss: 0.306195  [35232/42552]\n",
      "loss: 0.417201  [38432/42552]\n",
      "loss: 0.323712  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.263719 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.178760  [   32/42552]\n",
      "loss: 0.363567  [ 3232/42552]\n",
      "loss: 0.190028  [ 6432/42552]\n",
      "loss: 0.345757  [ 9632/42552]\n",
      "loss: 0.239951  [12832/42552]\n",
      "loss: 0.406143  [16032/42552]\n",
      "loss: 0.180472  [19232/42552]\n",
      "loss: 0.185495  [22432/42552]\n",
      "loss: 0.359652  [25632/42552]\n",
      "loss: 0.354801  [28832/42552]\n",
      "loss: 0.219164  [32032/42552]\n",
      "loss: 0.275797  [35232/42552]\n",
      "loss: 0.323805  [38432/42552]\n",
      "loss: 0.290018  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.265337 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.366037  [   32/42552]\n",
      "loss: 0.143009  [ 3232/42552]\n",
      "loss: 0.286866  [ 6432/42552]\n",
      "loss: 0.391883  [ 9632/42552]\n",
      "loss: 0.308943  [12832/42552]\n",
      "loss: 0.195864  [16032/42552]\n",
      "loss: 0.263857  [19232/42552]\n",
      "loss: 0.385783  [22432/42552]\n",
      "loss: 0.250794  [25632/42552]\n",
      "loss: 0.418160  [28832/42552]\n",
      "loss: 0.295982  [32032/42552]\n",
      "loss: 0.344238  [35232/42552]\n",
      "loss: 0.293553  [38432/42552]\n",
      "loss: 0.230975  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.266777 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.354126  [   32/42552]\n",
      "loss: 0.545803  [ 3232/42552]\n",
      "loss: 0.459513  [ 6432/42552]\n",
      "loss: 0.318143  [ 9632/42552]\n",
      "loss: 0.369633  [12832/42552]\n",
      "loss: 0.261469  [16032/42552]\n",
      "loss: 0.367223  [19232/42552]\n",
      "loss: 0.191108  [22432/42552]\n",
      "loss: 0.303143  [25632/42552]\n",
      "loss: 0.390957  [28832/42552]\n",
      "loss: 0.233170  [32032/42552]\n",
      "loss: 0.243505  [35232/42552]\n",
      "loss: 0.312664  [38432/42552]\n",
      "loss: 0.367612  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.261346 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.354273  [   32/42552]\n",
      "loss: 0.208825  [ 3232/42552]\n",
      "loss: 0.293004  [ 6432/42552]\n",
      "loss: 0.348477  [ 9632/42552]\n",
      "loss: 0.294538  [12832/42552]\n",
      "loss: 0.221641  [16032/42552]\n",
      "loss: 0.240118  [19232/42552]\n",
      "loss: 0.306855  [22432/42552]\n",
      "loss: 0.264780  [25632/42552]\n",
      "loss: 0.205638  [28832/42552]\n",
      "loss: 0.271004  [32032/42552]\n",
      "loss: 0.199552  [35232/42552]\n",
      "loss: 0.269518  [38432/42552]\n",
      "loss: 0.249644  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.267991 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.237774  [   32/42552]\n",
      "loss: 0.331598  [ 3232/42552]\n",
      "loss: 0.328749  [ 6432/42552]\n",
      "loss: 0.323745  [ 9632/42552]\n",
      "loss: 0.219534  [12832/42552]\n",
      "loss: 0.355273  [16032/42552]\n",
      "loss: 0.424182  [19232/42552]\n",
      "loss: 0.316814  [22432/42552]\n",
      "loss: 0.332565  [25632/42552]\n",
      "loss: 0.259145  [28832/42552]\n",
      "loss: 0.325020  [32032/42552]\n",
      "loss: 0.307189  [35232/42552]\n",
      "loss: 0.216076  [38432/42552]\n",
      "loss: 0.337569  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.259812 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.386721  [   32/42552]\n",
      "loss: 0.262327  [ 3232/42552]\n",
      "loss: 0.331684  [ 6432/42552]\n",
      "loss: 0.425510  [ 9632/42552]\n",
      "loss: 0.368166  [12832/42552]\n",
      "loss: 0.442543  [16032/42552]\n",
      "loss: 0.321070  [19232/42552]\n",
      "loss: 0.366244  [22432/42552]\n",
      "loss: 0.333730  [25632/42552]\n",
      "loss: 0.331513  [28832/42552]\n",
      "loss: 0.216369  [32032/42552]\n",
      "loss: 0.232748  [35232/42552]\n",
      "loss: 0.440164  [38432/42552]\n",
      "loss: 0.413571  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.261084 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.338085  [   32/42552]\n",
      "loss: 0.464087  [ 3232/42552]\n",
      "loss: 0.300891  [ 6432/42552]\n",
      "loss: 0.291826  [ 9632/42552]\n",
      "loss: 0.271652  [12832/42552]\n",
      "loss: 0.194095  [16032/42552]\n",
      "loss: 0.152977  [19232/42552]\n",
      "loss: 0.315855  [22432/42552]\n",
      "loss: 0.145426  [25632/42552]\n",
      "loss: 0.320212  [28832/42552]\n",
      "loss: 0.347689  [32032/42552]\n",
      "loss: 0.307227  [35232/42552]\n",
      "loss: 0.375781  [38432/42552]\n",
      "loss: 0.321697  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.263792 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.258998  [   32/42552]\n",
      "loss: 0.240714  [ 3232/42552]\n",
      "loss: 0.272913  [ 6432/42552]\n",
      "loss: 0.244908  [ 9632/42552]\n",
      "loss: 0.260877  [12832/42552]\n",
      "loss: 0.288364  [16032/42552]\n",
      "loss: 0.385780  [19232/42552]\n",
      "loss: 0.303615  [22432/42552]\n",
      "loss: 0.371406  [25632/42552]\n",
      "loss: 0.369596  [28832/42552]\n",
      "loss: 0.310946  [32032/42552]\n",
      "loss: 0.384049  [35232/42552]\n",
      "loss: 0.196648  [38432/42552]\n",
      "loss: 0.393716  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.257308 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.199851  [   32/42552]\n",
      "loss: 0.298164  [ 3232/42552]\n",
      "loss: 0.221355  [ 6432/42552]\n",
      "loss: 0.358324  [ 9632/42552]\n",
      "loss: 0.212921  [12832/42552]\n",
      "loss: 0.284216  [16032/42552]\n",
      "loss: 0.277978  [19232/42552]\n",
      "loss: 0.256448  [22432/42552]\n",
      "loss: 0.369339  [25632/42552]\n",
      "loss: 0.299006  [28832/42552]\n",
      "loss: 0.422565  [32032/42552]\n",
      "loss: 0.184314  [35232/42552]\n",
      "loss: 0.341768  [38432/42552]\n",
      "loss: 0.227434  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.255914 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.325362  [   32/42552]\n",
      "loss: 0.405140  [ 3232/42552]\n",
      "loss: 0.234623  [ 6432/42552]\n",
      "loss: 0.383666  [ 9632/42552]\n",
      "loss: 0.175411  [12832/42552]\n",
      "loss: 0.231991  [16032/42552]\n",
      "loss: 0.281013  [19232/42552]\n",
      "loss: 0.176802  [22432/42552]\n",
      "loss: 0.170626  [25632/42552]\n",
      "loss: 0.399548  [28832/42552]\n",
      "loss: 0.380140  [32032/42552]\n",
      "loss: 0.330695  [35232/42552]\n",
      "loss: 0.452229  [38432/42552]\n",
      "loss: 0.256868  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.260549 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.236408  [   32/42552]\n",
      "loss: 0.234353  [ 3232/42552]\n",
      "loss: 0.259718  [ 6432/42552]\n",
      "loss: 0.400927  [ 9632/42552]\n",
      "loss: 0.234768  [12832/42552]\n",
      "loss: 0.233681  [16032/42552]\n",
      "loss: 0.355058  [19232/42552]\n",
      "loss: 0.236378  [22432/42552]\n",
      "loss: 0.461212  [25632/42552]\n",
      "loss: 0.392492  [28832/42552]\n",
      "loss: 0.264587  [32032/42552]\n",
      "loss: 0.460526  [35232/42552]\n",
      "loss: 0.259194  [38432/42552]\n",
      "loss: 0.453926  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.263900 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.266466  [   32/42552]\n",
      "loss: 0.240084  [ 3232/42552]\n",
      "loss: 0.328018  [ 6432/42552]\n",
      "loss: 0.250454  [ 9632/42552]\n",
      "loss: 0.359912  [12832/42552]\n",
      "loss: 0.139680  [16032/42552]\n",
      "loss: 0.325140  [19232/42552]\n",
      "loss: 0.460430  [22432/42552]\n",
      "loss: 0.380025  [25632/42552]\n",
      "loss: 0.314661  [28832/42552]\n",
      "loss: 0.152042  [32032/42552]\n",
      "loss: 0.328985  [35232/42552]\n",
      "loss: 0.376172  [38432/42552]\n",
      "loss: 0.366932  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.255000 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.418147  [   32/42552]\n",
      "loss: 0.339340  [ 3232/42552]\n",
      "loss: 0.363332  [ 6432/42552]\n",
      "loss: 0.299400  [ 9632/42552]\n",
      "loss: 0.355715  [12832/42552]\n",
      "loss: 0.281136  [16032/42552]\n",
      "loss: 0.239838  [19232/42552]\n",
      "loss: 0.253504  [22432/42552]\n",
      "loss: 0.382404  [25632/42552]\n",
      "loss: 0.371814  [28832/42552]\n",
      "loss: 0.392682  [32032/42552]\n",
      "loss: 0.487170  [35232/42552]\n",
      "loss: 0.279988  [38432/42552]\n",
      "loss: 0.301823  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.268536 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.327462  [   32/42552]\n",
      "loss: 0.199332  [ 3232/42552]\n",
      "loss: 0.241525  [ 6432/42552]\n",
      "loss: 0.431200  [ 9632/42552]\n",
      "loss: 0.337177  [12832/42552]\n",
      "loss: 0.193593  [16032/42552]\n",
      "loss: 0.287652  [19232/42552]\n",
      "loss: 0.217359  [22432/42552]\n",
      "loss: 0.432576  [25632/42552]\n",
      "loss: 0.222238  [28832/42552]\n",
      "loss: 0.246553  [32032/42552]\n",
      "loss: 0.313518  [35232/42552]\n",
      "loss: 0.265778  [38432/42552]\n",
      "loss: 0.171893  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.256523 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.382612  [   32/42552]\n",
      "loss: 0.214768  [ 3232/42552]\n",
      "loss: 0.397279  [ 6432/42552]\n",
      "loss: 0.257559  [ 9632/42552]\n",
      "loss: 0.210190  [12832/42552]\n",
      "loss: 0.355410  [16032/42552]\n",
      "loss: 0.359927  [19232/42552]\n",
      "loss: 0.276346  [22432/42552]\n",
      "loss: 0.217562  [25632/42552]\n",
      "loss: 0.166804  [28832/42552]\n",
      "loss: 0.365668  [32032/42552]\n",
      "loss: 0.354598  [35232/42552]\n",
      "loss: 0.269196  [38432/42552]\n",
      "loss: 0.315307  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.254085 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.283865  [   32/42552]\n",
      "loss: 0.293268  [ 3232/42552]\n",
      "loss: 0.313942  [ 6432/42552]\n",
      "loss: 0.333286  [ 9632/42552]\n",
      "loss: 0.304384  [12832/42552]\n",
      "loss: 0.243561  [16032/42552]\n",
      "loss: 0.402251  [19232/42552]\n",
      "loss: 0.310778  [22432/42552]\n",
      "loss: 0.288132  [25632/42552]\n",
      "loss: 0.280738  [28832/42552]\n",
      "loss: 0.105145  [32032/42552]\n",
      "loss: 0.375138  [35232/42552]\n",
      "loss: 0.285410  [38432/42552]\n",
      "loss: 0.340114  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.258254 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.232112  [   32/42552]\n",
      "loss: 0.240592  [ 3232/42552]\n",
      "loss: 0.290673  [ 6432/42552]\n",
      "loss: 0.398440  [ 9632/42552]\n",
      "loss: 0.380458  [12832/42552]\n",
      "loss: 0.278409  [16032/42552]\n",
      "loss: 0.241447  [19232/42552]\n",
      "loss: 0.240184  [22432/42552]\n",
      "loss: 0.288650  [25632/42552]\n",
      "loss: 0.333837  [28832/42552]\n",
      "loss: 0.245178  [32032/42552]\n",
      "loss: 0.345090  [35232/42552]\n",
      "loss: 0.540994  [38432/42552]\n",
      "loss: 0.274395  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.257793 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.296541  [   32/42552]\n",
      "loss: 0.214735  [ 3232/42552]\n",
      "loss: 0.252357  [ 6432/42552]\n",
      "loss: 0.331367  [ 9632/42552]\n",
      "loss: 0.237571  [12832/42552]\n",
      "loss: 0.453995  [16032/42552]\n",
      "loss: 0.303966  [19232/42552]\n",
      "loss: 0.272208  [22432/42552]\n",
      "loss: 0.296737  [25632/42552]\n",
      "loss: 0.216905  [28832/42552]\n",
      "loss: 0.305230  [32032/42552]\n",
      "loss: 0.266641  [35232/42552]\n",
      "loss: 0.202660  [38432/42552]\n",
      "loss: 0.326861  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.261491 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.183642  [   32/42552]\n",
      "loss: 0.233074  [ 3232/42552]\n",
      "loss: 0.234912  [ 6432/42552]\n",
      "loss: 0.421129  [ 9632/42552]\n",
      "loss: 0.533184  [12832/42552]\n",
      "loss: 0.235364  [16032/42552]\n",
      "loss: 0.436110  [19232/42552]\n",
      "loss: 0.320055  [22432/42552]\n",
      "loss: 0.249422  [25632/42552]\n",
      "loss: 0.152647  [28832/42552]\n",
      "loss: 0.254188  [32032/42552]\n",
      "loss: 0.292467  [35232/42552]\n",
      "loss: 0.583513  [38432/42552]\n",
      "loss: 0.139124  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.258236 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.223962  [   32/42552]\n",
      "loss: 0.280348  [ 3232/42552]\n",
      "loss: 0.345925  [ 6432/42552]\n",
      "loss: 0.332560  [ 9632/42552]\n",
      "loss: 0.322569  [12832/42552]\n",
      "loss: 0.268676  [16032/42552]\n",
      "loss: 0.294828  [19232/42552]\n",
      "loss: 0.300340  [22432/42552]\n",
      "loss: 0.260881  [25632/42552]\n",
      "loss: 0.305960  [28832/42552]\n",
      "loss: 0.225861  [32032/42552]\n",
      "loss: 0.387761  [35232/42552]\n",
      "loss: 0.324241  [38432/42552]\n",
      "loss: 0.234891  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.253946 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.307361  [   32/42552]\n",
      "loss: 0.244012  [ 3232/42552]\n",
      "loss: 0.265713  [ 6432/42552]\n",
      "loss: 0.266361  [ 9632/42552]\n",
      "loss: 0.298889  [12832/42552]\n",
      "loss: 0.272236  [16032/42552]\n",
      "loss: 0.326160  [19232/42552]\n",
      "loss: 0.253227  [22432/42552]\n",
      "loss: 0.262337  [25632/42552]\n",
      "loss: 0.328134  [28832/42552]\n",
      "loss: 0.217581  [32032/42552]\n",
      "loss: 0.227234  [35232/42552]\n",
      "loss: 0.242820  [38432/42552]\n",
      "loss: 0.223803  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.257335 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.406624  [   32/42552]\n",
      "loss: 0.199161  [ 3232/42552]\n",
      "loss: 0.348505  [ 6432/42552]\n",
      "loss: 0.336381  [ 9632/42552]\n",
      "loss: 0.268059  [12832/42552]\n",
      "loss: 0.366892  [16032/42552]\n",
      "loss: 0.288697  [19232/42552]\n",
      "loss: 0.189423  [22432/42552]\n",
      "loss: 0.377389  [25632/42552]\n",
      "loss: 0.472848  [28832/42552]\n",
      "loss: 0.206095  [32032/42552]\n",
      "loss: 0.339020  [35232/42552]\n",
      "loss: 0.280472  [38432/42552]\n",
      "loss: 0.255046  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.256025 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.191938  [   32/42552]\n",
      "loss: 0.243022  [ 3232/42552]\n",
      "loss: 0.426185  [ 6432/42552]\n",
      "loss: 0.240867  [ 9632/42552]\n",
      "loss: 0.239156  [12832/42552]\n",
      "loss: 0.234016  [16032/42552]\n",
      "loss: 0.285476  [19232/42552]\n",
      "loss: 0.282219  [22432/42552]\n",
      "loss: 0.230884  [25632/42552]\n",
      "loss: 0.177710  [28832/42552]\n",
      "loss: 0.294818  [32032/42552]\n",
      "loss: 0.309422  [35232/42552]\n",
      "loss: 0.278251  [38432/42552]\n",
      "loss: 0.185365  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.263795 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.316494  [   32/42552]\n",
      "loss: 0.494839  [ 3232/42552]\n",
      "loss: 0.239141  [ 6432/42552]\n",
      "loss: 0.285507  [ 9632/42552]\n",
      "loss: 0.397584  [12832/42552]\n",
      "loss: 0.464813  [16032/42552]\n",
      "loss: 0.216750  [19232/42552]\n",
      "loss: 0.293189  [22432/42552]\n",
      "loss: 0.229047  [25632/42552]\n",
      "loss: 0.337307  [28832/42552]\n",
      "loss: 0.234596  [32032/42552]\n",
      "loss: 0.191132  [35232/42552]\n",
      "loss: 0.210177  [38432/42552]\n",
      "loss: 0.397257  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.275003 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.209307  [   32/42552]\n",
      "loss: 0.274179  [ 3232/42552]\n",
      "loss: 0.322465  [ 6432/42552]\n",
      "loss: 0.310993  [ 9632/42552]\n",
      "loss: 0.277089  [12832/42552]\n",
      "loss: 0.235748  [16032/42552]\n",
      "loss: 0.379426  [19232/42552]\n",
      "loss: 0.458355  [22432/42552]\n",
      "loss: 0.537571  [25632/42552]\n",
      "loss: 0.264331  [28832/42552]\n",
      "loss: 0.352107  [32032/42552]\n",
      "loss: 0.349202  [35232/42552]\n",
      "loss: 0.416867  [38432/42552]\n",
      "loss: 0.228886  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.260563 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.405061  [   32/42552]\n",
      "loss: 0.215444  [ 3232/42552]\n",
      "loss: 0.203965  [ 6432/42552]\n",
      "loss: 0.303389  [ 9632/42552]\n",
      "loss: 0.267979  [12832/42552]\n",
      "loss: 0.334019  [16032/42552]\n",
      "loss: 0.236708  [19232/42552]\n",
      "loss: 0.318506  [22432/42552]\n",
      "loss: 0.253410  [25632/42552]\n",
      "loss: 0.241833  [28832/42552]\n",
      "loss: 0.363065  [32032/42552]\n",
      "loss: 0.448360  [35232/42552]\n",
      "loss: 0.186796  [38432/42552]\n",
      "loss: 0.460792  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.260502 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.281697  [   32/42552]\n",
      "loss: 0.224591  [ 3232/42552]\n",
      "loss: 0.328573  [ 6432/42552]\n",
      "loss: 0.350178  [ 9632/42552]\n",
      "loss: 0.313660  [12832/42552]\n",
      "loss: 0.308471  [16032/42552]\n",
      "loss: 0.246471  [19232/42552]\n",
      "loss: 0.235732  [22432/42552]\n",
      "loss: 0.327098  [25632/42552]\n",
      "loss: 0.268233  [28832/42552]\n",
      "loss: 0.292588  [32032/42552]\n",
      "loss: 0.285761  [35232/42552]\n",
      "loss: 0.324706  [38432/42552]\n",
      "loss: 0.325416  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.256390 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.292428  [   32/42552]\n",
      "loss: 0.377349  [ 3232/42552]\n",
      "loss: 0.355014  [ 6432/42552]\n",
      "loss: 0.332123  [ 9632/42552]\n",
      "loss: 0.388748  [12832/42552]\n",
      "loss: 0.256140  [16032/42552]\n",
      "loss: 0.157923  [19232/42552]\n",
      "loss: 0.263910  [22432/42552]\n",
      "loss: 0.462409  [25632/42552]\n",
      "loss: 0.303887  [28832/42552]\n",
      "loss: 0.336546  [32032/42552]\n",
      "loss: 0.357692  [35232/42552]\n",
      "loss: 0.348960  [38432/42552]\n",
      "loss: 0.340456  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.263415 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.373560  [   32/42552]\n",
      "loss: 0.258232  [ 3232/42552]\n",
      "loss: 0.292962  [ 6432/42552]\n",
      "loss: 0.251416  [ 9632/42552]\n",
      "loss: 0.356095  [12832/42552]\n",
      "loss: 0.173779  [16032/42552]\n",
      "loss: 0.313660  [19232/42552]\n",
      "loss: 0.280991  [22432/42552]\n",
      "loss: 0.246862  [25632/42552]\n",
      "loss: 0.231593  [28832/42552]\n",
      "loss: 0.239031  [32032/42552]\n",
      "loss: 0.357378  [35232/42552]\n",
      "loss: 0.306912  [38432/42552]\n",
      "loss: 0.208741  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.258307 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.262433  [   32/42552]\n",
      "loss: 0.357336  [ 3232/42552]\n",
      "loss: 0.482872  [ 6432/42552]\n",
      "loss: 0.277120  [ 9632/42552]\n",
      "loss: 0.248150  [12832/42552]\n",
      "loss: 0.328550  [16032/42552]\n",
      "loss: 0.283124  [19232/42552]\n",
      "loss: 0.430993  [22432/42552]\n",
      "loss: 0.165859  [25632/42552]\n",
      "loss: 0.294833  [28832/42552]\n",
      "loss: 0.276170  [32032/42552]\n",
      "loss: 0.415645  [35232/42552]\n",
      "loss: 0.293848  [38432/42552]\n",
      "loss: 0.423844  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.260460 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.277496  [   32/42552]\n",
      "loss: 0.208744  [ 3232/42552]\n",
      "loss: 0.247840  [ 6432/42552]\n",
      "loss: 0.359831  [ 9632/42552]\n",
      "loss: 0.261828  [12832/42552]\n",
      "loss: 0.443887  [16032/42552]\n",
      "loss: 0.238393  [19232/42552]\n",
      "loss: 0.298106  [22432/42552]\n",
      "loss: 0.198168  [25632/42552]\n",
      "loss: 0.320301  [28832/42552]\n",
      "loss: 0.281291  [32032/42552]\n",
      "loss: 0.224434  [35232/42552]\n",
      "loss: 0.303759  [38432/42552]\n",
      "loss: 0.170699  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.261526 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.364032  [   32/42552]\n",
      "loss: 0.221577  [ 3232/42552]\n",
      "loss: 0.234580  [ 6432/42552]\n",
      "loss: 0.399842  [ 9632/42552]\n",
      "loss: 0.442202  [12832/42552]\n",
      "loss: 0.304638  [16032/42552]\n",
      "loss: 0.352905  [19232/42552]\n",
      "loss: 0.255372  [22432/42552]\n",
      "loss: 0.345967  [25632/42552]\n",
      "loss: 0.306792  [28832/42552]\n",
      "loss: 0.460724  [32032/42552]\n",
      "loss: 0.304834  [35232/42552]\n",
      "loss: 0.292861  [38432/42552]\n",
      "loss: 0.225715  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.254509 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.302005  [   32/42552]\n",
      "loss: 0.290296  [ 3232/42552]\n",
      "loss: 0.144745  [ 6432/42552]\n",
      "loss: 0.332931  [ 9632/42552]\n",
      "loss: 0.177892  [12832/42552]\n",
      "loss: 0.276886  [16032/42552]\n",
      "loss: 0.263249  [19232/42552]\n",
      "loss: 0.250726  [22432/42552]\n",
      "loss: 0.167545  [25632/42552]\n",
      "loss: 0.250186  [28832/42552]\n",
      "loss: 0.260031  [32032/42552]\n",
      "loss: 0.283399  [35232/42552]\n",
      "loss: 0.246894  [38432/42552]\n",
      "loss: 0.237996  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.254103 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.258103  [   32/42552]\n",
      "loss: 0.265022  [ 3232/42552]\n",
      "loss: 0.319734  [ 6432/42552]\n",
      "loss: 0.323712  [ 9632/42552]\n",
      "loss: 0.277220  [12832/42552]\n",
      "loss: 0.197078  [16032/42552]\n",
      "loss: 0.199376  [19232/42552]\n",
      "loss: 0.337142  [22432/42552]\n",
      "loss: 0.411484  [25632/42552]\n",
      "loss: 0.326619  [28832/42552]\n",
      "loss: 0.209219  [32032/42552]\n",
      "loss: 0.314847  [35232/42552]\n",
      "loss: 0.375883  [38432/42552]\n",
      "loss: 0.281942  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.254987 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.202392  [   32/42552]\n",
      "loss: 0.212283  [ 3232/42552]\n",
      "loss: 0.236619  [ 6432/42552]\n",
      "loss: 0.447454  [ 9632/42552]\n",
      "loss: 0.206082  [12832/42552]\n",
      "loss: 0.311734  [16032/42552]\n",
      "loss: 0.227639  [19232/42552]\n",
      "loss: 0.282783  [22432/42552]\n",
      "loss: 0.260337  [25632/42552]\n",
      "loss: 0.203014  [28832/42552]\n",
      "loss: 0.250404  [32032/42552]\n",
      "loss: 0.324574  [35232/42552]\n",
      "loss: 0.238438  [38432/42552]\n",
      "loss: 0.300942  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.251072 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.172323  [   32/42552]\n",
      "loss: 0.316302  [ 3232/42552]\n",
      "loss: 0.310783  [ 6432/42552]\n",
      "loss: 0.277090  [ 9632/42552]\n",
      "loss: 0.281188  [12832/42552]\n",
      "loss: 0.318999  [16032/42552]\n",
      "loss: 0.264693  [19232/42552]\n",
      "loss: 0.267927  [22432/42552]\n",
      "loss: 0.512645  [25632/42552]\n",
      "loss: 0.425861  [28832/42552]\n",
      "loss: 0.301199  [32032/42552]\n",
      "loss: 0.243947  [35232/42552]\n",
      "loss: 0.182320  [38432/42552]\n",
      "loss: 0.251660  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.253839 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.294207  [   32/42552]\n",
      "loss: 0.441880  [ 3232/42552]\n",
      "loss: 0.256329  [ 6432/42552]\n",
      "loss: 0.345595  [ 9632/42552]\n",
      "loss: 0.270372  [12832/42552]\n",
      "loss: 0.318655  [16032/42552]\n",
      "loss: 0.369061  [19232/42552]\n",
      "loss: 0.252416  [22432/42552]\n",
      "loss: 0.211464  [25632/42552]\n",
      "loss: 0.273020  [28832/42552]\n",
      "loss: 0.266347  [32032/42552]\n",
      "loss: 0.245572  [35232/42552]\n",
      "loss: 0.211159  [38432/42552]\n",
      "loss: 0.266961  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.250551 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.401457  [   32/42552]\n",
      "loss: 0.382471  [ 3232/42552]\n",
      "loss: 0.257011  [ 6432/42552]\n",
      "loss: 0.428793  [ 9632/42552]\n",
      "loss: 0.419915  [12832/42552]\n",
      "loss: 0.201734  [16032/42552]\n",
      "loss: 0.261289  [19232/42552]\n",
      "loss: 0.309115  [22432/42552]\n",
      "loss: 0.361997  [25632/42552]\n",
      "loss: 0.342530  [28832/42552]\n",
      "loss: 0.239141  [32032/42552]\n",
      "loss: 0.310006  [35232/42552]\n",
      "loss: 0.235642  [38432/42552]\n",
      "loss: 0.311277  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.269879 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.500010  [   32/42552]\n",
      "loss: 0.281230  [ 3232/42552]\n",
      "loss: 0.368930  [ 6432/42552]\n",
      "loss: 0.349448  [ 9632/42552]\n",
      "loss: 0.196200  [12832/42552]\n",
      "loss: 0.399833  [16032/42552]\n",
      "loss: 0.273674  [19232/42552]\n",
      "loss: 0.322710  [22432/42552]\n",
      "loss: 0.346000  [25632/42552]\n",
      "loss: 0.234241  [28832/42552]\n",
      "loss: 0.185943  [32032/42552]\n",
      "loss: 0.469649  [35232/42552]\n",
      "loss: 0.195600  [38432/42552]\n",
      "loss: 0.368159  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.255856 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.378262  [   32/42552]\n",
      "loss: 0.198517  [ 3232/42552]\n",
      "loss: 0.170908  [ 6432/42552]\n",
      "loss: 0.374030  [ 9632/42552]\n",
      "loss: 0.284267  [12832/42552]\n",
      "loss: 0.222156  [16032/42552]\n",
      "loss: 0.276210  [19232/42552]\n",
      "loss: 0.217261  [22432/42552]\n",
      "loss: 0.243874  [25632/42552]\n",
      "loss: 0.337285  [28832/42552]\n",
      "loss: 0.291391  [32032/42552]\n",
      "loss: 0.277050  [35232/42552]\n",
      "loss: 0.349767  [38432/42552]\n",
      "loss: 0.329800  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.252574 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.390330  [   32/42552]\n",
      "loss: 0.162550  [ 3232/42552]\n",
      "loss: 0.297570  [ 6432/42552]\n",
      "loss: 0.302171  [ 9632/42552]\n",
      "loss: 0.293913  [12832/42552]\n",
      "loss: 0.261960  [16032/42552]\n",
      "loss: 0.389246  [19232/42552]\n",
      "loss: 0.265524  [22432/42552]\n",
      "loss: 0.304457  [25632/42552]\n",
      "loss: 0.308447  [28832/42552]\n",
      "loss: 0.200660  [32032/42552]\n",
      "loss: 0.231152  [35232/42552]\n",
      "loss: 0.266384  [38432/42552]\n",
      "loss: 0.408707  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.266188 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.255158  [   32/42552]\n",
      "loss: 0.234841  [ 3232/42552]\n",
      "loss: 0.411476  [ 6432/42552]\n",
      "loss: 0.328524  [ 9632/42552]\n",
      "loss: 0.343660  [12832/42552]\n",
      "loss: 0.222880  [16032/42552]\n",
      "loss: 0.284484  [19232/42552]\n",
      "loss: 0.397024  [22432/42552]\n",
      "loss: 0.304084  [25632/42552]\n",
      "loss: 0.238848  [28832/42552]\n",
      "loss: 0.312983  [32032/42552]\n",
      "loss: 0.260839  [35232/42552]\n",
      "loss: 0.304602  [38432/42552]\n",
      "loss: 0.243920  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.257551 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.309703  [   32/42552]\n",
      "loss: 0.293201  [ 3232/42552]\n",
      "loss: 0.376830  [ 6432/42552]\n",
      "loss: 0.313463  [ 9632/42552]\n",
      "loss: 0.300844  [12832/42552]\n",
      "loss: 0.243235  [16032/42552]\n",
      "loss: 0.274488  [19232/42552]\n",
      "loss: 0.164221  [22432/42552]\n",
      "loss: 0.459898  [25632/42552]\n",
      "loss: 0.228438  [28832/42552]\n",
      "loss: 0.174478  [32032/42552]\n",
      "loss: 0.231278  [35232/42552]\n",
      "loss: 0.192509  [38432/42552]\n",
      "loss: 0.235915  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.252598 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.483468  [   32/42552]\n",
      "loss: 0.423541  [ 3232/42552]\n",
      "loss: 0.300810  [ 6432/42552]\n",
      "loss: 0.422029  [ 9632/42552]\n",
      "loss: 0.316724  [12832/42552]\n",
      "loss: 0.241310  [16032/42552]\n",
      "loss: 0.464534  [19232/42552]\n",
      "loss: 0.230836  [22432/42552]\n",
      "loss: 0.373205  [25632/42552]\n",
      "loss: 0.289623  [28832/42552]\n",
      "loss: 0.226547  [32032/42552]\n",
      "loss: 0.436195  [35232/42552]\n",
      "loss: 0.228943  [38432/42552]\n",
      "loss: 0.292172  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.250772 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.289545  [   32/42552]\n",
      "loss: 0.360642  [ 3232/42552]\n",
      "loss: 0.379228  [ 6432/42552]\n",
      "loss: 0.266085  [ 9632/42552]\n",
      "loss: 0.244751  [12832/42552]\n",
      "loss: 0.286389  [16032/42552]\n",
      "loss: 0.185407  [19232/42552]\n",
      "loss: 0.313416  [22432/42552]\n",
      "loss: 0.363518  [25632/42552]\n",
      "loss: 0.249452  [28832/42552]\n",
      "loss: 0.323735  [32032/42552]\n",
      "loss: 0.219821  [35232/42552]\n",
      "loss: 0.206760  [38432/42552]\n",
      "loss: 0.375308  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.257994 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.351278  [   32/42552]\n",
      "loss: 0.232125  [ 3232/42552]\n",
      "loss: 0.291824  [ 6432/42552]\n",
      "loss: 0.335060  [ 9632/42552]\n",
      "loss: 0.211537  [12832/42552]\n",
      "loss: 0.322696  [16032/42552]\n",
      "loss: 0.197691  [19232/42552]\n",
      "loss: 0.233102  [22432/42552]\n",
      "loss: 0.316350  [25632/42552]\n",
      "loss: 0.248950  [28832/42552]\n",
      "loss: 0.287960  [32032/42552]\n",
      "loss: 0.374835  [35232/42552]\n",
      "loss: 0.138831  [38432/42552]\n",
      "loss: 0.317578  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.245997 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.319647  [   32/42552]\n",
      "loss: 0.223985  [ 3232/42552]\n",
      "loss: 0.416707  [ 6432/42552]\n",
      "loss: 0.344959  [ 9632/42552]\n",
      "loss: 0.290542  [12832/42552]\n",
      "loss: 0.376286  [16032/42552]\n",
      "loss: 0.243442  [19232/42552]\n",
      "loss: 0.204983  [22432/42552]\n",
      "loss: 0.248724  [25632/42552]\n",
      "loss: 0.350604  [28832/42552]\n",
      "loss: 0.208968  [32032/42552]\n",
      "loss: 0.316887  [35232/42552]\n",
      "loss: 0.299916  [38432/42552]\n",
      "loss: 0.324344  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.247885 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.325403  [   32/42552]\n",
      "loss: 0.320046  [ 3232/42552]\n",
      "loss: 0.418970  [ 6432/42552]\n",
      "loss: 0.218957  [ 9632/42552]\n",
      "loss: 0.287486  [12832/42552]\n",
      "loss: 0.419257  [16032/42552]\n",
      "loss: 0.399535  [19232/42552]\n",
      "loss: 0.277421  [22432/42552]\n",
      "loss: 0.325358  [25632/42552]\n",
      "loss: 0.164579  [28832/42552]\n",
      "loss: 0.367177  [32032/42552]\n",
      "loss: 0.289996  [35232/42552]\n",
      "loss: 0.252764  [38432/42552]\n",
      "loss: 0.200602  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.254622 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.315599  [   32/42552]\n",
      "loss: 0.237467  [ 3232/42552]\n",
      "loss: 0.290887  [ 6432/42552]\n",
      "loss: 0.194482  [ 9632/42552]\n",
      "loss: 0.314192  [12832/42552]\n",
      "loss: 0.368203  [16032/42552]\n",
      "loss: 0.371006  [19232/42552]\n",
      "loss: 0.316088  [22432/42552]\n",
      "loss: 0.285600  [25632/42552]\n",
      "loss: 0.240794  [28832/42552]\n",
      "loss: 0.249573  [32032/42552]\n",
      "loss: 0.266272  [35232/42552]\n",
      "loss: 0.149027  [38432/42552]\n",
      "loss: 0.240138  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.250836 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.295768  [   32/42552]\n",
      "loss: 0.285383  [ 3232/42552]\n",
      "loss: 0.243156  [ 6432/42552]\n",
      "loss: 0.241598  [ 9632/42552]\n",
      "loss: 0.315386  [12832/42552]\n",
      "loss: 0.292934  [16032/42552]\n",
      "loss: 0.306746  [19232/42552]\n",
      "loss: 0.324143  [22432/42552]\n",
      "loss: 0.325078  [25632/42552]\n",
      "loss: 0.254084  [28832/42552]\n",
      "loss: 0.411919  [32032/42552]\n",
      "loss: 0.286119  [35232/42552]\n",
      "loss: 0.248807  [38432/42552]\n",
      "loss: 0.239633  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.247745 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.533022  [   32/42552]\n",
      "loss: 0.238043  [ 3232/42552]\n",
      "loss: 0.153796  [ 6432/42552]\n",
      "loss: 0.360174  [ 9632/42552]\n",
      "loss: 0.323105  [12832/42552]\n",
      "loss: 0.270126  [16032/42552]\n",
      "loss: 0.271289  [19232/42552]\n",
      "loss: 0.207886  [22432/42552]\n",
      "loss: 0.185349  [25632/42552]\n",
      "loss: 0.378455  [28832/42552]\n",
      "loss: 0.265399  [32032/42552]\n",
      "loss: 0.246157  [35232/42552]\n",
      "loss: 0.262459  [38432/42552]\n",
      "loss: 0.283178  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.249843 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.371497  [   32/42552]\n",
      "loss: 0.310439  [ 3232/42552]\n",
      "loss: 0.412396  [ 6432/42552]\n",
      "loss: 0.345953  [ 9632/42552]\n",
      "loss: 0.447573  [12832/42552]\n",
      "loss: 0.358075  [16032/42552]\n",
      "loss: 0.213166  [19232/42552]\n",
      "loss: 0.331779  [22432/42552]\n",
      "loss: 0.320117  [25632/42552]\n",
      "loss: 0.458297  [28832/42552]\n",
      "loss: 0.224450  [32032/42552]\n",
      "loss: 0.302429  [35232/42552]\n",
      "loss: 0.198379  [38432/42552]\n",
      "loss: 0.264445  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.248622 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.242835  [   32/42552]\n",
      "loss: 0.148328  [ 3232/42552]\n",
      "loss: 0.463041  [ 6432/42552]\n",
      "loss: 0.377559  [ 9632/42552]\n",
      "loss: 0.278528  [12832/42552]\n",
      "loss: 0.277624  [16032/42552]\n",
      "loss: 0.203040  [19232/42552]\n",
      "loss: 0.297829  [22432/42552]\n",
      "loss: 0.357319  [25632/42552]\n",
      "loss: 0.304586  [28832/42552]\n",
      "loss: 0.385438  [32032/42552]\n",
      "loss: 0.194052  [35232/42552]\n",
      "loss: 0.213104  [38432/42552]\n",
      "loss: 0.311401  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.261781 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.443571  [   32/42552]\n",
      "loss: 0.379844  [ 3232/42552]\n",
      "loss: 0.216279  [ 6432/42552]\n",
      "loss: 0.199440  [ 9632/42552]\n",
      "loss: 0.281059  [12832/42552]\n",
      "loss: 0.209579  [16032/42552]\n",
      "loss: 0.319223  [19232/42552]\n",
      "loss: 0.165712  [22432/42552]\n",
      "loss: 0.322120  [25632/42552]\n",
      "loss: 0.215434  [28832/42552]\n",
      "loss: 0.204453  [32032/42552]\n",
      "loss: 0.289574  [35232/42552]\n",
      "loss: 0.289980  [38432/42552]\n",
      "loss: 0.379450  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.254994 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.244721  [   32/42552]\n",
      "loss: 0.380224  [ 3232/42552]\n",
      "loss: 0.253844  [ 6432/42552]\n",
      "loss: 0.279459  [ 9632/42552]\n",
      "loss: 0.312676  [12832/42552]\n",
      "loss: 0.357720  [16032/42552]\n",
      "loss: 0.311290  [19232/42552]\n",
      "loss: 0.239705  [22432/42552]\n",
      "loss: 0.394481  [25632/42552]\n",
      "loss: 0.266090  [28832/42552]\n",
      "loss: 0.205152  [32032/42552]\n",
      "loss: 0.342097  [35232/42552]\n",
      "loss: 0.289705  [38432/42552]\n",
      "loss: 0.219138  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.245624 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.219917  [   32/42552]\n",
      "loss: 0.362807  [ 3232/42552]\n",
      "loss: 0.189217  [ 6432/42552]\n",
      "loss: 0.425711  [ 9632/42552]\n",
      "loss: 0.290670  [12832/42552]\n",
      "loss: 0.240566  [16032/42552]\n",
      "loss: 0.339998  [19232/42552]\n",
      "loss: 0.433534  [22432/42552]\n",
      "loss: 0.410453  [25632/42552]\n",
      "loss: 0.367336  [28832/42552]\n",
      "loss: 0.144121  [32032/42552]\n",
      "loss: 0.206452  [35232/42552]\n",
      "loss: 0.214041  [38432/42552]\n",
      "loss: 0.222020  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.247830 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.243307  [   32/42552]\n",
      "loss: 0.404103  [ 3232/42552]\n",
      "loss: 0.112254  [ 6432/42552]\n",
      "loss: 0.263900  [ 9632/42552]\n",
      "loss: 0.389865  [12832/42552]\n",
      "loss: 0.218931  [16032/42552]\n",
      "loss: 0.248686  [19232/42552]\n",
      "loss: 0.317318  [22432/42552]\n",
      "loss: 0.137179  [25632/42552]\n",
      "loss: 0.178575  [28832/42552]\n",
      "loss: 0.228377  [32032/42552]\n",
      "loss: 0.226061  [35232/42552]\n",
      "loss: 0.253504  [38432/42552]\n",
      "loss: 0.277716  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.251492 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.428642  [   32/42552]\n",
      "loss: 0.283793  [ 3232/42552]\n",
      "loss: 0.254607  [ 6432/42552]\n",
      "loss: 0.280307  [ 9632/42552]\n",
      "loss: 0.445123  [12832/42552]\n",
      "loss: 0.166546  [16032/42552]\n",
      "loss: 0.464308  [19232/42552]\n",
      "loss: 0.209470  [22432/42552]\n",
      "loss: 0.281364  [25632/42552]\n",
      "loss: 0.327096  [28832/42552]\n",
      "loss: 0.401078  [32032/42552]\n",
      "loss: 0.518762  [35232/42552]\n",
      "loss: 0.320409  [38432/42552]\n",
      "loss: 0.364713  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.249266 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.201243  [   32/42552]\n",
      "loss: 0.358399  [ 3232/42552]\n",
      "loss: 0.196468  [ 6432/42552]\n",
      "loss: 0.376883  [ 9632/42552]\n",
      "loss: 0.330966  [12832/42552]\n",
      "loss: 0.256765  [16032/42552]\n",
      "loss: 0.227859  [19232/42552]\n",
      "loss: 0.381023  [22432/42552]\n",
      "loss: 0.373958  [25632/42552]\n",
      "loss: 0.287289  [28832/42552]\n",
      "loss: 0.295587  [32032/42552]\n",
      "loss: 0.261313  [35232/42552]\n",
      "loss: 0.304218  [38432/42552]\n",
      "loss: 0.374598  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.244559 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.224459  [   32/42552]\n",
      "loss: 0.357057  [ 3232/42552]\n",
      "loss: 0.304838  [ 6432/42552]\n",
      "loss: 0.314011  [ 9632/42552]\n",
      "loss: 0.335467  [12832/42552]\n",
      "loss: 0.301375  [16032/42552]\n",
      "loss: 0.190335  [19232/42552]\n",
      "loss: 0.288266  [22432/42552]\n",
      "loss: 0.212078  [25632/42552]\n",
      "loss: 0.209964  [28832/42552]\n",
      "loss: 0.250700  [32032/42552]\n",
      "loss: 0.302671  [35232/42552]\n",
      "loss: 0.313195  [38432/42552]\n",
      "loss: 0.386106  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.252389 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.306266  [   32/42552]\n",
      "loss: 0.364207  [ 3232/42552]\n",
      "loss: 0.244997  [ 6432/42552]\n",
      "loss: 0.300589  [ 9632/42552]\n",
      "loss: 0.374213  [12832/42552]\n",
      "loss: 0.348656  [16032/42552]\n",
      "loss: 0.247652  [19232/42552]\n",
      "loss: 0.388835  [22432/42552]\n",
      "loss: 0.230363  [25632/42552]\n",
      "loss: 0.214726  [28832/42552]\n",
      "loss: 0.357814  [32032/42552]\n",
      "loss: 0.407888  [35232/42552]\n",
      "loss: 0.423294  [38432/42552]\n",
      "loss: 0.395305  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.253272 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.279027  [   32/42552]\n",
      "loss: 0.271582  [ 3232/42552]\n",
      "loss: 0.268822  [ 6432/42552]\n",
      "loss: 0.299794  [ 9632/42552]\n",
      "loss: 0.418948  [12832/42552]\n",
      "loss: 0.338906  [16032/42552]\n",
      "loss: 0.210599  [19232/42552]\n",
      "loss: 0.470497  [22432/42552]\n",
      "loss: 0.424379  [25632/42552]\n",
      "loss: 0.459725  [28832/42552]\n",
      "loss: 0.234332  [32032/42552]\n",
      "loss: 0.238360  [35232/42552]\n",
      "loss: 0.468395  [38432/42552]\n",
      "loss: 0.341768  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.248862 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.284793  [   32/42552]\n",
      "loss: 0.292655  [ 3232/42552]\n",
      "loss: 0.299843  [ 6432/42552]\n",
      "loss: 0.222920  [ 9632/42552]\n",
      "loss: 0.175714  [12832/42552]\n",
      "loss: 0.361517  [16032/42552]\n",
      "loss: 0.205998  [19232/42552]\n",
      "loss: 0.267725  [22432/42552]\n",
      "loss: 0.196401  [25632/42552]\n",
      "loss: 0.449398  [28832/42552]\n",
      "loss: 0.282452  [32032/42552]\n",
      "loss: 0.315629  [35232/42552]\n",
      "loss: 0.200604  [38432/42552]\n",
      "loss: 0.323266  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.248205 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.309294  [   32/42552]\n",
      "loss: 0.359105  [ 3232/42552]\n",
      "loss: 0.186196  [ 6432/42552]\n",
      "loss: 0.384546  [ 9632/42552]\n",
      "loss: 0.341050  [12832/42552]\n",
      "loss: 0.258854  [16032/42552]\n",
      "loss: 0.256682  [19232/42552]\n",
      "loss: 0.306835  [22432/42552]\n",
      "loss: 0.122325  [25632/42552]\n",
      "loss: 0.371211  [28832/42552]\n",
      "loss: 0.135016  [32032/42552]\n",
      "loss: 0.212750  [35232/42552]\n",
      "loss: 0.330646  [38432/42552]\n",
      "loss: 0.226403  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.248460 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.352742  [   32/42552]\n",
      "loss: 0.212065  [ 3232/42552]\n",
      "loss: 0.517740  [ 6432/42552]\n",
      "loss: 0.145149  [ 9632/42552]\n",
      "loss: 0.421398  [12832/42552]\n",
      "loss: 0.218706  [16032/42552]\n",
      "loss: 0.202920  [19232/42552]\n",
      "loss: 0.237515  [22432/42552]\n",
      "loss: 0.246158  [25632/42552]\n",
      "loss: 0.330985  [28832/42552]\n",
      "loss: 0.267953  [32032/42552]\n",
      "loss: 0.249186  [35232/42552]\n",
      "loss: 0.347031  [38432/42552]\n",
      "loss: 0.314139  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.247753 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.260770  [   32/42552]\n",
      "loss: 0.299119  [ 3232/42552]\n",
      "loss: 0.448571  [ 6432/42552]\n",
      "loss: 0.252336  [ 9632/42552]\n",
      "loss: 0.398751  [12832/42552]\n",
      "loss: 0.213232  [16032/42552]\n",
      "loss: 0.273198  [19232/42552]\n",
      "loss: 0.290733  [22432/42552]\n",
      "loss: 0.409178  [25632/42552]\n",
      "loss: 0.399918  [28832/42552]\n",
      "loss: 0.229662  [32032/42552]\n",
      "loss: 0.176297  [35232/42552]\n",
      "loss: 0.218293  [38432/42552]\n",
      "loss: 0.299758  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.260309 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.279588  [   32/42552]\n",
      "loss: 0.394636  [ 3232/42552]\n",
      "loss: 0.395133  [ 6432/42552]\n",
      "loss: 0.259492  [ 9632/42552]\n",
      "loss: 0.183384  [12832/42552]\n",
      "loss: 0.305458  [16032/42552]\n",
      "loss: 0.301549  [19232/42552]\n",
      "loss: 0.231415  [22432/42552]\n",
      "loss: 0.228555  [25632/42552]\n",
      "loss: 0.325867  [28832/42552]\n",
      "loss: 0.296266  [32032/42552]\n",
      "loss: 0.235673  [35232/42552]\n",
      "loss: 0.237541  [38432/42552]\n",
      "loss: 0.461110  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.245945 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.173969  [   32/42552]\n",
      "loss: 0.419990  [ 3232/42552]\n",
      "loss: 0.176080  [ 6432/42552]\n",
      "loss: 0.316192  [ 9632/42552]\n",
      "loss: 0.220051  [12832/42552]\n",
      "loss: 0.249669  [16032/42552]\n",
      "loss: 0.308811  [19232/42552]\n",
      "loss: 0.429053  [22432/42552]\n",
      "loss: 0.256177  [25632/42552]\n",
      "loss: 0.220040  [28832/42552]\n",
      "loss: 0.305645  [32032/42552]\n",
      "loss: 0.260538  [35232/42552]\n",
      "loss: 0.176028  [38432/42552]\n",
      "loss: 0.194163  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.247644 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.188421  [   32/42552]\n",
      "loss: 0.247394  [ 3232/42552]\n",
      "loss: 0.278516  [ 6432/42552]\n",
      "loss: 0.325678  [ 9632/42552]\n",
      "loss: 0.234934  [12832/42552]\n",
      "loss: 0.285308  [16032/42552]\n",
      "loss: 0.202804  [19232/42552]\n",
      "loss: 0.216361  [22432/42552]\n",
      "loss: 0.161714  [25632/42552]\n",
      "loss: 0.254965  [28832/42552]\n",
      "loss: 0.236897  [32032/42552]\n",
      "loss: 0.264097  [35232/42552]\n",
      "loss: 0.285762  [38432/42552]\n",
      "loss: 0.357824  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.254083 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.329546  [   32/42552]\n",
      "loss: 0.291127  [ 3232/42552]\n",
      "loss: 0.163410  [ 6432/42552]\n",
      "loss: 0.190561  [ 9632/42552]\n",
      "loss: 0.347219  [12832/42552]\n",
      "loss: 0.157379  [16032/42552]\n",
      "loss: 0.201036  [19232/42552]\n",
      "loss: 0.286238  [22432/42552]\n",
      "loss: 0.260888  [25632/42552]\n",
      "loss: 0.328935  [28832/42552]\n",
      "loss: 0.234649  [32032/42552]\n",
      "loss: 0.548479  [35232/42552]\n",
      "loss: 0.193516  [38432/42552]\n",
      "loss: 0.195754  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.249387 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.252116  [   32/42552]\n",
      "loss: 0.235161  [ 3232/42552]\n",
      "loss: 0.363587  [ 6432/42552]\n",
      "loss: 0.275753  [ 9632/42552]\n",
      "loss: 0.200061  [12832/42552]\n",
      "loss: 0.196002  [16032/42552]\n",
      "loss: 0.364401  [19232/42552]\n",
      "loss: 0.150210  [22432/42552]\n",
      "loss: 0.306559  [25632/42552]\n",
      "loss: 0.228721  [28832/42552]\n",
      "loss: 0.324220  [32032/42552]\n",
      "loss: 0.157964  [35232/42552]\n",
      "loss: 0.266793  [38432/42552]\n",
      "loss: 0.371088  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.248143 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.378874  [   32/42552]\n",
      "loss: 0.175414  [ 3232/42552]\n",
      "loss: 0.289525  [ 6432/42552]\n",
      "loss: 0.319749  [ 9632/42552]\n",
      "loss: 0.238065  [12832/42552]\n",
      "loss: 0.266559  [16032/42552]\n",
      "loss: 0.336656  [19232/42552]\n",
      "loss: 0.296871  [22432/42552]\n",
      "loss: 0.197012  [25632/42552]\n",
      "loss: 0.450715  [28832/42552]\n",
      "loss: 0.251679  [32032/42552]\n",
      "loss: 0.276115  [35232/42552]\n",
      "loss: 0.291549  [38432/42552]\n",
      "loss: 0.312502  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.261706 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.305588  [   32/42552]\n",
      "loss: 0.251078  [ 3232/42552]\n",
      "loss: 0.177942  [ 6432/42552]\n",
      "loss: 0.366372  [ 9632/42552]\n",
      "loss: 0.251033  [12832/42552]\n",
      "loss: 0.181512  [16032/42552]\n",
      "loss: 0.467999  [19232/42552]\n",
      "loss: 0.225295  [22432/42552]\n",
      "loss: 0.256199  [25632/42552]\n",
      "loss: 0.291471  [28832/42552]\n",
      "loss: 0.273510  [32032/42552]\n",
      "loss: 0.236371  [35232/42552]\n",
      "loss: 0.290444  [38432/42552]\n",
      "loss: 0.249401  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.245866 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.286707  [   32/42552]\n",
      "loss: 0.335151  [ 3232/42552]\n",
      "loss: 0.254945  [ 6432/42552]\n",
      "loss: 0.244853  [ 9632/42552]\n",
      "loss: 0.267204  [12832/42552]\n",
      "loss: 0.212296  [16032/42552]\n",
      "loss: 0.171465  [19232/42552]\n",
      "loss: 0.167829  [22432/42552]\n",
      "loss: 0.182893  [25632/42552]\n",
      "loss: 0.200236  [28832/42552]\n",
      "loss: 0.269847  [32032/42552]\n",
      "loss: 0.340554  [35232/42552]\n",
      "loss: 0.186185  [38432/42552]\n",
      "loss: 0.270949  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.247880 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.185807  [   32/42552]\n",
      "loss: 0.336205  [ 3232/42552]\n",
      "loss: 0.290027  [ 6432/42552]\n",
      "loss: 0.296814  [ 9632/42552]\n",
      "loss: 0.387108  [12832/42552]\n",
      "loss: 0.176765  [16032/42552]\n",
      "loss: 0.265511  [19232/42552]\n",
      "loss: 0.174126  [22432/42552]\n",
      "loss: 0.255767  [25632/42552]\n",
      "loss: 0.388060  [28832/42552]\n",
      "loss: 0.276375  [32032/42552]\n",
      "loss: 0.624712  [35232/42552]\n",
      "loss: 0.336296  [38432/42552]\n",
      "loss: 0.357991  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.249728 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.180411  [   32/42552]\n",
      "loss: 0.260168  [ 3232/42552]\n",
      "loss: 0.265666  [ 6432/42552]\n",
      "loss: 0.402619  [ 9632/42552]\n",
      "loss: 0.203940  [12832/42552]\n",
      "loss: 0.144114  [16032/42552]\n",
      "loss: 0.241287  [19232/42552]\n",
      "loss: 0.274540  [22432/42552]\n",
      "loss: 0.285118  [25632/42552]\n",
      "loss: 0.326010  [28832/42552]\n",
      "loss: 0.370496  [32032/42552]\n",
      "loss: 0.300786  [35232/42552]\n",
      "loss: 0.322464  [38432/42552]\n",
      "loss: 0.286802  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.243578 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.352473  [   32/42552]\n",
      "loss: 0.337404  [ 3232/42552]\n",
      "loss: 0.183281  [ 6432/42552]\n",
      "loss: 0.449675  [ 9632/42552]\n",
      "loss: 0.221764  [12832/42552]\n",
      "loss: 0.726695  [16032/42552]\n",
      "loss: 0.309186  [19232/42552]\n",
      "loss: 0.174279  [22432/42552]\n",
      "loss: 0.296832  [25632/42552]\n",
      "loss: 0.237423  [28832/42552]\n",
      "loss: 0.251814  [32032/42552]\n",
      "loss: 0.329558  [35232/42552]\n",
      "loss: 0.379752  [38432/42552]\n",
      "loss: 0.282121  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.238965 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.223820  [   32/42552]\n",
      "loss: 0.315306  [ 3232/42552]\n",
      "loss: 0.189485  [ 6432/42552]\n",
      "loss: 0.432345  [ 9632/42552]\n",
      "loss: 0.262206  [12832/42552]\n",
      "loss: 0.364522  [16032/42552]\n",
      "loss: 0.299373  [19232/42552]\n",
      "loss: 0.443885  [22432/42552]\n",
      "loss: 0.211780  [25632/42552]\n",
      "loss: 0.352948  [28832/42552]\n",
      "loss: 0.329874  [32032/42552]\n",
      "loss: 0.311255  [35232/42552]\n",
      "loss: 0.171873  [38432/42552]\n",
      "loss: 0.298860  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.244173 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.278685  [   32/42552]\n",
      "loss: 0.373151  [ 3232/42552]\n",
      "loss: 0.338300  [ 6432/42552]\n",
      "loss: 0.349568  [ 9632/42552]\n",
      "loss: 0.214632  [12832/42552]\n",
      "loss: 0.364896  [16032/42552]\n",
      "loss: 0.278574  [19232/42552]\n",
      "loss: 0.400722  [22432/42552]\n",
      "loss: 0.417535  [25632/42552]\n",
      "loss: 0.416863  [28832/42552]\n",
      "loss: 0.354454  [32032/42552]\n",
      "loss: 0.264867  [35232/42552]\n",
      "loss: 0.299376  [38432/42552]\n",
      "loss: 0.210754  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.244215 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.260252  [   32/42552]\n",
      "loss: 0.171166  [ 3232/42552]\n",
      "loss: 0.316095  [ 6432/42552]\n",
      "loss: 0.277989  [ 9632/42552]\n",
      "loss: 0.194941  [12832/42552]\n",
      "loss: 0.245078  [16032/42552]\n",
      "loss: 0.346349  [19232/42552]\n",
      "loss: 0.192343  [22432/42552]\n",
      "loss: 0.293942  [25632/42552]\n",
      "loss: 0.238296  [28832/42552]\n",
      "loss: 0.312463  [32032/42552]\n",
      "loss: 0.235385  [35232/42552]\n",
      "loss: 0.237158  [38432/42552]\n",
      "loss: 0.313898  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.246114 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.197479  [   32/42552]\n",
      "loss: 0.250333  [ 3232/42552]\n",
      "loss: 0.286547  [ 6432/42552]\n",
      "loss: 0.193123  [ 9632/42552]\n",
      "loss: 0.241088  [12832/42552]\n",
      "loss: 0.313592  [16032/42552]\n",
      "loss: 0.369220  [19232/42552]\n",
      "loss: 0.341696  [22432/42552]\n",
      "loss: 0.424069  [25632/42552]\n",
      "loss: 0.227345  [28832/42552]\n",
      "loss: 0.136995  [32032/42552]\n",
      "loss: 0.259588  [35232/42552]\n",
      "loss: 0.238464  [38432/42552]\n",
      "loss: 0.298505  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.246373 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.196595  [   32/42552]\n",
      "loss: 0.348950  [ 3232/42552]\n",
      "loss: 0.405579  [ 6432/42552]\n",
      "loss: 0.410635  [ 9632/42552]\n",
      "loss: 0.190659  [12832/42552]\n",
      "loss: 0.240024  [16032/42552]\n",
      "loss: 0.329294  [19232/42552]\n",
      "loss: 0.437233  [22432/42552]\n",
      "loss: 0.161645  [25632/42552]\n",
      "loss: 0.304014  [28832/42552]\n",
      "loss: 0.195810  [32032/42552]\n",
      "loss: 0.442785  [35232/42552]\n",
      "loss: 0.269162  [38432/42552]\n",
      "loss: 0.356319  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.249834 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.280156  [   32/42552]\n",
      "loss: 0.278609  [ 3232/42552]\n",
      "loss: 0.246793  [ 6432/42552]\n",
      "loss: 0.222633  [ 9632/42552]\n",
      "loss: 0.352493  [12832/42552]\n",
      "loss: 0.172031  [16032/42552]\n",
      "loss: 0.201810  [19232/42552]\n",
      "loss: 0.260278  [22432/42552]\n",
      "loss: 0.318632  [25632/42552]\n",
      "loss: 0.417303  [28832/42552]\n",
      "loss: 0.312398  [32032/42552]\n",
      "loss: 0.243370  [35232/42552]\n",
      "loss: 0.353964  [38432/42552]\n",
      "loss: 0.297579  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.248066 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.204990  [   32/42552]\n",
      "loss: 0.171076  [ 3232/42552]\n",
      "loss: 0.352316  [ 6432/42552]\n",
      "loss: 0.227652  [ 9632/42552]\n",
      "loss: 0.230125  [12832/42552]\n",
      "loss: 0.421480  [16032/42552]\n",
      "loss: 0.255814  [19232/42552]\n",
      "loss: 0.158663  [22432/42552]\n",
      "loss: 0.316928  [25632/42552]\n",
      "loss: 0.292729  [28832/42552]\n",
      "loss: 0.276794  [32032/42552]\n",
      "loss: 0.296089  [35232/42552]\n",
      "loss: 0.283478  [38432/42552]\n",
      "loss: 0.374376  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.258558 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.272824  [   32/42552]\n",
      "loss: 0.277840  [ 3232/42552]\n",
      "loss: 0.389980  [ 6432/42552]\n",
      "loss: 0.431909  [ 9632/42552]\n",
      "loss: 0.244140  [12832/42552]\n",
      "loss: 0.327912  [16032/42552]\n",
      "loss: 0.325032  [19232/42552]\n",
      "loss: 0.386305  [22432/42552]\n",
      "loss: 0.331889  [25632/42552]\n",
      "loss: 0.245984  [28832/42552]\n",
      "loss: 0.221798  [32032/42552]\n",
      "loss: 0.372733  [35232/42552]\n",
      "loss: 0.353420  [38432/42552]\n",
      "loss: 0.135462  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.248836 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.392180  [   32/42552]\n",
      "loss: 0.258129  [ 3232/42552]\n",
      "loss: 0.252879  [ 6432/42552]\n",
      "loss: 0.435329  [ 9632/42552]\n",
      "loss: 0.342488  [12832/42552]\n",
      "loss: 0.207917  [16032/42552]\n",
      "loss: 0.239931  [19232/42552]\n",
      "loss: 0.330194  [22432/42552]\n",
      "loss: 0.178310  [25632/42552]\n",
      "loss: 0.347305  [28832/42552]\n",
      "loss: 0.232924  [32032/42552]\n",
      "loss: 0.328434  [35232/42552]\n",
      "loss: 0.291020  [38432/42552]\n",
      "loss: 0.407357  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.249761 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.316047  [   32/42552]\n",
      "loss: 0.276779  [ 3232/42552]\n",
      "loss: 0.244443  [ 6432/42552]\n",
      "loss: 0.255402  [ 9632/42552]\n",
      "loss: 0.286132  [12832/42552]\n",
      "loss: 0.240131  [16032/42552]\n",
      "loss: 0.359415  [19232/42552]\n",
      "loss: 0.379528  [22432/42552]\n",
      "loss: 0.281206  [25632/42552]\n",
      "loss: 0.245124  [28832/42552]\n",
      "loss: 0.383260  [32032/42552]\n",
      "loss: 0.361342  [35232/42552]\n",
      "loss: 0.232906  [38432/42552]\n",
      "loss: 0.245219  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.256022 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.285224  [   32/42552]\n",
      "loss: 0.419645  [ 3232/42552]\n",
      "loss: 0.374744  [ 6432/42552]\n",
      "loss: 0.404837  [ 9632/42552]\n",
      "loss: 0.255619  [12832/42552]\n",
      "loss: 0.339040  [16032/42552]\n",
      "loss: 0.239200  [19232/42552]\n",
      "loss: 0.255387  [22432/42552]\n",
      "loss: 0.213995  [25632/42552]\n",
      "loss: 0.413291  [28832/42552]\n",
      "loss: 0.229529  [32032/42552]\n",
      "loss: 0.339982  [35232/42552]\n",
      "loss: 0.250400  [38432/42552]\n",
      "loss: 0.297450  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.252595 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.377231  [   32/42552]\n",
      "loss: 0.403656  [ 3232/42552]\n",
      "loss: 0.369413  [ 6432/42552]\n",
      "loss: 0.413319  [ 9632/42552]\n",
      "loss: 0.266468  [12832/42552]\n",
      "loss: 0.516858  [16032/42552]\n",
      "loss: 0.299360  [19232/42552]\n",
      "loss: 0.315062  [22432/42552]\n",
      "loss: 0.318685  [25632/42552]\n",
      "loss: 0.226705  [28832/42552]\n",
      "loss: 0.205053  [32032/42552]\n",
      "loss: 0.329781  [35232/42552]\n",
      "loss: 0.207738  [38432/42552]\n",
      "loss: 0.304454  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.248762 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.243437  [   32/42552]\n",
      "loss: 0.239103  [ 3232/42552]\n",
      "loss: 0.431880  [ 6432/42552]\n",
      "loss: 0.372958  [ 9632/42552]\n",
      "loss: 0.406894  [12832/42552]\n",
      "loss: 0.232591  [16032/42552]\n",
      "loss: 0.215212  [19232/42552]\n",
      "loss: 0.226652  [22432/42552]\n",
      "loss: 0.399872  [25632/42552]\n",
      "loss: 0.281316  [28832/42552]\n",
      "loss: 0.179945  [32032/42552]\n",
      "loss: 0.327171  [35232/42552]\n",
      "loss: 0.302450  [38432/42552]\n",
      "loss: 0.257324  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.249439 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.234546  [   32/42552]\n",
      "loss: 0.184580  [ 3232/42552]\n",
      "loss: 0.261590  [ 6432/42552]\n",
      "loss: 0.277883  [ 9632/42552]\n",
      "loss: 0.281354  [12832/42552]\n",
      "loss: 0.208759  [16032/42552]\n",
      "loss: 0.239361  [19232/42552]\n",
      "loss: 0.266222  [22432/42552]\n",
      "loss: 0.359165  [25632/42552]\n",
      "loss: 0.453315  [28832/42552]\n",
      "loss: 0.218527  [32032/42552]\n",
      "loss: 0.391136  [35232/42552]\n",
      "loss: 0.320775  [38432/42552]\n",
      "loss: 0.378519  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.245673 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.251230  [   32/42552]\n",
      "loss: 0.313446  [ 3232/42552]\n",
      "loss: 0.302418  [ 6432/42552]\n",
      "loss: 0.360486  [ 9632/42552]\n",
      "loss: 0.369057  [12832/42552]\n",
      "loss: 0.315470  [16032/42552]\n",
      "loss: 0.310979  [19232/42552]\n",
      "loss: 0.308315  [22432/42552]\n",
      "loss: 0.211696  [25632/42552]\n",
      "loss: 0.382254  [28832/42552]\n",
      "loss: 0.329196  [32032/42552]\n",
      "loss: 0.236009  [35232/42552]\n",
      "loss: 0.306203  [38432/42552]\n",
      "loss: 0.227380  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.245040 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.357266  [   32/42552]\n",
      "loss: 0.245563  [ 3232/42552]\n",
      "loss: 0.224290  [ 6432/42552]\n",
      "loss: 0.192229  [ 9632/42552]\n",
      "loss: 0.320907  [12832/42552]\n",
      "loss: 0.242449  [16032/42552]\n",
      "loss: 0.429293  [19232/42552]\n",
      "loss: 0.167404  [22432/42552]\n",
      "loss: 0.233259  [25632/42552]\n",
      "loss: 0.281365  [28832/42552]\n",
      "loss: 0.270844  [32032/42552]\n",
      "loss: 0.227948  [35232/42552]\n",
      "loss: 0.383909  [38432/42552]\n",
      "loss: 0.379516  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.241520 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.306245  [   32/42552]\n",
      "loss: 0.313154  [ 3232/42552]\n",
      "loss: 0.189487  [ 6432/42552]\n",
      "loss: 0.285139  [ 9632/42552]\n",
      "loss: 0.358308  [12832/42552]\n",
      "loss: 0.304349  [16032/42552]\n",
      "loss: 0.195723  [19232/42552]\n",
      "loss: 0.223269  [22432/42552]\n",
      "loss: 0.327846  [25632/42552]\n",
      "loss: 0.203182  [28832/42552]\n",
      "loss: 0.291341  [32032/42552]\n",
      "loss: 0.288980  [35232/42552]\n",
      "loss: 0.375864  [38432/42552]\n",
      "loss: 0.270231  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.243219 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.189002  [   32/42552]\n",
      "loss: 0.256607  [ 3232/42552]\n",
      "loss: 0.308607  [ 6432/42552]\n",
      "loss: 0.226298  [ 9632/42552]\n",
      "loss: 0.230505  [12832/42552]\n",
      "loss: 0.302792  [16032/42552]\n",
      "loss: 0.232843  [19232/42552]\n",
      "loss: 0.306787  [22432/42552]\n",
      "loss: 0.209651  [25632/42552]\n",
      "loss: 0.332324  [28832/42552]\n",
      "loss: 0.435913  [32032/42552]\n",
      "loss: 0.261488  [35232/42552]\n",
      "loss: 0.343644  [38432/42552]\n",
      "loss: 0.364030  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.244727 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.191492  [   32/42552]\n",
      "loss: 0.226822  [ 3232/42552]\n",
      "loss: 0.168128  [ 6432/42552]\n",
      "loss: 0.255206  [ 9632/42552]\n",
      "loss: 0.253070  [12832/42552]\n",
      "loss: 0.367599  [16032/42552]\n",
      "loss: 0.221050  [19232/42552]\n",
      "loss: 0.318214  [22432/42552]\n",
      "loss: 0.374487  [25632/42552]\n",
      "loss: 0.208577  [28832/42552]\n",
      "loss: 0.263306  [32032/42552]\n",
      "loss: 0.195994  [35232/42552]\n",
      "loss: 0.216838  [38432/42552]\n",
      "loss: 0.248247  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.259137 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.304596  [   32/42552]\n",
      "loss: 0.225618  [ 3232/42552]\n",
      "loss: 0.307746  [ 6432/42552]\n",
      "loss: 0.327629  [ 9632/42552]\n",
      "loss: 0.322414  [12832/42552]\n",
      "loss: 0.329236  [16032/42552]\n",
      "loss: 0.263614  [19232/42552]\n",
      "loss: 0.335008  [22432/42552]\n",
      "loss: 0.331377  [25632/42552]\n",
      "loss: 0.248093  [28832/42552]\n",
      "loss: 0.262580  [32032/42552]\n",
      "loss: 0.211454  [35232/42552]\n",
      "loss: 0.198874  [38432/42552]\n",
      "loss: 0.228713  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.244001 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.367660  [   32/42552]\n",
      "loss: 0.221221  [ 3232/42552]\n",
      "loss: 0.170947  [ 6432/42552]\n",
      "loss: 0.372717  [ 9632/42552]\n",
      "loss: 0.208834  [12832/42552]\n",
      "loss: 0.292782  [16032/42552]\n",
      "loss: 0.275236  [19232/42552]\n",
      "loss: 0.341880  [22432/42552]\n",
      "loss: 0.433732  [25632/42552]\n",
      "loss: 0.303339  [28832/42552]\n",
      "loss: 0.207151  [32032/42552]\n",
      "loss: 0.348688  [35232/42552]\n",
      "loss: 0.276538  [38432/42552]\n",
      "loss: 0.232424  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.244302 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.253726  [   32/42552]\n",
      "loss: 0.246019  [ 3232/42552]\n",
      "loss: 0.240758  [ 6432/42552]\n",
      "loss: 0.411471  [ 9632/42552]\n",
      "loss: 0.275460  [12832/42552]\n",
      "loss: 0.313635  [16032/42552]\n",
      "loss: 0.246680  [19232/42552]\n",
      "loss: 0.160486  [22432/42552]\n",
      "loss: 0.263282  [25632/42552]\n",
      "loss: 0.292603  [28832/42552]\n",
      "loss: 0.173174  [32032/42552]\n",
      "loss: 0.258158  [35232/42552]\n",
      "loss: 0.282437  [38432/42552]\n",
      "loss: 0.276682  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.246923 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.298731  [   32/42552]\n",
      "loss: 0.234490  [ 3232/42552]\n",
      "loss: 0.340411  [ 6432/42552]\n",
      "loss: 0.269304  [ 9632/42552]\n",
      "loss: 0.199465  [12832/42552]\n",
      "loss: 0.207517  [16032/42552]\n",
      "loss: 0.221353  [19232/42552]\n",
      "loss: 0.280042  [22432/42552]\n",
      "loss: 0.354698  [25632/42552]\n",
      "loss: 0.276802  [28832/42552]\n",
      "loss: 0.306943  [32032/42552]\n",
      "loss: 0.198715  [35232/42552]\n",
      "loss: 0.314372  [38432/42552]\n",
      "loss: 0.244716  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.245257 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.239294  [   32/42552]\n",
      "loss: 0.273372  [ 3232/42552]\n",
      "loss: 0.270238  [ 6432/42552]\n",
      "loss: 0.290742  [ 9632/42552]\n",
      "loss: 0.192197  [12832/42552]\n",
      "loss: 0.342408  [16032/42552]\n",
      "loss: 0.347491  [19232/42552]\n",
      "loss: 0.349556  [22432/42552]\n",
      "loss: 0.299388  [25632/42552]\n",
      "loss: 0.227812  [28832/42552]\n",
      "loss: 0.245780  [32032/42552]\n",
      "loss: 0.318998  [35232/42552]\n",
      "loss: 0.237869  [38432/42552]\n",
      "loss: 0.352730  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.243622 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.309000  [   32/42552]\n",
      "loss: 0.474923  [ 3232/42552]\n",
      "loss: 0.234473  [ 6432/42552]\n",
      "loss: 0.208791  [ 9632/42552]\n",
      "loss: 0.235122  [12832/42552]\n",
      "loss: 0.233357  [16032/42552]\n",
      "loss: 0.312613  [19232/42552]\n",
      "loss: 0.261413  [22432/42552]\n",
      "loss: 0.249024  [25632/42552]\n",
      "loss: 0.200185  [28832/42552]\n",
      "loss: 0.290914  [32032/42552]\n",
      "loss: 0.248565  [35232/42552]\n",
      "loss: 0.263322  [38432/42552]\n",
      "loss: 0.323460  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.249915 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.223785  [   32/42552]\n",
      "loss: 0.338930  [ 3232/42552]\n",
      "loss: 0.237453  [ 6432/42552]\n",
      "loss: 0.156939  [ 9632/42552]\n",
      "loss: 0.232735  [12832/42552]\n",
      "loss: 0.515136  [16032/42552]\n",
      "loss: 0.225359  [19232/42552]\n",
      "loss: 0.468227  [22432/42552]\n",
      "loss: 0.286178  [25632/42552]\n",
      "loss: 0.259509  [28832/42552]\n",
      "loss: 0.319449  [32032/42552]\n",
      "loss: 0.261421  [35232/42552]\n",
      "loss: 0.262063  [38432/42552]\n",
      "loss: 0.255567  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.245293 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.297888  [   32/42552]\n",
      "loss: 0.491535  [ 3232/42552]\n",
      "loss: 0.294877  [ 6432/42552]\n",
      "loss: 0.308792  [ 9632/42552]\n",
      "loss: 0.263117  [12832/42552]\n",
      "loss: 0.412875  [16032/42552]\n",
      "loss: 0.209214  [19232/42552]\n",
      "loss: 0.197257  [22432/42552]\n",
      "loss: 0.341494  [25632/42552]\n",
      "loss: 0.320691  [28832/42552]\n",
      "loss: 0.201549  [32032/42552]\n",
      "loss: 0.321187  [35232/42552]\n",
      "loss: 0.209388  [38432/42552]\n",
      "loss: 0.305384  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.249873 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.606427  [   32/42552]\n",
      "loss: 0.437839  [ 3232/42552]\n",
      "loss: 0.397767  [ 6432/42552]\n",
      "loss: 0.277199  [ 9632/42552]\n",
      "loss: 0.432927  [12832/42552]\n",
      "loss: 0.346175  [16032/42552]\n",
      "loss: 0.401567  [19232/42552]\n",
      "loss: 0.245538  [22432/42552]\n",
      "loss: 0.354764  [25632/42552]\n",
      "loss: 0.231854  [28832/42552]\n",
      "loss: 0.213013  [32032/42552]\n",
      "loss: 0.243117  [35232/42552]\n",
      "loss: 0.374027  [38432/42552]\n",
      "loss: 0.261285  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.247321 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.392505  [   32/42552]\n",
      "loss: 0.232404  [ 3232/42552]\n",
      "loss: 0.293406  [ 6432/42552]\n",
      "loss: 0.259699  [ 9632/42552]\n",
      "loss: 0.485817  [12832/42552]\n",
      "loss: 0.256742  [16032/42552]\n",
      "loss: 0.418319  [19232/42552]\n",
      "loss: 0.355065  [22432/42552]\n",
      "loss: 0.225812  [25632/42552]\n",
      "loss: 0.441348  [28832/42552]\n",
      "loss: 0.253445  [32032/42552]\n",
      "loss: 0.316176  [35232/42552]\n",
      "loss: 0.178613  [38432/42552]\n",
      "loss: 0.303277  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.243188 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.311819  [   32/42552]\n",
      "loss: 0.261770  [ 3232/42552]\n",
      "loss: 0.204603  [ 6432/42552]\n",
      "loss: 0.181198  [ 9632/42552]\n",
      "loss: 0.438294  [12832/42552]\n",
      "loss: 0.359011  [16032/42552]\n",
      "loss: 0.327562  [19232/42552]\n",
      "loss: 0.213411  [22432/42552]\n",
      "loss: 0.315596  [25632/42552]\n",
      "loss: 0.556973  [28832/42552]\n",
      "loss: 0.413838  [32032/42552]\n",
      "loss: 0.313130  [35232/42552]\n",
      "loss: 0.364691  [38432/42552]\n",
      "loss: 0.179126  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.244977 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.240155  [   32/42552]\n",
      "loss: 0.206555  [ 3232/42552]\n",
      "loss: 0.267571  [ 6432/42552]\n",
      "loss: 0.200069  [ 9632/42552]\n",
      "loss: 0.369650  [12832/42552]\n",
      "loss: 0.281506  [16032/42552]\n",
      "loss: 0.227653  [19232/42552]\n",
      "loss: 0.272127  [22432/42552]\n",
      "loss: 0.228601  [25632/42552]\n",
      "loss: 0.287761  [28832/42552]\n",
      "loss: 0.321855  [32032/42552]\n",
      "loss: 0.265476  [35232/42552]\n",
      "loss: 0.236636  [38432/42552]\n",
      "loss: 0.253116  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.250125 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.336501  [   32/42552]\n",
      "loss: 0.241984  [ 3232/42552]\n",
      "loss: 0.312398  [ 6432/42552]\n",
      "loss: 0.293902  [ 9632/42552]\n",
      "loss: 0.156610  [12832/42552]\n",
      "loss: 0.203439  [16032/42552]\n",
      "loss: 0.229029  [19232/42552]\n",
      "loss: 0.300487  [22432/42552]\n",
      "loss: 0.376036  [25632/42552]\n",
      "loss: 0.324358  [28832/42552]\n",
      "loss: 0.413390  [32032/42552]\n",
      "loss: 0.242274  [35232/42552]\n",
      "loss: 0.193802  [38432/42552]\n",
      "loss: 0.171927  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.246441 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.376626  [   32/42552]\n",
      "loss: 0.219984  [ 3232/42552]\n",
      "loss: 0.275033  [ 6432/42552]\n",
      "loss: 0.237503  [ 9632/42552]\n",
      "loss: 0.247398  [12832/42552]\n",
      "loss: 0.288211  [16032/42552]\n",
      "loss: 0.257158  [19232/42552]\n",
      "loss: 0.146666  [22432/42552]\n",
      "loss: 0.241995  [25632/42552]\n",
      "loss: 0.333358  [28832/42552]\n",
      "loss: 0.177722  [32032/42552]\n",
      "loss: 0.264053  [35232/42552]\n",
      "loss: 0.343257  [38432/42552]\n",
      "loss: 0.270912  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.241126 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.214353  [   32/42552]\n",
      "loss: 0.245198  [ 3232/42552]\n",
      "loss: 0.211947  [ 6432/42552]\n",
      "loss: 0.286049  [ 9632/42552]\n",
      "loss: 0.255524  [12832/42552]\n",
      "loss: 0.184535  [16032/42552]\n",
      "loss: 0.374247  [19232/42552]\n",
      "loss: 0.275164  [22432/42552]\n",
      "loss: 0.150689  [25632/42552]\n",
      "loss: 0.210942  [28832/42552]\n",
      "loss: 0.432726  [32032/42552]\n",
      "loss: 0.457677  [35232/42552]\n",
      "loss: 0.266531  [38432/42552]\n",
      "loss: 0.282270  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.243051 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.406527  [   32/42552]\n",
      "loss: 0.276250  [ 3232/42552]\n",
      "loss: 0.210889  [ 6432/42552]\n",
      "loss: 0.382621  [ 9632/42552]\n",
      "loss: 0.302789  [12832/42552]\n",
      "loss: 0.316705  [16032/42552]\n",
      "loss: 0.214754  [19232/42552]\n",
      "loss: 0.278695  [22432/42552]\n",
      "loss: 0.267222  [25632/42552]\n",
      "loss: 0.194004  [28832/42552]\n",
      "loss: 0.342311  [32032/42552]\n",
      "loss: 0.439548  [35232/42552]\n",
      "loss: 0.441013  [38432/42552]\n",
      "loss: 0.296266  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.247308 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.286799  [   32/42552]\n",
      "loss: 0.312900  [ 3232/42552]\n",
      "loss: 0.144377  [ 6432/42552]\n",
      "loss: 0.340669  [ 9632/42552]\n",
      "loss: 0.227083  [12832/42552]\n",
      "loss: 0.182598  [16032/42552]\n",
      "loss: 0.262946  [19232/42552]\n",
      "loss: 0.287994  [22432/42552]\n",
      "loss: 0.318198  [25632/42552]\n",
      "loss: 0.249757  [28832/42552]\n",
      "loss: 0.308282  [32032/42552]\n",
      "loss: 0.364918  [35232/42552]\n",
      "loss: 0.316855  [38432/42552]\n",
      "loss: 0.387779  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.243464 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.338330  [   32/42552]\n",
      "loss: 0.436743  [ 3232/42552]\n",
      "loss: 0.227106  [ 6432/42552]\n",
      "loss: 0.328131  [ 9632/42552]\n",
      "loss: 0.213469  [12832/42552]\n",
      "loss: 0.365575  [16032/42552]\n",
      "loss: 0.282396  [19232/42552]\n",
      "loss: 0.384059  [22432/42552]\n",
      "loss: 0.292628  [25632/42552]\n",
      "loss: 0.290077  [28832/42552]\n",
      "loss: 0.239150  [32032/42552]\n",
      "loss: 0.293678  [35232/42552]\n",
      "loss: 0.175777  [38432/42552]\n",
      "loss: 0.421148  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.243745 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.334928  [   32/42552]\n",
      "loss: 0.462375  [ 3232/42552]\n",
      "loss: 0.285415  [ 6432/42552]\n",
      "loss: 0.300322  [ 9632/42552]\n",
      "loss: 0.202148  [12832/42552]\n",
      "loss: 0.126714  [16032/42552]\n",
      "loss: 0.251285  [19232/42552]\n",
      "loss: 0.357928  [22432/42552]\n",
      "loss: 0.320540  [25632/42552]\n",
      "loss: 0.242243  [28832/42552]\n",
      "loss: 0.290238  [32032/42552]\n",
      "loss: 0.238480  [35232/42552]\n",
      "loss: 0.233852  [38432/42552]\n",
      "loss: 0.280800  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.251760 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.258885  [   32/42552]\n",
      "loss: 0.382091  [ 3232/42552]\n",
      "loss: 0.290861  [ 6432/42552]\n",
      "loss: 0.331207  [ 9632/42552]\n",
      "loss: 0.229245  [12832/42552]\n",
      "loss: 0.245155  [16032/42552]\n",
      "loss: 0.318275  [19232/42552]\n",
      "loss: 0.312296  [22432/42552]\n",
      "loss: 0.123236  [25632/42552]\n",
      "loss: 0.237397  [28832/42552]\n",
      "loss: 0.407809  [32032/42552]\n",
      "loss: 0.222701  [35232/42552]\n",
      "loss: 0.253935  [38432/42552]\n",
      "loss: 0.230185  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.244286 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.303875  [   32/42552]\n",
      "loss: 0.249916  [ 3232/42552]\n",
      "loss: 0.294611  [ 6432/42552]\n",
      "loss: 0.298504  [ 9632/42552]\n",
      "loss: 0.311961  [12832/42552]\n",
      "loss: 0.285493  [16032/42552]\n",
      "loss: 0.262909  [19232/42552]\n",
      "loss: 0.242255  [22432/42552]\n",
      "loss: 0.164626  [25632/42552]\n",
      "loss: 0.599887  [28832/42552]\n",
      "loss: 0.371707  [32032/42552]\n",
      "loss: 0.230157  [35232/42552]\n",
      "loss: 0.247527  [38432/42552]\n",
      "loss: 0.245545  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.243339 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.275071  [   32/42552]\n",
      "loss: 0.323339  [ 3232/42552]\n",
      "loss: 0.220334  [ 6432/42552]\n",
      "loss: 0.211132  [ 9632/42552]\n",
      "loss: 0.343179  [12832/42552]\n",
      "loss: 0.205177  [16032/42552]\n",
      "loss: 0.263977  [19232/42552]\n",
      "loss: 0.228737  [22432/42552]\n",
      "loss: 0.229021  [25632/42552]\n",
      "loss: 0.315732  [28832/42552]\n",
      "loss: 0.323327  [32032/42552]\n",
      "loss: 0.388485  [35232/42552]\n",
      "loss: 0.365138  [38432/42552]\n",
      "loss: 0.360138  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.244125 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.235736  [   32/42552]\n",
      "loss: 0.206888  [ 3232/42552]\n",
      "loss: 0.238707  [ 6432/42552]\n",
      "loss: 0.319729  [ 9632/42552]\n",
      "loss: 0.187434  [12832/42552]\n",
      "loss: 0.198598  [16032/42552]\n",
      "loss: 0.226198  [19232/42552]\n",
      "loss: 0.158058  [22432/42552]\n",
      "loss: 0.299881  [25632/42552]\n",
      "loss: 0.219807  [28832/42552]\n",
      "loss: 0.336798  [32032/42552]\n",
      "loss: 0.183184  [35232/42552]\n",
      "loss: 0.177907  [38432/42552]\n",
      "loss: 0.247009  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.254885 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.283591  [   32/42552]\n",
      "loss: 0.214717  [ 3232/42552]\n",
      "loss: 0.294306  [ 6432/42552]\n",
      "loss: 0.401416  [ 9632/42552]\n",
      "loss: 0.233730  [12832/42552]\n",
      "loss: 0.308826  [16032/42552]\n",
      "loss: 0.287470  [19232/42552]\n",
      "loss: 0.302263  [22432/42552]\n",
      "loss: 0.209374  [25632/42552]\n",
      "loss: 0.220680  [28832/42552]\n",
      "loss: 0.282911  [32032/42552]\n",
      "loss: 0.477122  [35232/42552]\n",
      "loss: 0.311519  [38432/42552]\n",
      "loss: 0.362337  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.243786 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.319868  [   32/42552]\n",
      "loss: 0.308540  [ 3232/42552]\n",
      "loss: 0.197529  [ 6432/42552]\n",
      "loss: 0.218760  [ 9632/42552]\n",
      "loss: 0.221748  [12832/42552]\n",
      "loss: 0.317260  [16032/42552]\n",
      "loss: 0.218835  [19232/42552]\n",
      "loss: 0.378711  [22432/42552]\n",
      "loss: 0.466428  [25632/42552]\n",
      "loss: 0.271125  [28832/42552]\n",
      "loss: 0.225820  [32032/42552]\n",
      "loss: 0.419620  [35232/42552]\n",
      "loss: 0.274576  [38432/42552]\n",
      "loss: 0.425386  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.249163 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.459080  [   32/42552]\n",
      "loss: 0.274447  [ 3232/42552]\n",
      "loss: 0.291883  [ 6432/42552]\n",
      "loss: 0.231650  [ 9632/42552]\n",
      "loss: 0.260690  [12832/42552]\n",
      "loss: 0.520146  [16032/42552]\n",
      "loss: 0.365678  [19232/42552]\n",
      "loss: 0.444897  [22432/42552]\n",
      "loss: 0.340416  [25632/42552]\n",
      "loss: 0.231700  [28832/42552]\n",
      "loss: 0.176795  [32032/42552]\n",
      "loss: 0.340684  [35232/42552]\n",
      "loss: 0.414072  [38432/42552]\n",
      "loss: 0.219415  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.238296 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.181005  [   32/42552]\n",
      "loss: 0.451748  [ 3232/42552]\n",
      "loss: 0.279770  [ 6432/42552]\n",
      "loss: 0.419146  [ 9632/42552]\n",
      "loss: 0.360103  [12832/42552]\n",
      "loss: 0.284075  [16032/42552]\n",
      "loss: 0.350791  [19232/42552]\n",
      "loss: 0.246227  [22432/42552]\n",
      "loss: 0.268836  [25632/42552]\n",
      "loss: 0.299657  [28832/42552]\n",
      "loss: 0.346708  [32032/42552]\n",
      "loss: 0.177162  [35232/42552]\n",
      "loss: 0.226408  [38432/42552]\n",
      "loss: 0.296247  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.247430 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.253870  [   32/42552]\n",
      "loss: 0.285577  [ 3232/42552]\n",
      "loss: 0.260425  [ 6432/42552]\n",
      "loss: 0.369179  [ 9632/42552]\n",
      "loss: 0.135224  [12832/42552]\n",
      "loss: 0.200958  [16032/42552]\n",
      "loss: 0.286655  [19232/42552]\n",
      "loss: 0.262347  [22432/42552]\n",
      "loss: 0.277432  [25632/42552]\n",
      "loss: 0.333197  [28832/42552]\n",
      "loss: 0.245348  [32032/42552]\n",
      "loss: 0.208646  [35232/42552]\n",
      "loss: 0.406184  [38432/42552]\n",
      "loss: 0.249082  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.240421 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.249596  [   32/42552]\n",
      "loss: 0.451957  [ 3232/42552]\n",
      "loss: 0.207402  [ 6432/42552]\n",
      "loss: 0.288121  [ 9632/42552]\n",
      "loss: 0.304934  [12832/42552]\n",
      "loss: 0.339414  [16032/42552]\n",
      "loss: 0.307793  [19232/42552]\n",
      "loss: 0.391326  [22432/42552]\n",
      "loss: 0.329666  [25632/42552]\n",
      "loss: 0.415476  [28832/42552]\n",
      "loss: 0.368201  [32032/42552]\n",
      "loss: 0.178559  [35232/42552]\n",
      "loss: 0.258704  [38432/42552]\n",
      "loss: 0.272743  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.248222 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.361516  [   32/42552]\n",
      "loss: 0.267186  [ 3232/42552]\n",
      "loss: 0.191893  [ 6432/42552]\n",
      "loss: 0.266451  [ 9632/42552]\n",
      "loss: 0.279781  [12832/42552]\n",
      "loss: 0.539285  [16032/42552]\n",
      "loss: 0.102843  [19232/42552]\n",
      "loss: 0.329361  [22432/42552]\n",
      "loss: 0.382701  [25632/42552]\n",
      "loss: 0.295426  [28832/42552]\n",
      "loss: 0.204891  [32032/42552]\n",
      "loss: 0.221320  [35232/42552]\n",
      "loss: 0.285610  [38432/42552]\n",
      "loss: 0.168530  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.249491 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.332340  [   32/42552]\n",
      "loss: 0.333501  [ 3232/42552]\n",
      "loss: 0.263679  [ 6432/42552]\n",
      "loss: 0.174298  [ 9632/42552]\n",
      "loss: 0.248018  [12832/42552]\n",
      "loss: 0.222428  [16032/42552]\n",
      "loss: 0.379213  [19232/42552]\n",
      "loss: 0.215413  [22432/42552]\n",
      "loss: 0.267445  [25632/42552]\n",
      "loss: 0.565586  [28832/42552]\n",
      "loss: 0.310812  [32032/42552]\n",
      "loss: 0.250298  [35232/42552]\n",
      "loss: 0.297992  [38432/42552]\n",
      "loss: 0.217073  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.239519 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.348819  [   32/42552]\n",
      "loss: 0.284553  [ 3232/42552]\n",
      "loss: 0.306630  [ 6432/42552]\n",
      "loss: 0.300136  [ 9632/42552]\n",
      "loss: 0.246193  [12832/42552]\n",
      "loss: 0.402225  [16032/42552]\n",
      "loss: 0.229087  [19232/42552]\n",
      "loss: 0.392010  [22432/42552]\n",
      "loss: 0.409505  [25632/42552]\n",
      "loss: 0.240249  [28832/42552]\n",
      "loss: 0.257363  [32032/42552]\n",
      "loss: 0.202852  [35232/42552]\n",
      "loss: 0.257532  [38432/42552]\n",
      "loss: 0.328588  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.246118 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.252880  [   32/42552]\n",
      "loss: 0.208118  [ 3232/42552]\n",
      "loss: 0.347095  [ 6432/42552]\n",
      "loss: 0.255070  [ 9632/42552]\n",
      "loss: 0.269927  [12832/42552]\n",
      "loss: 0.235394  [16032/42552]\n",
      "loss: 0.260363  [19232/42552]\n",
      "loss: 0.276588  [22432/42552]\n",
      "loss: 0.357740  [25632/42552]\n",
      "loss: 0.367713  [28832/42552]\n",
      "loss: 0.255345  [32032/42552]\n",
      "loss: 0.488775  [35232/42552]\n",
      "loss: 0.195380  [38432/42552]\n",
      "loss: 0.295002  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.250980 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.247658  [   32/42552]\n",
      "loss: 0.184373  [ 3232/42552]\n",
      "loss: 0.226098  [ 6432/42552]\n",
      "loss: 0.271011  [ 9632/42552]\n",
      "loss: 0.284437  [12832/42552]\n",
      "loss: 0.289345  [16032/42552]\n",
      "loss: 0.506781  [19232/42552]\n",
      "loss: 0.257247  [22432/42552]\n",
      "loss: 0.249652  [25632/42552]\n",
      "loss: 0.296670  [28832/42552]\n",
      "loss: 0.322021  [32032/42552]\n",
      "loss: 0.526317  [35232/42552]\n",
      "loss: 0.414551  [38432/42552]\n",
      "loss: 0.246395  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.242039 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.234399  [   32/42552]\n",
      "loss: 0.329602  [ 3232/42552]\n",
      "loss: 0.169423  [ 6432/42552]\n",
      "loss: 0.417253  [ 9632/42552]\n",
      "loss: 0.450876  [12832/42552]\n",
      "loss: 0.278279  [16032/42552]\n",
      "loss: 0.320893  [19232/42552]\n",
      "loss: 0.246497  [22432/42552]\n",
      "loss: 0.283112  [25632/42552]\n",
      "loss: 0.184467  [28832/42552]\n",
      "loss: 0.195313  [32032/42552]\n",
      "loss: 0.347870  [35232/42552]\n",
      "loss: 0.227981  [38432/42552]\n",
      "loss: 0.205356  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.247786 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.209334  [   32/42552]\n",
      "loss: 0.187187  [ 3232/42552]\n",
      "loss: 0.287427  [ 6432/42552]\n",
      "loss: 0.222086  [ 9632/42552]\n",
      "loss: 0.253357  [12832/42552]\n",
      "loss: 0.332924  [16032/42552]\n",
      "loss: 0.356176  [19232/42552]\n",
      "loss: 0.247296  [22432/42552]\n",
      "loss: 0.365879  [25632/42552]\n",
      "loss: 0.210279  [28832/42552]\n",
      "loss: 0.372259  [32032/42552]\n",
      "loss: 0.282199  [35232/42552]\n",
      "loss: 0.455347  [38432/42552]\n",
      "loss: 0.194938  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.257042 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.313380  [   32/42552]\n",
      "loss: 0.342472  [ 3232/42552]\n",
      "loss: 0.284594  [ 6432/42552]\n",
      "loss: 0.216311  [ 9632/42552]\n",
      "loss: 0.232275  [12832/42552]\n",
      "loss: 0.246340  [16032/42552]\n",
      "loss: 0.216381  [19232/42552]\n",
      "loss: 0.269350  [22432/42552]\n",
      "loss: 0.268405  [25632/42552]\n",
      "loss: 0.161182  [28832/42552]\n",
      "loss: 0.285857  [32032/42552]\n",
      "loss: 0.274850  [35232/42552]\n",
      "loss: 0.185109  [38432/42552]\n",
      "loss: 0.339071  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.250053 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.281869  [   32/42552]\n",
      "loss: 0.258318  [ 3232/42552]\n",
      "loss: 0.199839  [ 6432/42552]\n",
      "loss: 0.366594  [ 9632/42552]\n",
      "loss: 0.317748  [12832/42552]\n",
      "loss: 0.273439  [16032/42552]\n",
      "loss: 0.277827  [19232/42552]\n",
      "loss: 0.306954  [22432/42552]\n",
      "loss: 0.131654  [25632/42552]\n",
      "loss: 0.204320  [28832/42552]\n",
      "loss: 0.250718  [32032/42552]\n",
      "loss: 0.377483  [35232/42552]\n",
      "loss: 0.400153  [38432/42552]\n",
      "loss: 0.321826  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.258738 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.202331  [   32/42552]\n",
      "loss: 0.223372  [ 3232/42552]\n",
      "loss: 0.221704  [ 6432/42552]\n",
      "loss: 0.215920  [ 9632/42552]\n",
      "loss: 0.198898  [12832/42552]\n",
      "loss: 0.211006  [16032/42552]\n",
      "loss: 0.234920  [19232/42552]\n",
      "loss: 0.248572  [22432/42552]\n",
      "loss: 0.268073  [25632/42552]\n",
      "loss: 0.426524  [28832/42552]\n",
      "loss: 0.249904  [32032/42552]\n",
      "loss: 0.364100  [35232/42552]\n",
      "loss: 0.316327  [38432/42552]\n",
      "loss: 0.451420  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.237470 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.374847  [   32/42552]\n",
      "loss: 0.242364  [ 3232/42552]\n",
      "loss: 0.260074  [ 6432/42552]\n",
      "loss: 0.382872  [ 9632/42552]\n",
      "loss: 0.138046  [12832/42552]\n",
      "loss: 0.188622  [16032/42552]\n",
      "loss: 0.273256  [19232/42552]\n",
      "loss: 0.216697  [22432/42552]\n",
      "loss: 0.306902  [25632/42552]\n",
      "loss: 0.257213  [28832/42552]\n",
      "loss: 0.245359  [32032/42552]\n",
      "loss: 0.166357  [35232/42552]\n",
      "loss: 0.285876  [38432/42552]\n",
      "loss: 0.253129  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.240733 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.223565  [   32/42552]\n",
      "loss: 0.303279  [ 3232/42552]\n",
      "loss: 0.275475  [ 6432/42552]\n",
      "loss: 0.299918  [ 9632/42552]\n",
      "loss: 0.245627  [12832/42552]\n",
      "loss: 0.314079  [16032/42552]\n",
      "loss: 0.211582  [19232/42552]\n",
      "loss: 0.325644  [22432/42552]\n",
      "loss: 0.169429  [25632/42552]\n",
      "loss: 0.256799  [28832/42552]\n",
      "loss: 0.365135  [32032/42552]\n",
      "loss: 0.318195  [35232/42552]\n",
      "loss: 0.251898  [38432/42552]\n",
      "loss: 0.332842  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.240049 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.293990  [   32/42552]\n",
      "loss: 0.321935  [ 3232/42552]\n",
      "loss: 0.178280  [ 6432/42552]\n",
      "loss: 0.316895  [ 9632/42552]\n",
      "loss: 0.312664  [12832/42552]\n",
      "loss: 0.241375  [16032/42552]\n",
      "loss: 0.499155  [19232/42552]\n",
      "loss: 0.265926  [22432/42552]\n",
      "loss: 0.377480  [25632/42552]\n",
      "loss: 0.191520  [28832/42552]\n",
      "loss: 0.265842  [32032/42552]\n",
      "loss: 0.317289  [35232/42552]\n",
      "loss: 0.427155  [38432/42552]\n",
      "loss: 0.242945  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.240445 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.301129  [   32/42552]\n",
      "loss: 0.252553  [ 3232/42552]\n",
      "loss: 0.235982  [ 6432/42552]\n",
      "loss: 0.303621  [ 9632/42552]\n",
      "loss: 0.296762  [12832/42552]\n",
      "loss: 0.315664  [16032/42552]\n",
      "loss: 0.241326  [19232/42552]\n",
      "loss: 0.144415  [22432/42552]\n",
      "loss: 0.371076  [25632/42552]\n",
      "loss: 0.388649  [28832/42552]\n",
      "loss: 0.305164  [32032/42552]\n",
      "loss: 0.331450  [35232/42552]\n",
      "loss: 0.146575  [38432/42552]\n",
      "loss: 0.165342  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.244788 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.230149  [   32/42552]\n",
      "loss: 0.266193  [ 3232/42552]\n",
      "loss: 0.190857  [ 6432/42552]\n",
      "loss: 0.248989  [ 9632/42552]\n",
      "loss: 0.199279  [12832/42552]\n",
      "loss: 0.285457  [16032/42552]\n",
      "loss: 0.266178  [19232/42552]\n",
      "loss: 0.338366  [22432/42552]\n",
      "loss: 0.292011  [25632/42552]\n",
      "loss: 0.239140  [28832/42552]\n",
      "loss: 0.321676  [32032/42552]\n",
      "loss: 0.268751  [35232/42552]\n",
      "loss: 0.331505  [38432/42552]\n",
      "loss: 0.286417  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.239384 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.214906  [   32/42552]\n",
      "loss: 0.130470  [ 3232/42552]\n",
      "loss: 0.211018  [ 6432/42552]\n",
      "loss: 0.314621  [ 9632/42552]\n",
      "loss: 0.307030  [12832/42552]\n",
      "loss: 0.217629  [16032/42552]\n",
      "loss: 0.358247  [19232/42552]\n",
      "loss: 0.282066  [22432/42552]\n",
      "loss: 0.239235  [25632/42552]\n",
      "loss: 0.366750  [28832/42552]\n",
      "loss: 0.320794  [32032/42552]\n",
      "loss: 0.224385  [35232/42552]\n",
      "loss: 0.300112  [38432/42552]\n",
      "loss: 0.294880  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.238639 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.178428  [   32/42552]\n",
      "loss: 0.210777  [ 3232/42552]\n",
      "loss: 0.117197  [ 6432/42552]\n",
      "loss: 0.284081  [ 9632/42552]\n",
      "loss: 0.272279  [12832/42552]\n",
      "loss: 0.328197  [16032/42552]\n",
      "loss: 0.453218  [19232/42552]\n",
      "loss: 0.172473  [22432/42552]\n",
      "loss: 0.274077  [25632/42552]\n",
      "loss: 0.231185  [28832/42552]\n",
      "loss: 0.194175  [32032/42552]\n",
      "loss: 0.226395  [35232/42552]\n",
      "loss: 0.266926  [38432/42552]\n",
      "loss: 0.265091  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.240217 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.219185  [   32/42552]\n",
      "loss: 0.148471  [ 3232/42552]\n",
      "loss: 0.253068  [ 6432/42552]\n",
      "loss: 0.309537  [ 9632/42552]\n",
      "loss: 0.236512  [12832/42552]\n",
      "loss: 0.218890  [16032/42552]\n",
      "loss: 0.305690  [19232/42552]\n",
      "loss: 0.231437  [22432/42552]\n",
      "loss: 0.396087  [25632/42552]\n",
      "loss: 0.292003  [28832/42552]\n",
      "loss: 0.218065  [32032/42552]\n",
      "loss: 0.221994  [35232/42552]\n",
      "loss: 0.221164  [38432/42552]\n",
      "loss: 0.228759  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.248033 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.203562  [   32/42552]\n",
      "loss: 0.349603  [ 3232/42552]\n",
      "loss: 0.287037  [ 6432/42552]\n",
      "loss: 0.380197  [ 9632/42552]\n",
      "loss: 0.220403  [12832/42552]\n",
      "loss: 0.328137  [16032/42552]\n",
      "loss: 0.398904  [19232/42552]\n",
      "loss: 0.256934  [22432/42552]\n",
      "loss: 0.225411  [25632/42552]\n",
      "loss: 0.271506  [28832/42552]\n",
      "loss: 0.270965  [32032/42552]\n",
      "loss: 0.284428  [35232/42552]\n",
      "loss: 0.241933  [38432/42552]\n",
      "loss: 0.243996  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.240676 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.364075  [   32/42552]\n",
      "loss: 0.193070  [ 3232/42552]\n",
      "loss: 0.256811  [ 6432/42552]\n",
      "loss: 0.253314  [ 9632/42552]\n",
      "loss: 0.278162  [12832/42552]\n",
      "loss: 0.365510  [16032/42552]\n",
      "loss: 0.215273  [19232/42552]\n",
      "loss: 0.417808  [22432/42552]\n",
      "loss: 0.357972  [25632/42552]\n",
      "loss: 0.373266  [28832/42552]\n",
      "loss: 0.362255  [32032/42552]\n",
      "loss: 0.415211  [35232/42552]\n",
      "loss: 0.228722  [38432/42552]\n",
      "loss: 0.304566  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.238106 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.389813  [   32/42552]\n",
      "loss: 0.278563  [ 3232/42552]\n",
      "loss: 0.200754  [ 6432/42552]\n",
      "loss: 0.193036  [ 9632/42552]\n",
      "loss: 0.261068  [12832/42552]\n",
      "loss: 0.324268  [16032/42552]\n",
      "loss: 0.235768  [19232/42552]\n",
      "loss: 0.183511  [22432/42552]\n",
      "loss: 0.283745  [25632/42552]\n",
      "loss: 0.451562  [28832/42552]\n",
      "loss: 0.329777  [32032/42552]\n",
      "loss: 0.277217  [35232/42552]\n",
      "loss: 0.279453  [38432/42552]\n",
      "loss: 0.284179  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.240995 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.190464  [   32/42552]\n",
      "loss: 0.161084  [ 3232/42552]\n",
      "loss: 0.268379  [ 6432/42552]\n",
      "loss: 0.169853  [ 9632/42552]\n",
      "loss: 0.288646  [12832/42552]\n",
      "loss: 0.264152  [16032/42552]\n",
      "loss: 0.423453  [19232/42552]\n",
      "loss: 0.271877  [22432/42552]\n",
      "loss: 0.314580  [25632/42552]\n",
      "loss: 0.285132  [28832/42552]\n",
      "loss: 0.171780  [32032/42552]\n",
      "loss: 0.273563  [35232/42552]\n",
      "loss: 0.298481  [38432/42552]\n",
      "loss: 0.268819  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.236548 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.342274  [   32/42552]\n",
      "loss: 0.399805  [ 3232/42552]\n",
      "loss: 0.151823  [ 6432/42552]\n",
      "loss: 0.304958  [ 9632/42552]\n",
      "loss: 0.189224  [12832/42552]\n",
      "loss: 0.321130  [16032/42552]\n",
      "loss: 0.151452  [19232/42552]\n",
      "loss: 0.270831  [22432/42552]\n",
      "loss: 0.302665  [25632/42552]\n",
      "loss: 0.103671  [28832/42552]\n",
      "loss: 0.271992  [32032/42552]\n",
      "loss: 0.207360  [35232/42552]\n",
      "loss: 0.253544  [38432/42552]\n",
      "loss: 0.406501  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.240975 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.205298  [   32/42552]\n",
      "loss: 0.237009  [ 3232/42552]\n",
      "loss: 0.419772  [ 6432/42552]\n",
      "loss: 0.405675  [ 9632/42552]\n",
      "loss: 0.311008  [12832/42552]\n",
      "loss: 0.223096  [16032/42552]\n",
      "loss: 0.322255  [19232/42552]\n",
      "loss: 0.282578  [22432/42552]\n",
      "loss: 0.175413  [25632/42552]\n",
      "loss: 0.182462  [28832/42552]\n",
      "loss: 0.364515  [32032/42552]\n",
      "loss: 0.366785  [35232/42552]\n",
      "loss: 0.315826  [38432/42552]\n",
      "loss: 0.342666  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.237802 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.195538  [   32/42552]\n",
      "loss: 0.295589  [ 3232/42552]\n",
      "loss: 0.202640  [ 6432/42552]\n",
      "loss: 0.173481  [ 9632/42552]\n",
      "loss: 0.433654  [12832/42552]\n",
      "loss: 0.382378  [16032/42552]\n",
      "loss: 0.391820  [19232/42552]\n",
      "loss: 0.176392  [22432/42552]\n",
      "loss: 0.304865  [25632/42552]\n",
      "loss: 0.286927  [28832/42552]\n",
      "loss: 0.158697  [32032/42552]\n",
      "loss: 0.253377  [35232/42552]\n",
      "loss: 0.455862  [38432/42552]\n",
      "loss: 0.145702  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.239705 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.410558  [   32/42552]\n",
      "loss: 0.346101  [ 3232/42552]\n",
      "loss: 0.203775  [ 6432/42552]\n",
      "loss: 0.491928  [ 9632/42552]\n",
      "loss: 0.384455  [12832/42552]\n",
      "loss: 0.345248  [16032/42552]\n",
      "loss: 0.310457  [19232/42552]\n",
      "loss: 0.401883  [22432/42552]\n",
      "loss: 0.345374  [25632/42552]\n",
      "loss: 0.201274  [28832/42552]\n",
      "loss: 0.342410  [32032/42552]\n",
      "loss: 0.148604  [35232/42552]\n",
      "loss: 0.277473  [38432/42552]\n",
      "loss: 0.224238  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.239357 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.363499  [   32/42552]\n",
      "loss: 0.210373  [ 3232/42552]\n",
      "loss: 0.227764  [ 6432/42552]\n",
      "loss: 0.231620  [ 9632/42552]\n",
      "loss: 0.281015  [12832/42552]\n",
      "loss: 0.418517  [16032/42552]\n",
      "loss: 0.221429  [19232/42552]\n",
      "loss: 0.372274  [22432/42552]\n",
      "loss: 0.492732  [25632/42552]\n",
      "loss: 0.209801  [28832/42552]\n",
      "loss: 0.129396  [32032/42552]\n",
      "loss: 0.388136  [35232/42552]\n",
      "loss: 0.258861  [38432/42552]\n",
      "loss: 0.186291  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.268164 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.184246  [   32/42552]\n",
      "loss: 0.203827  [ 3232/42552]\n",
      "loss: 0.317708  [ 6432/42552]\n",
      "loss: 0.315457  [ 9632/42552]\n",
      "loss: 0.373117  [12832/42552]\n",
      "loss: 0.516889  [16032/42552]\n",
      "loss: 0.286595  [19232/42552]\n",
      "loss: 0.286866  [22432/42552]\n",
      "loss: 0.168430  [25632/42552]\n",
      "loss: 0.236896  [28832/42552]\n",
      "loss: 0.282959  [32032/42552]\n",
      "loss: 0.356171  [35232/42552]\n",
      "loss: 0.374090  [38432/42552]\n",
      "loss: 0.185996  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.235298 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.366668  [   32/42552]\n",
      "loss: 0.267289  [ 3232/42552]\n",
      "loss: 0.564119  [ 6432/42552]\n",
      "loss: 0.399206  [ 9632/42552]\n",
      "loss: 0.323938  [12832/42552]\n",
      "loss: 0.187086  [16032/42552]\n",
      "loss: 0.410628  [19232/42552]\n",
      "loss: 0.347770  [22432/42552]\n",
      "loss: 0.224554  [25632/42552]\n",
      "loss: 0.219504  [28832/42552]\n",
      "loss: 0.387078  [32032/42552]\n",
      "loss: 0.396413  [35232/42552]\n",
      "loss: 0.421936  [38432/42552]\n",
      "loss: 0.271417  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.241642 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.382105  [   32/42552]\n",
      "loss: 0.334987  [ 3232/42552]\n",
      "loss: 0.301888  [ 6432/42552]\n",
      "loss: 0.258625  [ 9632/42552]\n",
      "loss: 0.225107  [12832/42552]\n",
      "loss: 0.269638  [16032/42552]\n",
      "loss: 0.250737  [19232/42552]\n",
      "loss: 0.223361  [22432/42552]\n",
      "loss: 0.382393  [25632/42552]\n",
      "loss: 0.270502  [28832/42552]\n",
      "loss: 0.218624  [32032/42552]\n",
      "loss: 0.306022  [35232/42552]\n",
      "loss: 0.345717  [38432/42552]\n",
      "loss: 0.334840  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.242284 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.376297  [   32/42552]\n",
      "loss: 0.305672  [ 3232/42552]\n",
      "loss: 0.231316  [ 6432/42552]\n",
      "loss: 0.319974  [ 9632/42552]\n",
      "loss: 0.309314  [12832/42552]\n",
      "loss: 0.315204  [16032/42552]\n",
      "loss: 0.217258  [19232/42552]\n",
      "loss: 0.204528  [22432/42552]\n",
      "loss: 0.389407  [25632/42552]\n",
      "loss: 0.206036  [28832/42552]\n",
      "loss: 0.243220  [32032/42552]\n",
      "loss: 0.242976  [35232/42552]\n",
      "loss: 0.366061  [38432/42552]\n",
      "loss: 0.149677  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.237569 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.198979  [   32/42552]\n",
      "loss: 0.474090  [ 3232/42552]\n",
      "loss: 0.207748  [ 6432/42552]\n",
      "loss: 0.256989  [ 9632/42552]\n",
      "loss: 0.222190  [12832/42552]\n",
      "loss: 0.262710  [16032/42552]\n",
      "loss: 0.213452  [19232/42552]\n",
      "loss: 0.224605  [22432/42552]\n",
      "loss: 0.297663  [25632/42552]\n",
      "loss: 0.135897  [28832/42552]\n",
      "loss: 0.236064  [32032/42552]\n",
      "loss: 0.156305  [35232/42552]\n",
      "loss: 0.148853  [38432/42552]\n",
      "loss: 0.285728  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.236712 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.153365  [   32/42552]\n",
      "loss: 0.213897  [ 3232/42552]\n",
      "loss: 0.325587  [ 6432/42552]\n",
      "loss: 0.215454  [ 9632/42552]\n",
      "loss: 0.405946  [12832/42552]\n",
      "loss: 0.411449  [16032/42552]\n",
      "loss: 0.264115  [19232/42552]\n",
      "loss: 0.216067  [22432/42552]\n",
      "loss: 0.311688  [25632/42552]\n",
      "loss: 0.264095  [28832/42552]\n",
      "loss: 0.349737  [32032/42552]\n",
      "loss: 0.271398  [35232/42552]\n",
      "loss: 0.269355  [38432/42552]\n",
      "loss: 0.190240  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.238529 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.344045  [   32/42552]\n",
      "loss: 0.380385  [ 3232/42552]\n",
      "loss: 0.429650  [ 6432/42552]\n",
      "loss: 0.224024  [ 9632/42552]\n",
      "loss: 0.240761  [12832/42552]\n",
      "loss: 0.200815  [16032/42552]\n",
      "loss: 0.315930  [19232/42552]\n",
      "loss: 0.319368  [22432/42552]\n",
      "loss: 0.337572  [25632/42552]\n",
      "loss: 0.172318  [28832/42552]\n",
      "loss: 0.143724  [32032/42552]\n",
      "loss: 0.230208  [35232/42552]\n",
      "loss: 0.240556  [38432/42552]\n",
      "loss: 0.167784  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.237977 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.286346  [   32/42552]\n",
      "loss: 0.292794  [ 3232/42552]\n",
      "loss: 0.347527  [ 6432/42552]\n",
      "loss: 0.297716  [ 9632/42552]\n",
      "loss: 0.232543  [12832/42552]\n",
      "loss: 0.258419  [16032/42552]\n",
      "loss: 0.329492  [19232/42552]\n",
      "loss: 0.263478  [22432/42552]\n",
      "loss: 0.328835  [25632/42552]\n",
      "loss: 0.347683  [28832/42552]\n",
      "loss: 0.290192  [32032/42552]\n",
      "loss: 0.312620  [35232/42552]\n",
      "loss: 0.259866  [38432/42552]\n",
      "loss: 0.479479  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.239876 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.333263  [   32/42552]\n",
      "loss: 0.270866  [ 3232/42552]\n",
      "loss: 0.273641  [ 6432/42552]\n",
      "loss: 0.244918  [ 9632/42552]\n",
      "loss: 0.330484  [12832/42552]\n",
      "loss: 0.321984  [16032/42552]\n",
      "loss: 0.192436  [19232/42552]\n",
      "loss: 0.571712  [22432/42552]\n",
      "loss: 0.307000  [25632/42552]\n",
      "loss: 0.396185  [28832/42552]\n",
      "loss: 0.124832  [32032/42552]\n",
      "loss: 0.222113  [35232/42552]\n",
      "loss: 0.413601  [38432/42552]\n",
      "loss: 0.437522  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.234673 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.284490  [   32/42552]\n",
      "loss: 0.311002  [ 3232/42552]\n",
      "loss: 0.289426  [ 6432/42552]\n",
      "loss: 0.403469  [ 9632/42552]\n",
      "loss: 0.140581  [12832/42552]\n",
      "loss: 0.382457  [16032/42552]\n",
      "loss: 0.459704  [19232/42552]\n",
      "loss: 0.284007  [22432/42552]\n",
      "loss: 0.260407  [25632/42552]\n",
      "loss: 0.254844  [28832/42552]\n",
      "loss: 0.136804  [32032/42552]\n",
      "loss: 0.280279  [35232/42552]\n",
      "loss: 0.224558  [38432/42552]\n",
      "loss: 0.348886  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.258973 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.171508  [   32/42552]\n",
      "loss: 0.250957  [ 3232/42552]\n",
      "loss: 0.283472  [ 6432/42552]\n",
      "loss: 0.219860  [ 9632/42552]\n",
      "loss: 0.230320  [12832/42552]\n",
      "loss: 0.230005  [16032/42552]\n",
      "loss: 0.414110  [19232/42552]\n",
      "loss: 0.356838  [22432/42552]\n",
      "loss: 0.353642  [25632/42552]\n",
      "loss: 0.413443  [28832/42552]\n",
      "loss: 0.278751  [32032/42552]\n",
      "loss: 0.513871  [35232/42552]\n",
      "loss: 0.208154  [38432/42552]\n",
      "loss: 0.415499  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.241306 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.298819  [   32/42552]\n",
      "loss: 0.136935  [ 3232/42552]\n",
      "loss: 0.340325  [ 6432/42552]\n",
      "loss: 0.206179  [ 9632/42552]\n",
      "loss: 0.234589  [12832/42552]\n",
      "loss: 0.189132  [16032/42552]\n",
      "loss: 0.232297  [19232/42552]\n",
      "loss: 0.273639  [22432/42552]\n",
      "loss: 0.326742  [25632/42552]\n",
      "loss: 0.320951  [28832/42552]\n",
      "loss: 0.290573  [32032/42552]\n",
      "loss: 0.178928  [35232/42552]\n",
      "loss: 0.251653  [38432/42552]\n",
      "loss: 0.353923  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.237656 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.401967  [   32/42552]\n",
      "loss: 0.249138  [ 3232/42552]\n",
      "loss: 0.280914  [ 6432/42552]\n",
      "loss: 0.261412  [ 9632/42552]\n",
      "loss: 0.270020  [12832/42552]\n",
      "loss: 0.356420  [16032/42552]\n",
      "loss: 0.228927  [19232/42552]\n",
      "loss: 0.362267  [22432/42552]\n",
      "loss: 0.264769  [25632/42552]\n",
      "loss: 0.265786  [28832/42552]\n",
      "loss: 0.359228  [32032/42552]\n",
      "loss: 0.216426  [35232/42552]\n",
      "loss: 0.252263  [38432/42552]\n",
      "loss: 0.176145  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.239965 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.255025  [   32/42552]\n",
      "loss: 0.472262  [ 3232/42552]\n",
      "loss: 0.312780  [ 6432/42552]\n",
      "loss: 0.181214  [ 9632/42552]\n",
      "loss: 0.280529  [12832/42552]\n",
      "loss: 0.214464  [16032/42552]\n",
      "loss: 0.310037  [19232/42552]\n",
      "loss: 0.236925  [22432/42552]\n",
      "loss: 0.390491  [25632/42552]\n",
      "loss: 0.425569  [28832/42552]\n",
      "loss: 0.203751  [32032/42552]\n",
      "loss: 0.284374  [35232/42552]\n",
      "loss: 0.242088  [38432/42552]\n",
      "loss: 0.284247  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.233870 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.207366  [   32/42552]\n",
      "loss: 0.187086  [ 3232/42552]\n",
      "loss: 0.286780  [ 6432/42552]\n",
      "loss: 0.344256  [ 9632/42552]\n",
      "loss: 0.319553  [12832/42552]\n",
      "loss: 0.193941  [16032/42552]\n",
      "loss: 0.200125  [19232/42552]\n",
      "loss: 0.500395  [22432/42552]\n",
      "loss: 0.254496  [25632/42552]\n",
      "loss: 0.265577  [28832/42552]\n",
      "loss: 0.396925  [32032/42552]\n",
      "loss: 0.376005  [35232/42552]\n",
      "loss: 0.162188  [38432/42552]\n",
      "loss: 0.413595  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.237805 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.273275  [   32/42552]\n",
      "loss: 0.212158  [ 3232/42552]\n",
      "loss: 0.203999  [ 6432/42552]\n",
      "loss: 0.259340  [ 9632/42552]\n",
      "loss: 0.107183  [12832/42552]\n",
      "loss: 0.226167  [16032/42552]\n",
      "loss: 0.273527  [19232/42552]\n",
      "loss: 0.246411  [22432/42552]\n",
      "loss: 0.192906  [25632/42552]\n",
      "loss: 0.231899  [28832/42552]\n",
      "loss: 0.258794  [32032/42552]\n",
      "loss: 0.247381  [35232/42552]\n",
      "loss: 0.236233  [38432/42552]\n",
      "loss: 0.185690  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.238876 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.187397  [   32/42552]\n",
      "loss: 0.150581  [ 3232/42552]\n",
      "loss: 0.212325  [ 6432/42552]\n",
      "loss: 0.345356  [ 9632/42552]\n",
      "loss: 0.188271  [12832/42552]\n",
      "loss: 0.250237  [16032/42552]\n",
      "loss: 0.186550  [19232/42552]\n",
      "loss: 0.369494  [22432/42552]\n",
      "loss: 0.183414  [25632/42552]\n",
      "loss: 0.266733  [28832/42552]\n",
      "loss: 0.198355  [32032/42552]\n",
      "loss: 0.234430  [35232/42552]\n",
      "loss: 0.339938  [38432/42552]\n",
      "loss: 0.346590  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.234397 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.313862  [   32/42552]\n",
      "loss: 0.242964  [ 3232/42552]\n",
      "loss: 0.376523  [ 6432/42552]\n",
      "loss: 0.173054  [ 9632/42552]\n",
      "loss: 0.237154  [12832/42552]\n",
      "loss: 0.204142  [16032/42552]\n",
      "loss: 0.309233  [19232/42552]\n",
      "loss: 0.200238  [22432/42552]\n",
      "loss: 0.352626  [25632/42552]\n",
      "loss: 0.169331  [28832/42552]\n",
      "loss: 0.304562  [32032/42552]\n",
      "loss: 0.326612  [35232/42552]\n",
      "loss: 0.221076  [38432/42552]\n",
      "loss: 0.285639  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.239064 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.173084  [   32/42552]\n",
      "loss: 0.275824  [ 3232/42552]\n",
      "loss: 0.225974  [ 6432/42552]\n",
      "loss: 0.307028  [ 9632/42552]\n",
      "loss: 0.282846  [12832/42552]\n",
      "loss: 0.173359  [16032/42552]\n",
      "loss: 0.353732  [19232/42552]\n",
      "loss: 0.301524  [22432/42552]\n",
      "loss: 0.355265  [25632/42552]\n",
      "loss: 0.368384  [28832/42552]\n",
      "loss: 0.344467  [32032/42552]\n",
      "loss: 0.254109  [35232/42552]\n",
      "loss: 0.302078  [38432/42552]\n",
      "loss: 0.309839  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.241912 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.230064  [   32/42552]\n",
      "loss: 0.221051  [ 3232/42552]\n",
      "loss: 0.288566  [ 6432/42552]\n",
      "loss: 0.299007  [ 9632/42552]\n",
      "loss: 0.170022  [12832/42552]\n",
      "loss: 0.299270  [16032/42552]\n",
      "loss: 0.222210  [19232/42552]\n",
      "loss: 0.251347  [22432/42552]\n",
      "loss: 0.179043  [25632/42552]\n",
      "loss: 0.267059  [28832/42552]\n",
      "loss: 0.366504  [32032/42552]\n",
      "loss: 0.161368  [35232/42552]\n",
      "loss: 0.220694  [38432/42552]\n",
      "loss: 0.206835  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.235116 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.298512  [   32/42552]\n",
      "loss: 0.267612  [ 3232/42552]\n",
      "loss: 0.214996  [ 6432/42552]\n",
      "loss: 0.228896  [ 9632/42552]\n",
      "loss: 0.175546  [12832/42552]\n",
      "loss: 0.415503  [16032/42552]\n",
      "loss: 0.292641  [19232/42552]\n",
      "loss: 0.270550  [22432/42552]\n",
      "loss: 0.277010  [25632/42552]\n",
      "loss: 0.394701  [28832/42552]\n",
      "loss: 0.291500  [32032/42552]\n",
      "loss: 0.194143  [35232/42552]\n",
      "loss: 0.289866  [38432/42552]\n",
      "loss: 0.235335  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.232620 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.234145  [   32/42552]\n",
      "loss: 0.264545  [ 3232/42552]\n",
      "loss: 0.294013  [ 6432/42552]\n",
      "loss: 0.206998  [ 9632/42552]\n",
      "loss: 0.263299  [12832/42552]\n",
      "loss: 0.285931  [16032/42552]\n",
      "loss: 0.199472  [19232/42552]\n",
      "loss: 0.334139  [22432/42552]\n",
      "loss: 0.265644  [25632/42552]\n",
      "loss: 0.278014  [28832/42552]\n",
      "loss: 0.192987  [32032/42552]\n",
      "loss: 0.151220  [35232/42552]\n",
      "loss: 0.326512  [38432/42552]\n",
      "loss: 0.208618  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.235274 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.285185  [   32/42552]\n",
      "loss: 0.247131  [ 3232/42552]\n",
      "loss: 0.456562  [ 6432/42552]\n",
      "loss: 0.259085  [ 9632/42552]\n",
      "loss: 0.372039  [12832/42552]\n",
      "loss: 0.182058  [16032/42552]\n",
      "loss: 0.477301  [19232/42552]\n",
      "loss: 0.322495  [22432/42552]\n",
      "loss: 0.273478  [25632/42552]\n",
      "loss: 0.374363  [28832/42552]\n",
      "loss: 0.353801  [32032/42552]\n",
      "loss: 0.275894  [35232/42552]\n",
      "loss: 0.318430  [38432/42552]\n",
      "loss: 0.313146  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.245761 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.258713  [   32/42552]\n",
      "loss: 0.220520  [ 3232/42552]\n",
      "loss: 0.247175  [ 6432/42552]\n",
      "loss: 0.307977  [ 9632/42552]\n",
      "loss: 0.196298  [12832/42552]\n",
      "loss: 0.269347  [16032/42552]\n",
      "loss: 0.370108  [19232/42552]\n",
      "loss: 0.328064  [22432/42552]\n",
      "loss: 0.261984  [25632/42552]\n",
      "loss: 0.245371  [28832/42552]\n",
      "loss: 0.412603  [32032/42552]\n",
      "loss: 0.229026  [35232/42552]\n",
      "loss: 0.194975  [38432/42552]\n",
      "loss: 0.299434  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.245757 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.327502  [   32/42552]\n",
      "loss: 0.145017  [ 3232/42552]\n",
      "loss: 0.190725  [ 6432/42552]\n",
      "loss: 0.274421  [ 9632/42552]\n",
      "loss: 0.324324  [12832/42552]\n",
      "loss: 0.393954  [16032/42552]\n",
      "loss: 0.387224  [19232/42552]\n",
      "loss: 0.332163  [22432/42552]\n",
      "loss: 0.383088  [25632/42552]\n",
      "loss: 0.235616  [28832/42552]\n",
      "loss: 0.322747  [32032/42552]\n",
      "loss: 0.282382  [35232/42552]\n",
      "loss: 0.340693  [38432/42552]\n",
      "loss: 0.362584  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.240785 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.306118  [   32/42552]\n",
      "loss: 0.246524  [ 3232/42552]\n",
      "loss: 0.302936  [ 6432/42552]\n",
      "loss: 0.305209  [ 9632/42552]\n",
      "loss: 0.358186  [12832/42552]\n",
      "loss: 0.241391  [16032/42552]\n",
      "loss: 0.301655  [19232/42552]\n",
      "loss: 0.212100  [22432/42552]\n",
      "loss: 0.230122  [25632/42552]\n",
      "loss: 0.319829  [28832/42552]\n",
      "loss: 0.278536  [32032/42552]\n",
      "loss: 0.307904  [35232/42552]\n",
      "loss: 0.200969  [38432/42552]\n",
      "loss: 0.220033  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.248156 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.335327  [   32/42552]\n",
      "loss: 0.159134  [ 3232/42552]\n",
      "loss: 0.259863  [ 6432/42552]\n",
      "loss: 0.222049  [ 9632/42552]\n",
      "loss: 0.178834  [12832/42552]\n",
      "loss: 0.212290  [16032/42552]\n",
      "loss: 0.292883  [19232/42552]\n",
      "loss: 0.243963  [22432/42552]\n",
      "loss: 0.277368  [25632/42552]\n",
      "loss: 0.306135  [28832/42552]\n",
      "loss: 0.152384  [32032/42552]\n",
      "loss: 0.459519  [35232/42552]\n",
      "loss: 0.195641  [38432/42552]\n",
      "loss: 0.252437  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.232742 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.191334  [   32/42552]\n",
      "loss: 0.140469  [ 3232/42552]\n",
      "loss: 0.189769  [ 6432/42552]\n",
      "loss: 0.180727  [ 9632/42552]\n",
      "loss: 0.203421  [12832/42552]\n",
      "loss: 0.259967  [16032/42552]\n",
      "loss: 0.472841  [19232/42552]\n",
      "loss: 0.243306  [22432/42552]\n",
      "loss: 0.373391  [25632/42552]\n",
      "loss: 0.318307  [28832/42552]\n",
      "loss: 0.313299  [32032/42552]\n",
      "loss: 0.188897  [35232/42552]\n",
      "loss: 0.390750  [38432/42552]\n",
      "loss: 0.277325  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.241169 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.208769  [   32/42552]\n",
      "loss: 0.280542  [ 3232/42552]\n",
      "loss: 0.266001  [ 6432/42552]\n",
      "loss: 0.220327  [ 9632/42552]\n",
      "loss: 0.279936  [12832/42552]\n",
      "loss: 0.221017  [16032/42552]\n",
      "loss: 0.286023  [19232/42552]\n",
      "loss: 0.210957  [22432/42552]\n",
      "loss: 0.236678  [25632/42552]\n",
      "loss: 0.219127  [28832/42552]\n",
      "loss: 0.284156  [32032/42552]\n",
      "loss: 0.260401  [35232/42552]\n",
      "loss: 0.151338  [38432/42552]\n",
      "loss: 0.288091  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.238148 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.359498  [   32/42552]\n",
      "loss: 0.399695  [ 3232/42552]\n",
      "loss: 0.235610  [ 6432/42552]\n",
      "loss: 0.428059  [ 9632/42552]\n",
      "loss: 0.143016  [12832/42552]\n",
      "loss: 0.321474  [16032/42552]\n",
      "loss: 0.250069  [19232/42552]\n",
      "loss: 0.176516  [22432/42552]\n",
      "loss: 0.137179  [25632/42552]\n",
      "loss: 0.267547  [28832/42552]\n",
      "loss: 0.241221  [32032/42552]\n",
      "loss: 0.345578  [35232/42552]\n",
      "loss: 0.249007  [38432/42552]\n",
      "loss: 0.477243  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.242002 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.369290  [   32/42552]\n",
      "loss: 0.224409  [ 3232/42552]\n",
      "loss: 0.292639  [ 6432/42552]\n",
      "loss: 0.507964  [ 9632/42552]\n",
      "loss: 0.334014  [12832/42552]\n",
      "loss: 0.199242  [16032/42552]\n",
      "loss: 0.259259  [19232/42552]\n",
      "loss: 0.172764  [22432/42552]\n",
      "loss: 0.270590  [25632/42552]\n",
      "loss: 0.175223  [28832/42552]\n",
      "loss: 0.272258  [32032/42552]\n",
      "loss: 0.272752  [35232/42552]\n",
      "loss: 0.289776  [38432/42552]\n",
      "loss: 0.267743  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.236309 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.385002  [   32/42552]\n",
      "loss: 0.540090  [ 3232/42552]\n",
      "loss: 0.253964  [ 6432/42552]\n",
      "loss: 0.225233  [ 9632/42552]\n",
      "loss: 0.198225  [12832/42552]\n",
      "loss: 0.185689  [16032/42552]\n",
      "loss: 0.255599  [19232/42552]\n",
      "loss: 0.324647  [22432/42552]\n",
      "loss: 0.360768  [25632/42552]\n",
      "loss: 0.328607  [28832/42552]\n",
      "loss: 0.178242  [32032/42552]\n",
      "loss: 0.325032  [35232/42552]\n",
      "loss: 0.287308  [38432/42552]\n",
      "loss: 0.235434  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.247801 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.219816  [   32/42552]\n",
      "loss: 0.409741  [ 3232/42552]\n",
      "loss: 0.236180  [ 6432/42552]\n",
      "loss: 0.262367  [ 9632/42552]\n",
      "loss: 0.205168  [12832/42552]\n",
      "loss: 0.320910  [16032/42552]\n",
      "loss: 0.448454  [19232/42552]\n",
      "loss: 0.298633  [22432/42552]\n",
      "loss: 0.168686  [25632/42552]\n",
      "loss: 0.199571  [28832/42552]\n",
      "loss: 0.292042  [32032/42552]\n",
      "loss: 0.317310  [35232/42552]\n",
      "loss: 0.295033  [38432/42552]\n",
      "loss: 0.320812  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.245726 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.460848  [   32/42552]\n",
      "loss: 0.443918  [ 3232/42552]\n",
      "loss: 0.214111  [ 6432/42552]\n",
      "loss: 0.364917  [ 9632/42552]\n",
      "loss: 0.504112  [12832/42552]\n",
      "loss: 0.266046  [16032/42552]\n",
      "loss: 0.400347  [19232/42552]\n",
      "loss: 0.235340  [22432/42552]\n",
      "loss: 0.197153  [25632/42552]\n",
      "loss: 0.294644  [28832/42552]\n",
      "loss: 0.243723  [32032/42552]\n",
      "loss: 0.195385  [35232/42552]\n",
      "loss: 0.252803  [38432/42552]\n",
      "loss: 0.284554  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.236777 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.220308  [   32/42552]\n",
      "loss: 0.402485  [ 3232/42552]\n",
      "loss: 0.217623  [ 6432/42552]\n",
      "loss: 0.168909  [ 9632/42552]\n",
      "loss: 0.252090  [12832/42552]\n",
      "loss: 0.371071  [16032/42552]\n",
      "loss: 0.176317  [19232/42552]\n",
      "loss: 0.336149  [22432/42552]\n",
      "loss: 0.313841  [25632/42552]\n",
      "loss: 0.332438  [28832/42552]\n",
      "loss: 0.204536  [32032/42552]\n",
      "loss: 0.283816  [35232/42552]\n",
      "loss: 0.280352  [38432/42552]\n",
      "loss: 0.267509  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.239396 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.281253  [   32/42552]\n",
      "loss: 0.314945  [ 3232/42552]\n",
      "loss: 0.268344  [ 6432/42552]\n",
      "loss: 0.276979  [ 9632/42552]\n",
      "loss: 0.311878  [12832/42552]\n",
      "loss: 0.363687  [16032/42552]\n",
      "loss: 0.214232  [19232/42552]\n",
      "loss: 0.147266  [22432/42552]\n",
      "loss: 0.272314  [25632/42552]\n",
      "loss: 0.254641  [28832/42552]\n",
      "loss: 0.280387  [32032/42552]\n",
      "loss: 0.238647  [35232/42552]\n",
      "loss: 0.350866  [38432/42552]\n",
      "loss: 0.337130  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.238217 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.381665  [   32/42552]\n",
      "loss: 0.203821  [ 3232/42552]\n",
      "loss: 0.245213  [ 6432/42552]\n",
      "loss: 0.375635  [ 9632/42552]\n",
      "loss: 0.272531  [12832/42552]\n",
      "loss: 0.330994  [16032/42552]\n",
      "loss: 0.187995  [19232/42552]\n",
      "loss: 0.191850  [22432/42552]\n",
      "loss: 0.344692  [25632/42552]\n",
      "loss: 0.330407  [28832/42552]\n",
      "loss: 0.231270  [32032/42552]\n",
      "loss: 0.219040  [35232/42552]\n",
      "loss: 0.360790  [38432/42552]\n",
      "loss: 0.323742  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.240755 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.227285  [   32/42552]\n",
      "loss: 0.228049  [ 3232/42552]\n",
      "loss: 0.252445  [ 6432/42552]\n",
      "loss: 0.147629  [ 9632/42552]\n",
      "loss: 0.269344  [12832/42552]\n",
      "loss: 0.197643  [16032/42552]\n",
      "loss: 0.316093  [19232/42552]\n",
      "loss: 0.279219  [22432/42552]\n",
      "loss: 0.382844  [25632/42552]\n",
      "loss: 0.372061  [28832/42552]\n",
      "loss: 0.289662  [32032/42552]\n",
      "loss: 0.304423  [35232/42552]\n",
      "loss: 0.112162  [38432/42552]\n",
      "loss: 0.296443  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.239353 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.307636  [   32/42552]\n",
      "loss: 0.327327  [ 3232/42552]\n",
      "loss: 0.275705  [ 6432/42552]\n",
      "loss: 0.273330  [ 9632/42552]\n",
      "loss: 0.192609  [12832/42552]\n",
      "loss: 0.266468  [16032/42552]\n",
      "loss: 0.277920  [19232/42552]\n",
      "loss: 0.239600  [22432/42552]\n",
      "loss: 0.212267  [25632/42552]\n",
      "loss: 0.296166  [28832/42552]\n",
      "loss: 0.210326  [32032/42552]\n",
      "loss: 0.245152  [35232/42552]\n",
      "loss: 0.446470  [38432/42552]\n",
      "loss: 0.269373  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.232398 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.316094  [   32/42552]\n",
      "loss: 0.262025  [ 3232/42552]\n",
      "loss: 0.211723  [ 6432/42552]\n",
      "loss: 0.265237  [ 9632/42552]\n",
      "loss: 0.443840  [12832/42552]\n",
      "loss: 0.227263  [16032/42552]\n",
      "loss: 0.196382  [19232/42552]\n",
      "loss: 0.362382  [22432/42552]\n",
      "loss: 0.270581  [25632/42552]\n",
      "loss: 0.161668  [28832/42552]\n",
      "loss: 0.598051  [32032/42552]\n",
      "loss: 0.394472  [35232/42552]\n",
      "loss: 0.301547  [38432/42552]\n",
      "loss: 0.264205  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.233839 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.345594  [   32/42552]\n",
      "loss: 0.447025  [ 3232/42552]\n",
      "loss: 0.424306  [ 6432/42552]\n",
      "loss: 0.354077  [ 9632/42552]\n",
      "loss: 0.229601  [12832/42552]\n",
      "loss: 0.189402  [16032/42552]\n",
      "loss: 0.291228  [19232/42552]\n",
      "loss: 0.220687  [22432/42552]\n",
      "loss: 0.214693  [25632/42552]\n",
      "loss: 0.293443  [28832/42552]\n",
      "loss: 0.276187  [32032/42552]\n",
      "loss: 0.320630  [35232/42552]\n",
      "loss: 0.262893  [38432/42552]\n",
      "loss: 0.195968  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.234543 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.188503  [   32/42552]\n",
      "loss: 0.284356  [ 3232/42552]\n",
      "loss: 0.301172  [ 6432/42552]\n",
      "loss: 0.357342  [ 9632/42552]\n",
      "loss: 0.279567  [12832/42552]\n",
      "loss: 0.235403  [16032/42552]\n",
      "loss: 0.165746  [19232/42552]\n",
      "loss: 0.336479  [22432/42552]\n",
      "loss: 0.253087  [25632/42552]\n",
      "loss: 0.219417  [28832/42552]\n",
      "loss: 0.142025  [32032/42552]\n",
      "loss: 0.332791  [35232/42552]\n",
      "loss: 0.224003  [38432/42552]\n",
      "loss: 0.274350  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.237384 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.391529  [   32/42552]\n",
      "loss: 0.357591  [ 3232/42552]\n",
      "loss: 0.250196  [ 6432/42552]\n",
      "loss: 0.401117  [ 9632/42552]\n",
      "loss: 0.402901  [12832/42552]\n",
      "loss: 0.360203  [16032/42552]\n",
      "loss: 0.284053  [19232/42552]\n",
      "loss: 0.431187  [22432/42552]\n",
      "loss: 0.226717  [25632/42552]\n",
      "loss: 0.286303  [28832/42552]\n",
      "loss: 0.300035  [32032/42552]\n",
      "loss: 0.307307  [35232/42552]\n",
      "loss: 0.300047  [38432/42552]\n",
      "loss: 0.291125  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.233818 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.255563  [   32/42552]\n",
      "loss: 0.295814  [ 3232/42552]\n",
      "loss: 0.204476  [ 6432/42552]\n",
      "loss: 0.177672  [ 9632/42552]\n",
      "loss: 0.241289  [12832/42552]\n",
      "loss: 0.262869  [16032/42552]\n",
      "loss: 0.219635  [19232/42552]\n",
      "loss: 0.305420  [22432/42552]\n",
      "loss: 0.568265  [25632/42552]\n",
      "loss: 0.179739  [28832/42552]\n",
      "loss: 0.166055  [32032/42552]\n",
      "loss: 0.301241  [35232/42552]\n",
      "loss: 0.279995  [38432/42552]\n",
      "loss: 0.239202  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.232944 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.394936  [   32/42552]\n",
      "loss: 0.208755  [ 3232/42552]\n",
      "loss: 0.217043  [ 6432/42552]\n",
      "loss: 0.454891  [ 9632/42552]\n",
      "loss: 0.260015  [12832/42552]\n",
      "loss: 0.253637  [16032/42552]\n",
      "loss: 0.391992  [19232/42552]\n",
      "loss: 0.366963  [22432/42552]\n",
      "loss: 0.319648  [25632/42552]\n",
      "loss: 0.315648  [28832/42552]\n",
      "loss: 0.273090  [32032/42552]\n",
      "loss: 0.210991  [35232/42552]\n",
      "loss: 0.164335  [38432/42552]\n",
      "loss: 0.231761  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.255867 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.359045  [   32/42552]\n",
      "loss: 0.213583  [ 3232/42552]\n",
      "loss: 0.234777  [ 6432/42552]\n",
      "loss: 0.288950  [ 9632/42552]\n",
      "loss: 0.285816  [12832/42552]\n",
      "loss: 0.233708  [16032/42552]\n",
      "loss: 0.153754  [19232/42552]\n",
      "loss: 0.148930  [22432/42552]\n",
      "loss: 0.232996  [25632/42552]\n",
      "loss: 0.494445  [28832/42552]\n",
      "loss: 0.210243  [32032/42552]\n",
      "loss: 0.375809  [35232/42552]\n",
      "loss: 0.259092  [38432/42552]\n",
      "loss: 0.244433  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.238172 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.385045  [   32/42552]\n",
      "loss: 0.278057  [ 3232/42552]\n",
      "loss: 0.214550  [ 6432/42552]\n",
      "loss: 0.273484  [ 9632/42552]\n",
      "loss: 0.215612  [12832/42552]\n",
      "loss: 0.405885  [16032/42552]\n",
      "loss: 0.254135  [19232/42552]\n",
      "loss: 0.238458  [22432/42552]\n",
      "loss: 0.285667  [25632/42552]\n",
      "loss: 0.285707  [28832/42552]\n",
      "loss: 0.408127  [32032/42552]\n",
      "loss: 0.276740  [35232/42552]\n",
      "loss: 0.201145  [38432/42552]\n",
      "loss: 0.202815  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.248644 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.154713  [   32/42552]\n",
      "loss: 0.255815  [ 3232/42552]\n",
      "loss: 0.173763  [ 6432/42552]\n",
      "loss: 0.259513  [ 9632/42552]\n",
      "loss: 0.260217  [12832/42552]\n",
      "loss: 0.168469  [16032/42552]\n",
      "loss: 0.197019  [19232/42552]\n",
      "loss: 0.245458  [22432/42552]\n",
      "loss: 0.175005  [25632/42552]\n",
      "loss: 0.346181  [28832/42552]\n",
      "loss: 0.205291  [32032/42552]\n",
      "loss: 0.270586  [35232/42552]\n",
      "loss: 0.200463  [38432/42552]\n",
      "loss: 0.215527  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.233535 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.298905  [   32/42552]\n",
      "loss: 0.282305  [ 3232/42552]\n",
      "loss: 0.247218  [ 6432/42552]\n",
      "loss: 0.256993  [ 9632/42552]\n",
      "loss: 0.158973  [12832/42552]\n",
      "loss: 0.329379  [16032/42552]\n",
      "loss: 0.200755  [19232/42552]\n",
      "loss: 0.256848  [22432/42552]\n",
      "loss: 0.271947  [25632/42552]\n",
      "loss: 0.182487  [28832/42552]\n",
      "loss: 0.237654  [32032/42552]\n",
      "loss: 0.176429  [35232/42552]\n",
      "loss: 0.290839  [38432/42552]\n",
      "loss: 0.292666  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.232970 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.280522  [   32/42552]\n",
      "loss: 0.268718  [ 3232/42552]\n",
      "loss: 0.334850  [ 6432/42552]\n",
      "loss: 0.186133  [ 9632/42552]\n",
      "loss: 0.194000  [12832/42552]\n",
      "loss: 0.178851  [16032/42552]\n",
      "loss: 0.263216  [19232/42552]\n",
      "loss: 0.486066  [22432/42552]\n",
      "loss: 0.460051  [25632/42552]\n",
      "loss: 0.399218  [28832/42552]\n",
      "loss: 0.349629  [32032/42552]\n",
      "loss: 0.269003  [35232/42552]\n",
      "loss: 0.397971  [38432/42552]\n",
      "loss: 0.319504  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.239951 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.227718  [   32/42552]\n",
      "loss: 0.355533  [ 3232/42552]\n",
      "loss: 0.193708  [ 6432/42552]\n",
      "loss: 0.231084  [ 9632/42552]\n",
      "loss: 0.163418  [12832/42552]\n",
      "loss: 0.275361  [16032/42552]\n",
      "loss: 0.276238  [19232/42552]\n",
      "loss: 0.251081  [22432/42552]\n",
      "loss: 0.195560  [25632/42552]\n",
      "loss: 0.306625  [28832/42552]\n",
      "loss: 0.187704  [32032/42552]\n",
      "loss: 0.334984  [35232/42552]\n",
      "loss: 0.326096  [38432/42552]\n",
      "loss: 0.162275  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.242708 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.202392  [   32/42552]\n",
      "loss: 0.301092  [ 3232/42552]\n",
      "loss: 0.197767  [ 6432/42552]\n",
      "loss: 0.229898  [ 9632/42552]\n",
      "loss: 0.284618  [12832/42552]\n",
      "loss: 0.345243  [16032/42552]\n",
      "loss: 0.214991  [19232/42552]\n",
      "loss: 0.236386  [22432/42552]\n",
      "loss: 0.213609  [25632/42552]\n",
      "loss: 0.188287  [28832/42552]\n",
      "loss: 0.253402  [32032/42552]\n",
      "loss: 0.347469  [35232/42552]\n",
      "loss: 0.300575  [38432/42552]\n",
      "loss: 0.238086  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.233743 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.166399  [   32/42552]\n",
      "loss: 0.213495  [ 3232/42552]\n",
      "loss: 0.243286  [ 6432/42552]\n",
      "loss: 0.257512  [ 9632/42552]\n",
      "loss: 0.530152  [12832/42552]\n",
      "loss: 0.353663  [16032/42552]\n",
      "loss: 0.425207  [19232/42552]\n",
      "loss: 0.240891  [22432/42552]\n",
      "loss: 0.304594  [25632/42552]\n",
      "loss: 0.466151  [28832/42552]\n",
      "loss: 0.248176  [32032/42552]\n",
      "loss: 0.242416  [35232/42552]\n",
      "loss: 0.352657  [38432/42552]\n",
      "loss: 0.213607  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.235317 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.227401  [   32/42552]\n",
      "loss: 0.248331  [ 3232/42552]\n",
      "loss: 0.192010  [ 6432/42552]\n",
      "loss: 0.251407  [ 9632/42552]\n",
      "loss: 0.419623  [12832/42552]\n",
      "loss: 0.230946  [16032/42552]\n",
      "loss: 0.199283  [19232/42552]\n",
      "loss: 0.234266  [22432/42552]\n",
      "loss: 0.381628  [25632/42552]\n",
      "loss: 0.305200  [28832/42552]\n",
      "loss: 0.375595  [32032/42552]\n",
      "loss: 0.318588  [35232/42552]\n",
      "loss: 0.160011  [38432/42552]\n",
      "loss: 0.308737  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.236755 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.287384  [   32/42552]\n",
      "loss: 0.404434  [ 3232/42552]\n",
      "loss: 0.222197  [ 6432/42552]\n",
      "loss: 0.109330  [ 9632/42552]\n",
      "loss: 0.341465  [12832/42552]\n",
      "loss: 0.216883  [16032/42552]\n",
      "loss: 0.246182  [19232/42552]\n",
      "loss: 0.255410  [22432/42552]\n",
      "loss: 0.442239  [25632/42552]\n",
      "loss: 0.413979  [28832/42552]\n",
      "loss: 0.197896  [32032/42552]\n",
      "loss: 0.252272  [35232/42552]\n",
      "loss: 0.207370  [38432/42552]\n",
      "loss: 0.403911  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.237098 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.203578  [   32/42552]\n",
      "loss: 0.294413  [ 3232/42552]\n",
      "loss: 0.504549  [ 6432/42552]\n",
      "loss: 0.437490  [ 9632/42552]\n",
      "loss: 0.224098  [12832/42552]\n",
      "loss: 0.353851  [16032/42552]\n",
      "loss: 0.224760  [19232/42552]\n",
      "loss: 0.365054  [22432/42552]\n",
      "loss: 0.239652  [25632/42552]\n",
      "loss: 0.204091  [28832/42552]\n",
      "loss: 0.472487  [32032/42552]\n",
      "loss: 0.268218  [35232/42552]\n",
      "loss: 0.233629  [38432/42552]\n",
      "loss: 0.189745  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.236211 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.329112  [   32/42552]\n",
      "loss: 0.384314  [ 3232/42552]\n",
      "loss: 0.261321  [ 6432/42552]\n",
      "loss: 0.361436  [ 9632/42552]\n",
      "loss: 0.315434  [12832/42552]\n",
      "loss: 0.286436  [16032/42552]\n",
      "loss: 0.456700  [19232/42552]\n",
      "loss: 0.279244  [22432/42552]\n",
      "loss: 0.303777  [25632/42552]\n",
      "loss: 0.222153  [28832/42552]\n",
      "loss: 0.155256  [32032/42552]\n",
      "loss: 0.230338  [35232/42552]\n",
      "loss: 0.136225  [38432/42552]\n",
      "loss: 0.243808  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.235669 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.200621  [   32/42552]\n",
      "loss: 0.399713  [ 3232/42552]\n",
      "loss: 0.344056  [ 6432/42552]\n",
      "loss: 0.337668  [ 9632/42552]\n",
      "loss: 0.241758  [12832/42552]\n",
      "loss: 0.319073  [16032/42552]\n",
      "loss: 0.244160  [19232/42552]\n",
      "loss: 0.348419  [22432/42552]\n",
      "loss: 0.335542  [25632/42552]\n",
      "loss: 0.233814  [28832/42552]\n",
      "loss: 0.175166  [32032/42552]\n",
      "loss: 0.181377  [35232/42552]\n",
      "loss: 0.286514  [38432/42552]\n",
      "loss: 0.185930  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.238003 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.458568  [   32/42552]\n",
      "loss: 0.220341  [ 3232/42552]\n",
      "loss: 0.291865  [ 6432/42552]\n",
      "loss: 0.239382  [ 9632/42552]\n",
      "loss: 0.248838  [12832/42552]\n",
      "loss: 0.232201  [16032/42552]\n",
      "loss: 0.136966  [19232/42552]\n",
      "loss: 0.276156  [22432/42552]\n",
      "loss: 0.407516  [25632/42552]\n",
      "loss: 0.271468  [28832/42552]\n",
      "loss: 0.262162  [32032/42552]\n",
      "loss: 0.351428  [35232/42552]\n",
      "loss: 0.194913  [38432/42552]\n",
      "loss: 0.182657  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.231421 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.397914  [   32/42552]\n",
      "loss: 0.204676  [ 3232/42552]\n",
      "loss: 0.247692  [ 6432/42552]\n",
      "loss: 0.418301  [ 9632/42552]\n",
      "loss: 0.222014  [12832/42552]\n",
      "loss: 0.283657  [16032/42552]\n",
      "loss: 0.325486  [19232/42552]\n",
      "loss: 0.308038  [22432/42552]\n",
      "loss: 0.371166  [25632/42552]\n",
      "loss: 0.287516  [28832/42552]\n",
      "loss: 0.234554  [32032/42552]\n",
      "loss: 0.149705  [35232/42552]\n",
      "loss: 0.327898  [38432/42552]\n",
      "loss: 0.290330  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.232303 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.259000  [   32/42552]\n",
      "loss: 0.291571  [ 3232/42552]\n",
      "loss: 0.309113  [ 6432/42552]\n",
      "loss: 0.204325  [ 9632/42552]\n",
      "loss: 0.268162  [12832/42552]\n",
      "loss: 0.207208  [16032/42552]\n",
      "loss: 0.344708  [19232/42552]\n",
      "loss: 0.294068  [22432/42552]\n",
      "loss: 0.278009  [25632/42552]\n",
      "loss: 0.262401  [28832/42552]\n",
      "loss: 0.245009  [32032/42552]\n",
      "loss: 0.253510  [35232/42552]\n",
      "loss: 0.162394  [38432/42552]\n",
      "loss: 0.239754  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.237298 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.291381  [   32/42552]\n",
      "loss: 0.284198  [ 3232/42552]\n",
      "loss: 0.312648  [ 6432/42552]\n",
      "loss: 0.357771  [ 9632/42552]\n",
      "loss: 0.337957  [12832/42552]\n",
      "loss: 0.353351  [16032/42552]\n",
      "loss: 0.276738  [19232/42552]\n",
      "loss: 0.197429  [22432/42552]\n",
      "loss: 0.158066  [25632/42552]\n",
      "loss: 0.295246  [28832/42552]\n",
      "loss: 0.251546  [32032/42552]\n",
      "loss: 0.206048  [35232/42552]\n",
      "loss: 0.428442  [38432/42552]\n",
      "loss: 0.195333  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.236979 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.184198  [   32/42552]\n",
      "loss: 0.208597  [ 3232/42552]\n",
      "loss: 0.329346  [ 6432/42552]\n",
      "loss: 0.178333  [ 9632/42552]\n",
      "loss: 0.231149  [12832/42552]\n",
      "loss: 0.209132  [16032/42552]\n",
      "loss: 0.352828  [19232/42552]\n",
      "loss: 0.216765  [22432/42552]\n",
      "loss: 0.284262  [25632/42552]\n",
      "loss: 0.229494  [28832/42552]\n",
      "loss: 0.164449  [32032/42552]\n",
      "loss: 0.293195  [35232/42552]\n",
      "loss: 0.209147  [38432/42552]\n",
      "loss: 0.228855  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.235565 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.283521  [   32/42552]\n",
      "loss: 0.334282  [ 3232/42552]\n",
      "loss: 0.194866  [ 6432/42552]\n",
      "loss: 0.453583  [ 9632/42552]\n",
      "loss: 0.136815  [12832/42552]\n",
      "loss: 0.272217  [16032/42552]\n",
      "loss: 0.203469  [19232/42552]\n",
      "loss: 0.271174  [22432/42552]\n",
      "loss: 0.284210  [25632/42552]\n",
      "loss: 0.296297  [28832/42552]\n",
      "loss: 0.206191  [32032/42552]\n",
      "loss: 0.285395  [35232/42552]\n",
      "loss: 0.297290  [38432/42552]\n",
      "loss: 0.279379  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.244754 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.154383  [   32/42552]\n",
      "loss: 0.301686  [ 3232/42552]\n",
      "loss: 0.379078  [ 6432/42552]\n",
      "loss: 0.310150  [ 9632/42552]\n",
      "loss: 0.192833  [12832/42552]\n",
      "loss: 0.217663  [16032/42552]\n",
      "loss: 0.234883  [19232/42552]\n",
      "loss: 0.212392  [22432/42552]\n",
      "loss: 0.270320  [25632/42552]\n",
      "loss: 0.162258  [28832/42552]\n",
      "loss: 0.231857  [32032/42552]\n",
      "loss: 0.222722  [35232/42552]\n",
      "loss: 0.187889  [38432/42552]\n",
      "loss: 0.250225  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.233570 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.230753  [   32/42552]\n",
      "loss: 0.272537  [ 3232/42552]\n",
      "loss: 0.122161  [ 6432/42552]\n",
      "loss: 0.308169  [ 9632/42552]\n",
      "loss: 0.311207  [12832/42552]\n",
      "loss: 0.229890  [16032/42552]\n",
      "loss: 0.317384  [19232/42552]\n",
      "loss: 0.172753  [22432/42552]\n",
      "loss: 0.220331  [25632/42552]\n",
      "loss: 0.268443  [28832/42552]\n",
      "loss: 0.248480  [32032/42552]\n",
      "loss: 0.233091  [35232/42552]\n",
      "loss: 0.251117  [38432/42552]\n",
      "loss: 0.383920  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.231588 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.248937  [   32/42552]\n",
      "loss: 0.234097  [ 3232/42552]\n",
      "loss: 0.121022  [ 6432/42552]\n",
      "loss: 0.291594  [ 9632/42552]\n",
      "loss: 0.497292  [12832/42552]\n",
      "loss: 0.183594  [16032/42552]\n",
      "loss: 0.371541  [19232/42552]\n",
      "loss: 0.179550  [22432/42552]\n",
      "loss: 0.210811  [25632/42552]\n",
      "loss: 0.222514  [28832/42552]\n",
      "loss: 0.268260  [32032/42552]\n",
      "loss: 0.346946  [35232/42552]\n",
      "loss: 0.314656  [38432/42552]\n",
      "loss: 0.245443  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.235650 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.245113  [   32/42552]\n",
      "loss: 0.257736  [ 3232/42552]\n",
      "loss: 0.236288  [ 6432/42552]\n",
      "loss: 0.209047  [ 9632/42552]\n",
      "loss: 0.301251  [12832/42552]\n",
      "loss: 0.232090  [16032/42552]\n",
      "loss: 0.320201  [19232/42552]\n",
      "loss: 0.170844  [22432/42552]\n",
      "loss: 0.212286  [25632/42552]\n",
      "loss: 0.250752  [28832/42552]\n",
      "loss: 0.545923  [32032/42552]\n",
      "loss: 0.230801  [35232/42552]\n",
      "loss: 0.173204  [38432/42552]\n",
      "loss: 0.348844  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.233298 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.330820  [   32/42552]\n",
      "loss: 0.177104  [ 3232/42552]\n",
      "loss: 0.302203  [ 6432/42552]\n",
      "loss: 0.206244  [ 9632/42552]\n",
      "loss: 0.197064  [12832/42552]\n",
      "loss: 0.221610  [16032/42552]\n",
      "loss: 0.265159  [19232/42552]\n",
      "loss: 0.301379  [22432/42552]\n",
      "loss: 0.264113  [25632/42552]\n",
      "loss: 0.231253  [28832/42552]\n",
      "loss: 0.266234  [32032/42552]\n",
      "loss: 0.269485  [35232/42552]\n",
      "loss: 0.220654  [38432/42552]\n",
      "loss: 0.183072  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.236246 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.169032  [   32/42552]\n",
      "loss: 0.182456  [ 3232/42552]\n",
      "loss: 0.261425  [ 6432/42552]\n",
      "loss: 0.250993  [ 9632/42552]\n",
      "loss: 0.246527  [12832/42552]\n",
      "loss: 0.313763  [16032/42552]\n",
      "loss: 0.174670  [19232/42552]\n",
      "loss: 0.322663  [22432/42552]\n",
      "loss: 0.240440  [25632/42552]\n",
      "loss: 0.250787  [28832/42552]\n",
      "loss: 0.286101  [32032/42552]\n",
      "loss: 0.302756  [35232/42552]\n",
      "loss: 0.263416  [38432/42552]\n",
      "loss: 0.358902  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.228369 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.238842  [   32/42552]\n",
      "loss: 0.232432  [ 3232/42552]\n",
      "loss: 0.341059  [ 6432/42552]\n",
      "loss: 0.175113  [ 9632/42552]\n",
      "loss: 0.157944  [12832/42552]\n",
      "loss: 0.166025  [16032/42552]\n",
      "loss: 0.347274  [19232/42552]\n",
      "loss: 0.297217  [22432/42552]\n",
      "loss: 0.263929  [25632/42552]\n",
      "loss: 0.223659  [28832/42552]\n",
      "loss: 0.176364  [32032/42552]\n",
      "loss: 0.230312  [35232/42552]\n",
      "loss: 0.220364  [38432/42552]\n",
      "loss: 0.249194  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.235711 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.225134  [   32/42552]\n",
      "loss: 0.148352  [ 3232/42552]\n",
      "loss: 0.262196  [ 6432/42552]\n",
      "loss: 0.233285  [ 9632/42552]\n",
      "loss: 0.297400  [12832/42552]\n",
      "loss: 0.276108  [16032/42552]\n",
      "loss: 0.286150  [19232/42552]\n",
      "loss: 0.176310  [22432/42552]\n",
      "loss: 0.289755  [25632/42552]\n",
      "loss: 0.257293  [28832/42552]\n",
      "loss: 0.247169  [32032/42552]\n",
      "loss: 0.248601  [35232/42552]\n",
      "loss: 0.250975  [38432/42552]\n",
      "loss: 0.267524  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.239700 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.370581  [   32/42552]\n",
      "loss: 0.333725  [ 3232/42552]\n",
      "loss: 0.151964  [ 6432/42552]\n",
      "loss: 0.241064  [ 9632/42552]\n",
      "loss: 0.334442  [12832/42552]\n",
      "loss: 0.266602  [16032/42552]\n",
      "loss: 0.325741  [19232/42552]\n",
      "loss: 0.298722  [22432/42552]\n",
      "loss: 0.383248  [25632/42552]\n",
      "loss: 0.141742  [28832/42552]\n",
      "loss: 0.202021  [32032/42552]\n",
      "loss: 0.244593  [35232/42552]\n",
      "loss: 0.299633  [38432/42552]\n",
      "loss: 0.239765  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.232369 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.256309  [   32/42552]\n",
      "loss: 0.392051  [ 3232/42552]\n",
      "loss: 0.390684  [ 6432/42552]\n",
      "loss: 0.265804  [ 9632/42552]\n",
      "loss: 0.328902  [12832/42552]\n",
      "loss: 0.277385  [16032/42552]\n",
      "loss: 0.344707  [19232/42552]\n",
      "loss: 0.259083  [22432/42552]\n",
      "loss: 0.340638  [25632/42552]\n",
      "loss: 0.385056  [28832/42552]\n",
      "loss: 0.275284  [32032/42552]\n",
      "loss: 0.330867  [35232/42552]\n",
      "loss: 0.266714  [38432/42552]\n",
      "loss: 0.302260  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.237272 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.250406  [   32/42552]\n",
      "loss: 0.189605  [ 3232/42552]\n",
      "loss: 0.294484  [ 6432/42552]\n",
      "loss: 0.354755  [ 9632/42552]\n",
      "loss: 0.301929  [12832/42552]\n",
      "loss: 0.470625  [16032/42552]\n",
      "loss: 0.252693  [19232/42552]\n",
      "loss: 0.401624  [22432/42552]\n",
      "loss: 0.266648  [25632/42552]\n",
      "loss: 0.462653  [28832/42552]\n",
      "loss: 0.176067  [32032/42552]\n",
      "loss: 0.134400  [35232/42552]\n",
      "loss: 0.159784  [38432/42552]\n",
      "loss: 0.229634  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.227356 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.224266  [   32/42552]\n",
      "loss: 0.241417  [ 3232/42552]\n",
      "loss: 0.158042  [ 6432/42552]\n",
      "loss: 0.249699  [ 9632/42552]\n",
      "loss: 0.214285  [12832/42552]\n",
      "loss: 0.234064  [16032/42552]\n",
      "loss: 0.201412  [19232/42552]\n",
      "loss: 0.176033  [22432/42552]\n",
      "loss: 0.449439  [25632/42552]\n",
      "loss: 0.183213  [28832/42552]\n",
      "loss: 0.332149  [32032/42552]\n",
      "loss: 0.151451  [35232/42552]\n",
      "loss: 0.215176  [38432/42552]\n",
      "loss: 0.201118  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.229878 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.317450  [   32/42552]\n",
      "loss: 0.261161  [ 3232/42552]\n",
      "loss: 0.307434  [ 6432/42552]\n",
      "loss: 0.201443  [ 9632/42552]\n",
      "loss: 0.210218  [12832/42552]\n",
      "loss: 0.294308  [16032/42552]\n",
      "loss: 0.485308  [19232/42552]\n",
      "loss: 0.356243  [22432/42552]\n",
      "loss: 0.165786  [25632/42552]\n",
      "loss: 0.229571  [28832/42552]\n",
      "loss: 0.206158  [32032/42552]\n",
      "loss: 0.212607  [35232/42552]\n",
      "loss: 0.287981  [38432/42552]\n",
      "loss: 0.339461  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.235784 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.309511  [   32/42552]\n",
      "loss: 0.176671  [ 3232/42552]\n",
      "loss: 0.171399  [ 6432/42552]\n",
      "loss: 0.323398  [ 9632/42552]\n",
      "loss: 0.253581  [12832/42552]\n",
      "loss: 0.167104  [16032/42552]\n",
      "loss: 0.331454  [19232/42552]\n",
      "loss: 0.300200  [22432/42552]\n",
      "loss: 0.291103  [25632/42552]\n",
      "loss: 0.266023  [28832/42552]\n",
      "loss: 0.226811  [32032/42552]\n",
      "loss: 0.237684  [35232/42552]\n",
      "loss: 0.293846  [38432/42552]\n",
      "loss: 0.298933  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.241076 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.445878  [   32/42552]\n",
      "loss: 0.370467  [ 3232/42552]\n",
      "loss: 0.233226  [ 6432/42552]\n",
      "loss: 0.311932  [ 9632/42552]\n",
      "loss: 0.316878  [12832/42552]\n",
      "loss: 0.300472  [16032/42552]\n",
      "loss: 0.235321  [19232/42552]\n",
      "loss: 0.252659  [22432/42552]\n",
      "loss: 0.232622  [25632/42552]\n",
      "loss: 0.272514  [28832/42552]\n",
      "loss: 0.288659  [32032/42552]\n",
      "loss: 0.252731  [35232/42552]\n",
      "loss: 0.428413  [38432/42552]\n",
      "loss: 0.213033  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.230913 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.173375  [   32/42552]\n",
      "loss: 0.185284  [ 3232/42552]\n",
      "loss: 0.215438  [ 6432/42552]\n",
      "loss: 0.491646  [ 9632/42552]\n",
      "loss: 0.112920  [12832/42552]\n",
      "loss: 0.213756  [16032/42552]\n",
      "loss: 0.158920  [19232/42552]\n",
      "loss: 0.214016  [22432/42552]\n",
      "loss: 0.263980  [25632/42552]\n",
      "loss: 0.279420  [28832/42552]\n",
      "loss: 0.531439  [32032/42552]\n",
      "loss: 0.411288  [35232/42552]\n",
      "loss: 0.191210  [38432/42552]\n",
      "loss: 0.248905  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.241874 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.198923  [   32/42552]\n",
      "loss: 0.368188  [ 3232/42552]\n",
      "loss: 0.279342  [ 6432/42552]\n",
      "loss: 0.307744  [ 9632/42552]\n",
      "loss: 0.243735  [12832/42552]\n",
      "loss: 0.379598  [16032/42552]\n",
      "loss: 0.262808  [19232/42552]\n",
      "loss: 0.145642  [22432/42552]\n",
      "loss: 0.455448  [25632/42552]\n",
      "loss: 0.353473  [28832/42552]\n",
      "loss: 0.395100  [32032/42552]\n",
      "loss: 0.134946  [35232/42552]\n",
      "loss: 0.231936  [38432/42552]\n",
      "loss: 0.306009  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.238425 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.274131  [   32/42552]\n",
      "loss: 0.227422  [ 3232/42552]\n",
      "loss: 0.252288  [ 6432/42552]\n",
      "loss: 0.362455  [ 9632/42552]\n",
      "loss: 0.382834  [12832/42552]\n",
      "loss: 0.213925  [16032/42552]\n",
      "loss: 0.491410  [19232/42552]\n",
      "loss: 0.500791  [22432/42552]\n",
      "loss: 0.361080  [25632/42552]\n",
      "loss: 0.294581  [28832/42552]\n",
      "loss: 0.234865  [32032/42552]\n",
      "loss: 0.443895  [35232/42552]\n",
      "loss: 0.149845  [38432/42552]\n",
      "loss: 0.323424  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.236749 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.275922  [   32/42552]\n",
      "loss: 0.246879  [ 3232/42552]\n",
      "loss: 0.180320  [ 6432/42552]\n",
      "loss: 0.333713  [ 9632/42552]\n",
      "loss: 0.212176  [12832/42552]\n",
      "loss: 0.367171  [16032/42552]\n",
      "loss: 0.364566  [19232/42552]\n",
      "loss: 0.253036  [22432/42552]\n",
      "loss: 0.313996  [25632/42552]\n",
      "loss: 0.355861  [28832/42552]\n",
      "loss: 0.250195  [32032/42552]\n",
      "loss: 0.324487  [35232/42552]\n",
      "loss: 0.238032  [38432/42552]\n",
      "loss: 0.279246  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.227373 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.365285  [   32/42552]\n",
      "loss: 0.222902  [ 3232/42552]\n",
      "loss: 0.428987  [ 6432/42552]\n",
      "loss: 0.329909  [ 9632/42552]\n",
      "loss: 0.181891  [12832/42552]\n",
      "loss: 0.157260  [16032/42552]\n",
      "loss: 0.153591  [19232/42552]\n",
      "loss: 0.219156  [22432/42552]\n",
      "loss: 0.393196  [25632/42552]\n",
      "loss: 0.272133  [28832/42552]\n",
      "loss: 0.142722  [32032/42552]\n",
      "loss: 0.229637  [35232/42552]\n",
      "loss: 0.237752  [38432/42552]\n",
      "loss: 0.157231  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.238696 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.243693  [   32/42552]\n",
      "loss: 0.198277  [ 3232/42552]\n",
      "loss: 0.092431  [ 6432/42552]\n",
      "loss: 0.275135  [ 9632/42552]\n",
      "loss: 0.199709  [12832/42552]\n",
      "loss: 0.212636  [16032/42552]\n",
      "loss: 0.317629  [19232/42552]\n",
      "loss: 0.187702  [22432/42552]\n",
      "loss: 0.395037  [25632/42552]\n",
      "loss: 0.356475  [28832/42552]\n",
      "loss: 0.225939  [32032/42552]\n",
      "loss: 0.232231  [35232/42552]\n",
      "loss: 0.262400  [38432/42552]\n",
      "loss: 0.205050  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.229255 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.357245  [   32/42552]\n",
      "loss: 0.295876  [ 3232/42552]\n",
      "loss: 0.325244  [ 6432/42552]\n",
      "loss: 0.219359  [ 9632/42552]\n",
      "loss: 0.215446  [12832/42552]\n",
      "loss: 0.315205  [16032/42552]\n",
      "loss: 0.194867  [19232/42552]\n",
      "loss: 0.262227  [22432/42552]\n",
      "loss: 0.297162  [25632/42552]\n",
      "loss: 0.258661  [28832/42552]\n",
      "loss: 0.229455  [32032/42552]\n",
      "loss: 0.258254  [35232/42552]\n",
      "loss: 0.436480  [38432/42552]\n",
      "loss: 0.248532  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.231616 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.409687  [   32/42552]\n",
      "loss: 0.375537  [ 3232/42552]\n",
      "loss: 0.213275  [ 6432/42552]\n",
      "loss: 0.227812  [ 9632/42552]\n",
      "loss: 0.190153  [12832/42552]\n",
      "loss: 0.247715  [16032/42552]\n",
      "loss: 0.213781  [19232/42552]\n",
      "loss: 0.330344  [22432/42552]\n",
      "loss: 0.301316  [25632/42552]\n",
      "loss: 0.301929  [28832/42552]\n",
      "loss: 0.268610  [32032/42552]\n",
      "loss: 0.158305  [35232/42552]\n",
      "loss: 0.249960  [38432/42552]\n",
      "loss: 0.379554  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.233317 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.233043  [   32/42552]\n",
      "loss: 0.179970  [ 3232/42552]\n",
      "loss: 0.307530  [ 6432/42552]\n",
      "loss: 0.254906  [ 9632/42552]\n",
      "loss: 0.278178  [12832/42552]\n",
      "loss: 0.251667  [16032/42552]\n",
      "loss: 0.276866  [19232/42552]\n",
      "loss: 0.383259  [22432/42552]\n",
      "loss: 0.197008  [25632/42552]\n",
      "loss: 0.231755  [28832/42552]\n",
      "loss: 0.268442  [32032/42552]\n",
      "loss: 0.329521  [35232/42552]\n",
      "loss: 0.337834  [38432/42552]\n",
      "loss: 0.236222  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.238328 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.324239  [   32/42552]\n",
      "loss: 0.347075  [ 3232/42552]\n",
      "loss: 0.176338  [ 6432/42552]\n",
      "loss: 0.280781  [ 9632/42552]\n",
      "loss: 0.199562  [12832/42552]\n",
      "loss: 0.236165  [16032/42552]\n",
      "loss: 0.322613  [19232/42552]\n",
      "loss: 0.252899  [22432/42552]\n",
      "loss: 0.242797  [25632/42552]\n",
      "loss: 0.286546  [28832/42552]\n",
      "loss: 0.364751  [32032/42552]\n",
      "loss: 0.138390  [35232/42552]\n",
      "loss: 0.164390  [38432/42552]\n",
      "loss: 0.428621  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.234981 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.206329  [   32/42552]\n",
      "loss: 0.195642  [ 3232/42552]\n",
      "loss: 0.225962  [ 6432/42552]\n",
      "loss: 0.238987  [ 9632/42552]\n",
      "loss: 0.343343  [12832/42552]\n",
      "loss: 0.264312  [16032/42552]\n",
      "loss: 0.163570  [19232/42552]\n",
      "loss: 0.206533  [22432/42552]\n",
      "loss: 0.290176  [25632/42552]\n",
      "loss: 0.259839  [28832/42552]\n",
      "loss: 0.219745  [32032/42552]\n",
      "loss: 0.485692  [35232/42552]\n",
      "loss: 0.160351  [38432/42552]\n",
      "loss: 0.307617  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.230313 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.289382  [   32/42552]\n",
      "loss: 0.301274  [ 3232/42552]\n",
      "loss: 0.341394  [ 6432/42552]\n",
      "loss: 0.373307  [ 9632/42552]\n",
      "loss: 0.382678  [12832/42552]\n",
      "loss: 0.311055  [16032/42552]\n",
      "loss: 0.158092  [19232/42552]\n",
      "loss: 0.410079  [22432/42552]\n",
      "loss: 0.307713  [25632/42552]\n",
      "loss: 0.368972  [28832/42552]\n",
      "loss: 0.271238  [32032/42552]\n",
      "loss: 0.164253  [35232/42552]\n",
      "loss: 0.277444  [38432/42552]\n",
      "loss: 0.400105  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.229290 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.260886  [   32/42552]\n",
      "loss: 0.362668  [ 3232/42552]\n",
      "loss: 0.296119  [ 6432/42552]\n",
      "loss: 0.255599  [ 9632/42552]\n",
      "loss: 0.271722  [12832/42552]\n",
      "loss: 0.318814  [16032/42552]\n",
      "loss: 0.412796  [19232/42552]\n",
      "loss: 0.312174  [22432/42552]\n",
      "loss: 0.249032  [25632/42552]\n",
      "loss: 0.252710  [28832/42552]\n",
      "loss: 0.230003  [32032/42552]\n",
      "loss: 0.224028  [35232/42552]\n",
      "loss: 0.295356  [38432/42552]\n",
      "loss: 0.317418  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.250884 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.278904  [   32/42552]\n",
      "loss: 0.313792  [ 3232/42552]\n",
      "loss: 0.165234  [ 6432/42552]\n",
      "loss: 0.306048  [ 9632/42552]\n",
      "loss: 0.215515  [12832/42552]\n",
      "loss: 0.214857  [16032/42552]\n",
      "loss: 0.365461  [19232/42552]\n",
      "loss: 0.286530  [22432/42552]\n",
      "loss: 0.202350  [25632/42552]\n",
      "loss: 0.292300  [28832/42552]\n",
      "loss: 0.176936  [32032/42552]\n",
      "loss: 0.327517  [35232/42552]\n",
      "loss: 0.247469  [38432/42552]\n",
      "loss: 0.265421  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.231889 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.237975  [   32/42552]\n",
      "loss: 0.311669  [ 3232/42552]\n",
      "loss: 0.188650  [ 6432/42552]\n",
      "loss: 0.223875  [ 9632/42552]\n",
      "loss: 0.217497  [12832/42552]\n",
      "loss: 0.289470  [16032/42552]\n",
      "loss: 0.165022  [19232/42552]\n",
      "loss: 0.255016  [22432/42552]\n",
      "loss: 0.195018  [25632/42552]\n",
      "loss: 0.280160  [28832/42552]\n",
      "loss: 0.223424  [32032/42552]\n",
      "loss: 0.285583  [35232/42552]\n",
      "loss: 0.262031  [38432/42552]\n",
      "loss: 0.239015  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.229326 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.191210  [   32/42552]\n",
      "loss: 0.374811  [ 3232/42552]\n",
      "loss: 0.382787  [ 6432/42552]\n",
      "loss: 0.459148  [ 9632/42552]\n",
      "loss: 0.237054  [12832/42552]\n",
      "loss: 0.313314  [16032/42552]\n",
      "loss: 0.269287  [19232/42552]\n",
      "loss: 0.186710  [22432/42552]\n",
      "loss: 0.225766  [25632/42552]\n",
      "loss: 0.195704  [28832/42552]\n",
      "loss: 0.245684  [32032/42552]\n",
      "loss: 0.331213  [35232/42552]\n",
      "loss: 0.452593  [38432/42552]\n",
      "loss: 0.258998  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.232847 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.327743  [   32/42552]\n",
      "loss: 0.226475  [ 3232/42552]\n",
      "loss: 0.364267  [ 6432/42552]\n",
      "loss: 0.280001  [ 9632/42552]\n",
      "loss: 0.174854  [12832/42552]\n",
      "loss: 0.187409  [16032/42552]\n",
      "loss: 0.219271  [19232/42552]\n",
      "loss: 0.281720  [22432/42552]\n",
      "loss: 0.431315  [25632/42552]\n",
      "loss: 0.302872  [28832/42552]\n",
      "loss: 0.380379  [32032/42552]\n",
      "loss: 0.290770  [35232/42552]\n",
      "loss: 0.170893  [38432/42552]\n",
      "loss: 0.180718  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.236662 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.277934  [   32/42552]\n",
      "loss: 0.334821  [ 3232/42552]\n",
      "loss: 0.284239  [ 6432/42552]\n",
      "loss: 0.202188  [ 9632/42552]\n",
      "loss: 0.164697  [12832/42552]\n",
      "loss: 0.184294  [16032/42552]\n",
      "loss: 0.207324  [19232/42552]\n",
      "loss: 0.276301  [22432/42552]\n",
      "loss: 0.177491  [25632/42552]\n",
      "loss: 0.411583  [28832/42552]\n",
      "loss: 0.233597  [32032/42552]\n",
      "loss: 0.338348  [35232/42552]\n",
      "loss: 0.229370  [38432/42552]\n",
      "loss: 0.272936  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.232125 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.246980  [   32/42552]\n",
      "loss: 0.221499  [ 3232/42552]\n",
      "loss: 0.154286  [ 6432/42552]\n",
      "loss: 0.449947  [ 9632/42552]\n",
      "loss: 0.345202  [12832/42552]\n",
      "loss: 0.215311  [16032/42552]\n",
      "loss: 0.223705  [19232/42552]\n",
      "loss: 0.323390  [22432/42552]\n",
      "loss: 0.200273  [25632/42552]\n",
      "loss: 0.319347  [28832/42552]\n",
      "loss: 0.252643  [32032/42552]\n",
      "loss: 0.261265  [35232/42552]\n",
      "loss: 0.273302  [38432/42552]\n",
      "loss: 0.243668  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.234150 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.230380  [   32/42552]\n",
      "loss: 0.232984  [ 3232/42552]\n",
      "loss: 0.181200  [ 6432/42552]\n",
      "loss: 0.206404  [ 9632/42552]\n",
      "loss: 0.347906  [12832/42552]\n",
      "loss: 0.388826  [16032/42552]\n",
      "loss: 0.284709  [19232/42552]\n",
      "loss: 0.148958  [22432/42552]\n",
      "loss: 0.234409  [25632/42552]\n",
      "loss: 0.187807  [28832/42552]\n",
      "loss: 0.240531  [32032/42552]\n",
      "loss: 0.204148  [35232/42552]\n",
      "loss: 0.203880  [38432/42552]\n",
      "loss: 0.199961  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.235305 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.183792  [   32/42552]\n",
      "loss: 0.274557  [ 3232/42552]\n",
      "loss: 0.327830  [ 6432/42552]\n",
      "loss: 0.248129  [ 9632/42552]\n",
      "loss: 0.206658  [12832/42552]\n",
      "loss: 0.339622  [16032/42552]\n",
      "loss: 0.198276  [19232/42552]\n",
      "loss: 0.209960  [22432/42552]\n",
      "loss: 0.383240  [25632/42552]\n",
      "loss: 0.214568  [28832/42552]\n",
      "loss: 0.234638  [32032/42552]\n",
      "loss: 0.214060  [35232/42552]\n",
      "loss: 0.142381  [38432/42552]\n",
      "loss: 0.217642  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.239325 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.328191  [   32/42552]\n",
      "loss: 0.216670  [ 3232/42552]\n",
      "loss: 0.352530  [ 6432/42552]\n",
      "loss: 0.333515  [ 9632/42552]\n",
      "loss: 0.159408  [12832/42552]\n",
      "loss: 0.264191  [16032/42552]\n",
      "loss: 0.220735  [19232/42552]\n",
      "loss: 0.306038  [22432/42552]\n",
      "loss: 0.378781  [25632/42552]\n",
      "loss: 0.217484  [28832/42552]\n",
      "loss: 0.191092  [32032/42552]\n",
      "loss: 0.234714  [35232/42552]\n",
      "loss: 0.334319  [38432/42552]\n",
      "loss: 0.168410  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.238152 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.442510  [   32/42552]\n",
      "loss: 0.294266  [ 3232/42552]\n",
      "loss: 0.250983  [ 6432/42552]\n",
      "loss: 0.339770  [ 9632/42552]\n",
      "loss: 0.157148  [12832/42552]\n",
      "loss: 0.230936  [16032/42552]\n",
      "loss: 0.345726  [19232/42552]\n",
      "loss: 0.195408  [22432/42552]\n",
      "loss: 0.250003  [25632/42552]\n",
      "loss: 0.259820  [28832/42552]\n",
      "loss: 0.266970  [32032/42552]\n",
      "loss: 0.179584  [35232/42552]\n",
      "loss: 0.250032  [38432/42552]\n",
      "loss: 0.250391  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.229567 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.238170  [   32/42552]\n",
      "loss: 0.290268  [ 3232/42552]\n",
      "loss: 0.251391  [ 6432/42552]\n",
      "loss: 0.219789  [ 9632/42552]\n",
      "loss: 0.274424  [12832/42552]\n",
      "loss: 0.221399  [16032/42552]\n",
      "loss: 0.387316  [19232/42552]\n",
      "loss: 0.408601  [22432/42552]\n",
      "loss: 0.230932  [25632/42552]\n",
      "loss: 0.235714  [28832/42552]\n",
      "loss: 0.360154  [32032/42552]\n",
      "loss: 0.340321  [35232/42552]\n",
      "loss: 0.233817  [38432/42552]\n",
      "loss: 0.154571  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.236699 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.202829  [   32/42552]\n",
      "loss: 0.260841  [ 3232/42552]\n",
      "loss: 0.184580  [ 6432/42552]\n",
      "loss: 0.202309  [ 9632/42552]\n",
      "loss: 0.200045  [12832/42552]\n",
      "loss: 0.241727  [16032/42552]\n",
      "loss: 0.288777  [19232/42552]\n",
      "loss: 0.200522  [22432/42552]\n",
      "loss: 0.299483  [25632/42552]\n",
      "loss: 0.321532  [28832/42552]\n",
      "loss: 0.358159  [32032/42552]\n",
      "loss: 0.212684  [35232/42552]\n",
      "loss: 0.261392  [38432/42552]\n",
      "loss: 0.183603  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.231001 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.248530  [   32/42552]\n",
      "loss: 0.210165  [ 3232/42552]\n",
      "loss: 0.299607  [ 6432/42552]\n",
      "loss: 0.227456  [ 9632/42552]\n",
      "loss: 0.219798  [12832/42552]\n",
      "loss: 0.218888  [16032/42552]\n",
      "loss: 0.311923  [19232/42552]\n",
      "loss: 0.407938  [22432/42552]\n",
      "loss: 0.326624  [25632/42552]\n",
      "loss: 0.254002  [28832/42552]\n",
      "loss: 0.362184  [32032/42552]\n",
      "loss: 0.359239  [35232/42552]\n",
      "loss: 0.281669  [38432/42552]\n",
      "loss: 0.262330  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.228038 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.236428  [   32/42552]\n",
      "loss: 0.247393  [ 3232/42552]\n",
      "loss: 0.246090  [ 6432/42552]\n",
      "loss: 0.387866  [ 9632/42552]\n",
      "loss: 0.262892  [12832/42552]\n",
      "loss: 0.211251  [16032/42552]\n",
      "loss: 0.314673  [19232/42552]\n",
      "loss: 0.255476  [22432/42552]\n",
      "loss: 0.337543  [25632/42552]\n",
      "loss: 0.190514  [28832/42552]\n",
      "loss: 0.309003  [32032/42552]\n",
      "loss: 0.375467  [35232/42552]\n",
      "loss: 0.283244  [38432/42552]\n",
      "loss: 0.190285  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.228778 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.217467  [   32/42552]\n",
      "loss: 0.279554  [ 3232/42552]\n",
      "loss: 0.340349  [ 6432/42552]\n",
      "loss: 0.318270  [ 9632/42552]\n",
      "loss: 0.409934  [12832/42552]\n",
      "loss: 0.319463  [16032/42552]\n",
      "loss: 0.231161  [19232/42552]\n",
      "loss: 0.333436  [22432/42552]\n",
      "loss: 0.358591  [25632/42552]\n",
      "loss: 0.287427  [28832/42552]\n",
      "loss: 0.261656  [32032/42552]\n",
      "loss: 0.224705  [35232/42552]\n",
      "loss: 0.256947  [38432/42552]\n",
      "loss: 0.238610  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.239593 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.413831  [   32/42552]\n",
      "loss: 0.246770  [ 3232/42552]\n",
      "loss: 0.225596  [ 6432/42552]\n",
      "loss: 0.187773  [ 9632/42552]\n",
      "loss: 0.389975  [12832/42552]\n",
      "loss: 0.152424  [16032/42552]\n",
      "loss: 0.217248  [19232/42552]\n",
      "loss: 0.235702  [22432/42552]\n",
      "loss: 0.183445  [25632/42552]\n",
      "loss: 0.171760  [28832/42552]\n",
      "loss: 0.152841  [32032/42552]\n",
      "loss: 0.245157  [35232/42552]\n",
      "loss: 0.343013  [38432/42552]\n",
      "loss: 0.278533  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.228134 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.220148  [   32/42552]\n",
      "loss: 0.255242  [ 3232/42552]\n",
      "loss: 0.178815  [ 6432/42552]\n",
      "loss: 0.266733  [ 9632/42552]\n",
      "loss: 0.280032  [12832/42552]\n",
      "loss: 0.325193  [16032/42552]\n",
      "loss: 0.136215  [19232/42552]\n",
      "loss: 0.269773  [22432/42552]\n",
      "loss: 0.194385  [25632/42552]\n",
      "loss: 0.281901  [28832/42552]\n",
      "loss: 0.247520  [32032/42552]\n",
      "loss: 0.156862  [35232/42552]\n",
      "loss: 0.148288  [38432/42552]\n",
      "loss: 0.316201  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.238375 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.275646  [   32/42552]\n",
      "loss: 0.228084  [ 3232/42552]\n",
      "loss: 0.227747  [ 6432/42552]\n",
      "loss: 0.639896  [ 9632/42552]\n",
      "loss: 0.314190  [12832/42552]\n",
      "loss: 0.275664  [16032/42552]\n",
      "loss: 0.258553  [19232/42552]\n",
      "loss: 0.287146  [22432/42552]\n",
      "loss: 0.242826  [25632/42552]\n",
      "loss: 0.306085  [28832/42552]\n",
      "loss: 0.220573  [32032/42552]\n",
      "loss: 0.318230  [35232/42552]\n",
      "loss: 0.177974  [38432/42552]\n",
      "loss: 0.174159  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.229384 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.548806  [   32/42552]\n",
      "loss: 0.454369  [ 3232/42552]\n",
      "loss: 0.252517  [ 6432/42552]\n",
      "loss: 0.238246  [ 9632/42552]\n",
      "loss: 0.315611  [12832/42552]\n",
      "loss: 0.267898  [16032/42552]\n",
      "loss: 0.329142  [19232/42552]\n",
      "loss: 0.178646  [22432/42552]\n",
      "loss: 0.291650  [25632/42552]\n",
      "loss: 0.240879  [28832/42552]\n",
      "loss: 0.281735  [32032/42552]\n",
      "loss: 0.236114  [35232/42552]\n",
      "loss: 0.306968  [38432/42552]\n",
      "loss: 0.199561  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.248539 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.325076  [   32/42552]\n",
      "loss: 0.300845  [ 3232/42552]\n",
      "loss: 0.246812  [ 6432/42552]\n",
      "loss: 0.246425  [ 9632/42552]\n",
      "loss: 0.136468  [12832/42552]\n",
      "loss: 0.258005  [16032/42552]\n",
      "loss: 0.241134  [19232/42552]\n",
      "loss: 0.256031  [22432/42552]\n",
      "loss: 0.322590  [25632/42552]\n",
      "loss: 0.227809  [28832/42552]\n",
      "loss: 0.311595  [32032/42552]\n",
      "loss: 0.211428  [35232/42552]\n",
      "loss: 0.243408  [38432/42552]\n",
      "loss: 0.208642  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.238564 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.246503  [   32/42552]\n",
      "loss: 0.312835  [ 3232/42552]\n",
      "loss: 0.299211  [ 6432/42552]\n",
      "loss: 0.615138  [ 9632/42552]\n",
      "loss: 0.265708  [12832/42552]\n",
      "loss: 0.174921  [16032/42552]\n",
      "loss: 0.339070  [19232/42552]\n",
      "loss: 0.249436  [22432/42552]\n",
      "loss: 0.227035  [25632/42552]\n",
      "loss: 0.173025  [28832/42552]\n",
      "loss: 0.232527  [32032/42552]\n",
      "loss: 0.369090  [35232/42552]\n",
      "loss: 0.151641  [38432/42552]\n",
      "loss: 0.271105  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.236823 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.315450  [   32/42552]\n",
      "loss: 0.226677  [ 3232/42552]\n",
      "loss: 0.242472  [ 6432/42552]\n",
      "loss: 0.335867  [ 9632/42552]\n",
      "loss: 0.319279  [12832/42552]\n",
      "loss: 0.281061  [16032/42552]\n",
      "loss: 0.219638  [19232/42552]\n",
      "loss: 0.254714  [22432/42552]\n",
      "loss: 0.210423  [25632/42552]\n",
      "loss: 0.177838  [28832/42552]\n",
      "loss: 0.179295  [32032/42552]\n",
      "loss: 0.356177  [35232/42552]\n",
      "loss: 0.261039  [38432/42552]\n",
      "loss: 0.210627  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.237825 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.308414  [   32/42552]\n",
      "loss: 0.366646  [ 3232/42552]\n",
      "loss: 0.324240  [ 6432/42552]\n",
      "loss: 0.228739  [ 9632/42552]\n",
      "loss: 0.307416  [12832/42552]\n",
      "loss: 0.415391  [16032/42552]\n",
      "loss: 0.241065  [19232/42552]\n",
      "loss: 0.274494  [22432/42552]\n",
      "loss: 0.269099  [25632/42552]\n",
      "loss: 0.318443  [28832/42552]\n",
      "loss: 0.168501  [32032/42552]\n",
      "loss: 0.256763  [35232/42552]\n",
      "loss: 0.223201  [38432/42552]\n",
      "loss: 0.459740  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.236598 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.388275  [   32/42552]\n",
      "loss: 0.221882  [ 3232/42552]\n",
      "loss: 0.264693  [ 6432/42552]\n",
      "loss: 0.393876  [ 9632/42552]\n",
      "loss: 0.186062  [12832/42552]\n",
      "loss: 0.346523  [16032/42552]\n",
      "loss: 0.426732  [19232/42552]\n",
      "loss: 0.274315  [22432/42552]\n",
      "loss: 0.373154  [25632/42552]\n",
      "loss: 0.194845  [28832/42552]\n",
      "loss: 0.247854  [32032/42552]\n",
      "loss: 0.263428  [35232/42552]\n",
      "loss: 0.394276  [38432/42552]\n",
      "loss: 0.155358  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.225562 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.253532  [   32/42552]\n",
      "loss: 0.430502  [ 3232/42552]\n",
      "loss: 0.446272  [ 6432/42552]\n",
      "loss: 0.256489  [ 9632/42552]\n",
      "loss: 0.242884  [12832/42552]\n",
      "loss: 0.209423  [16032/42552]\n",
      "loss: 0.328790  [19232/42552]\n",
      "loss: 0.312588  [22432/42552]\n",
      "loss: 0.352733  [25632/42552]\n",
      "loss: 0.249271  [28832/42552]\n",
      "loss: 0.155844  [32032/42552]\n",
      "loss: 0.170678  [35232/42552]\n",
      "loss: 0.228311  [38432/42552]\n",
      "loss: 0.350831  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.229189 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.278578  [   32/42552]\n",
      "loss: 0.199676  [ 3232/42552]\n",
      "loss: 0.175788  [ 6432/42552]\n",
      "loss: 0.281432  [ 9632/42552]\n",
      "loss: 0.244778  [12832/42552]\n",
      "loss: 0.369423  [16032/42552]\n",
      "loss: 0.361539  [19232/42552]\n",
      "loss: 0.315626  [22432/42552]\n",
      "loss: 0.377161  [25632/42552]\n",
      "loss: 0.176363  [28832/42552]\n",
      "loss: 0.329998  [32032/42552]\n",
      "loss: 0.253654  [35232/42552]\n",
      "loss: 0.314298  [38432/42552]\n",
      "loss: 0.341750  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.229415 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.374189  [   32/42552]\n",
      "loss: 0.471566  [ 3232/42552]\n",
      "loss: 0.202405  [ 6432/42552]\n",
      "loss: 0.193469  [ 9632/42552]\n",
      "loss: 0.203623  [12832/42552]\n",
      "loss: 0.269103  [16032/42552]\n",
      "loss: 0.285209  [19232/42552]\n",
      "loss: 0.276539  [22432/42552]\n",
      "loss: 0.212218  [25632/42552]\n",
      "loss: 0.440056  [28832/42552]\n",
      "loss: 0.428706  [32032/42552]\n",
      "loss: 0.188480  [35232/42552]\n",
      "loss: 0.307697  [38432/42552]\n",
      "loss: 0.253894  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.231056 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.290871  [   32/42552]\n",
      "loss: 0.250225  [ 3232/42552]\n",
      "loss: 0.406660  [ 6432/42552]\n",
      "loss: 0.281763  [ 9632/42552]\n",
      "loss: 0.249407  [12832/42552]\n",
      "loss: 0.194218  [16032/42552]\n",
      "loss: 0.218367  [19232/42552]\n",
      "loss: 0.229060  [22432/42552]\n",
      "loss: 0.230546  [25632/42552]\n",
      "loss: 0.447887  [28832/42552]\n",
      "loss: 0.383621  [32032/42552]\n",
      "loss: 0.269934  [35232/42552]\n",
      "loss: 0.305834  [38432/42552]\n",
      "loss: 0.393790  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.225164 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.256885  [   32/42552]\n",
      "loss: 0.180821  [ 3232/42552]\n",
      "loss: 0.215625  [ 6432/42552]\n",
      "loss: 0.373111  [ 9632/42552]\n",
      "loss: 0.247713  [12832/42552]\n",
      "loss: 0.211132  [16032/42552]\n",
      "loss: 0.224404  [19232/42552]\n",
      "loss: 0.252390  [22432/42552]\n",
      "loss: 0.112551  [25632/42552]\n",
      "loss: 0.301925  [28832/42552]\n",
      "loss: 0.258755  [32032/42552]\n",
      "loss: 0.214248  [35232/42552]\n",
      "loss: 0.137695  [38432/42552]\n",
      "loss: 0.322124  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.228225 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.464636  [   32/42552]\n",
      "loss: 0.249513  [ 3232/42552]\n",
      "loss: 0.273023  [ 6432/42552]\n",
      "loss: 0.209988  [ 9632/42552]\n",
      "loss: 0.226594  [12832/42552]\n",
      "loss: 0.223137  [16032/42552]\n",
      "loss: 0.178192  [19232/42552]\n",
      "loss: 0.326519  [22432/42552]\n",
      "loss: 0.284652  [25632/42552]\n",
      "loss: 0.258151  [28832/42552]\n",
      "loss: 0.223140  [32032/42552]\n",
      "loss: 0.327957  [35232/42552]\n",
      "loss: 0.219791  [38432/42552]\n",
      "loss: 0.204506  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.232002 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.350578  [   32/42552]\n",
      "loss: 0.256563  [ 3232/42552]\n",
      "loss: 0.357631  [ 6432/42552]\n",
      "loss: 0.223253  [ 9632/42552]\n",
      "loss: 0.295803  [12832/42552]\n",
      "loss: 0.304835  [16032/42552]\n",
      "loss: 0.262081  [19232/42552]\n",
      "loss: 0.205535  [22432/42552]\n",
      "loss: 0.327609  [25632/42552]\n",
      "loss: 0.345150  [28832/42552]\n",
      "loss: 0.348668  [32032/42552]\n",
      "loss: 0.383137  [35232/42552]\n",
      "loss: 0.296158  [38432/42552]\n",
      "loss: 0.392146  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.230752 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.194434  [   32/42552]\n",
      "loss: 0.331448  [ 3232/42552]\n",
      "loss: 0.347929  [ 6432/42552]\n",
      "loss: 0.240051  [ 9632/42552]\n",
      "loss: 0.238563  [12832/42552]\n",
      "loss: 0.274392  [16032/42552]\n",
      "loss: 0.263184  [19232/42552]\n",
      "loss: 0.236929  [22432/42552]\n",
      "loss: 0.395923  [25632/42552]\n",
      "loss: 0.264469  [28832/42552]\n",
      "loss: 0.266826  [32032/42552]\n",
      "loss: 0.413403  [35232/42552]\n",
      "loss: 0.219132  [38432/42552]\n",
      "loss: 0.363218  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.233126 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.253478  [   32/42552]\n",
      "loss: 0.336273  [ 3232/42552]\n",
      "loss: 0.295263  [ 6432/42552]\n",
      "loss: 0.293701  [ 9632/42552]\n",
      "loss: 0.211909  [12832/42552]\n",
      "loss: 0.298740  [16032/42552]\n",
      "loss: 0.412594  [19232/42552]\n",
      "loss: 0.338572  [22432/42552]\n",
      "loss: 0.162686  [25632/42552]\n",
      "loss: 0.263771  [28832/42552]\n",
      "loss: 0.178985  [32032/42552]\n",
      "loss: 0.338452  [35232/42552]\n",
      "loss: 0.255957  [38432/42552]\n",
      "loss: 0.220798  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.232419 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.194754  [   32/42552]\n",
      "loss: 0.296739  [ 3232/42552]\n",
      "loss: 0.251831  [ 6432/42552]\n",
      "loss: 0.211773  [ 9632/42552]\n",
      "loss: 0.271357  [12832/42552]\n",
      "loss: 0.280025  [16032/42552]\n",
      "loss: 0.376159  [19232/42552]\n",
      "loss: 0.242357  [22432/42552]\n",
      "loss: 0.309697  [25632/42552]\n",
      "loss: 0.188332  [28832/42552]\n",
      "loss: 0.212235  [32032/42552]\n",
      "loss: 0.235477  [35232/42552]\n",
      "loss: 0.171066  [38432/42552]\n",
      "loss: 0.244498  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.236500 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.263642  [   32/42552]\n",
      "loss: 0.238106  [ 3232/42552]\n",
      "loss: 0.179028  [ 6432/42552]\n",
      "loss: 0.349051  [ 9632/42552]\n",
      "loss: 0.398297  [12832/42552]\n",
      "loss: 0.257665  [16032/42552]\n",
      "loss: 0.216919  [19232/42552]\n",
      "loss: 0.282410  [22432/42552]\n",
      "loss: 0.188775  [25632/42552]\n",
      "loss: 0.305430  [28832/42552]\n",
      "loss: 0.175291  [32032/42552]\n",
      "loss: 0.284885  [35232/42552]\n",
      "loss: 0.383173  [38432/42552]\n",
      "loss: 0.260092  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.234031 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.270003  [   32/42552]\n",
      "loss: 0.352192  [ 3232/42552]\n",
      "loss: 0.182964  [ 6432/42552]\n",
      "loss: 0.194023  [ 9632/42552]\n",
      "loss: 0.330859  [12832/42552]\n",
      "loss: 0.314488  [16032/42552]\n",
      "loss: 0.167010  [19232/42552]\n",
      "loss: 0.293432  [22432/42552]\n",
      "loss: 0.349710  [25632/42552]\n",
      "loss: 0.207448  [28832/42552]\n",
      "loss: 0.355334  [32032/42552]\n",
      "loss: 0.351375  [35232/42552]\n",
      "loss: 0.256894  [38432/42552]\n",
      "loss: 0.214825  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.250087 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.397002  [   32/42552]\n",
      "loss: 0.170014  [ 3232/42552]\n",
      "loss: 0.279675  [ 6432/42552]\n",
      "loss: 0.251258  [ 9632/42552]\n",
      "loss: 0.250705  [12832/42552]\n",
      "loss: 0.294052  [16032/42552]\n",
      "loss: 0.316354  [19232/42552]\n",
      "loss: 0.224560  [22432/42552]\n",
      "loss: 0.172059  [25632/42552]\n",
      "loss: 0.137831  [28832/42552]\n",
      "loss: 0.219625  [32032/42552]\n",
      "loss: 0.189088  [35232/42552]\n",
      "loss: 0.338794  [38432/42552]\n",
      "loss: 0.210889  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.229703 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.287532  [   32/42552]\n",
      "loss: 0.176022  [ 3232/42552]\n",
      "loss: 0.331448  [ 6432/42552]\n",
      "loss: 0.195500  [ 9632/42552]\n",
      "loss: 0.185854  [12832/42552]\n",
      "loss: 0.261421  [16032/42552]\n",
      "loss: 0.308181  [19232/42552]\n",
      "loss: 0.317739  [22432/42552]\n",
      "loss: 0.190442  [25632/42552]\n",
      "loss: 0.223482  [28832/42552]\n",
      "loss: 0.336359  [32032/42552]\n",
      "loss: 0.274102  [35232/42552]\n",
      "loss: 0.444980  [38432/42552]\n",
      "loss: 0.203946  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.230932 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.210038  [   32/42552]\n",
      "loss: 0.249336  [ 3232/42552]\n",
      "loss: 0.217375  [ 6432/42552]\n",
      "loss: 0.206564  [ 9632/42552]\n",
      "loss: 0.252542  [12832/42552]\n",
      "loss: 0.300876  [16032/42552]\n",
      "loss: 0.206432  [19232/42552]\n",
      "loss: 0.373494  [22432/42552]\n",
      "loss: 0.261889  [25632/42552]\n",
      "loss: 0.339033  [28832/42552]\n",
      "loss: 0.331946  [32032/42552]\n",
      "loss: 0.155221  [35232/42552]\n",
      "loss: 0.368409  [38432/42552]\n",
      "loss: 0.326155  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.232426 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.419616  [   32/42552]\n",
      "loss: 0.233001  [ 3232/42552]\n",
      "loss: 0.291449  [ 6432/42552]\n",
      "loss: 0.194269  [ 9632/42552]\n",
      "loss: 0.233052  [12832/42552]\n",
      "loss: 0.188212  [16032/42552]\n",
      "loss: 0.242126  [19232/42552]\n",
      "loss: 0.327001  [22432/42552]\n",
      "loss: 0.143523  [25632/42552]\n",
      "loss: 0.395844  [28832/42552]\n",
      "loss: 0.312241  [32032/42552]\n",
      "loss: 0.389679  [35232/42552]\n",
      "loss: 0.382745  [38432/42552]\n",
      "loss: 0.320027  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.240625 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.318986  [   32/42552]\n",
      "loss: 0.187588  [ 3232/42552]\n",
      "loss: 0.192401  [ 6432/42552]\n",
      "loss: 0.185291  [ 9632/42552]\n",
      "loss: 0.466307  [12832/42552]\n",
      "loss: 0.208413  [16032/42552]\n",
      "loss: 0.280024  [19232/42552]\n",
      "loss: 0.270090  [22432/42552]\n",
      "loss: 0.221750  [25632/42552]\n",
      "loss: 0.207870  [28832/42552]\n",
      "loss: 0.327181  [32032/42552]\n",
      "loss: 0.320264  [35232/42552]\n",
      "loss: 0.231223  [38432/42552]\n",
      "loss: 0.535863  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.247769 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.283696  [   32/42552]\n",
      "loss: 0.278543  [ 3232/42552]\n",
      "loss: 0.134498  [ 6432/42552]\n",
      "loss: 0.212481  [ 9632/42552]\n",
      "loss: 0.259745  [12832/42552]\n",
      "loss: 0.209564  [16032/42552]\n",
      "loss: 0.229065  [19232/42552]\n",
      "loss: 0.221656  [22432/42552]\n",
      "loss: 0.235562  [25632/42552]\n",
      "loss: 0.259515  [28832/42552]\n",
      "loss: 0.349553  [32032/42552]\n",
      "loss: 0.339450  [35232/42552]\n",
      "loss: 0.207830  [38432/42552]\n",
      "loss: 0.274711  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.227821 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.214253  [   32/42552]\n",
      "loss: 0.266668  [ 3232/42552]\n",
      "loss: 0.226263  [ 6432/42552]\n",
      "loss: 0.242959  [ 9632/42552]\n",
      "loss: 0.343920  [12832/42552]\n",
      "loss: 0.133221  [16032/42552]\n",
      "loss: 0.345097  [19232/42552]\n",
      "loss: 0.222317  [22432/42552]\n",
      "loss: 0.231346  [25632/42552]\n",
      "loss: 0.254740  [28832/42552]\n",
      "loss: 0.289403  [32032/42552]\n",
      "loss: 0.220861  [35232/42552]\n",
      "loss: 0.342491  [38432/42552]\n",
      "loss: 0.250655  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.232468 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.318194  [   32/42552]\n",
      "loss: 0.274003  [ 3232/42552]\n",
      "loss: 0.331852  [ 6432/42552]\n",
      "loss: 0.333670  [ 9632/42552]\n",
      "loss: 0.168877  [12832/42552]\n",
      "loss: 0.272680  [16032/42552]\n",
      "loss: 0.270826  [19232/42552]\n",
      "loss: 0.383045  [22432/42552]\n",
      "loss: 0.177517  [25632/42552]\n",
      "loss: 0.151422  [28832/42552]\n",
      "loss: 0.210192  [32032/42552]\n",
      "loss: 0.202775  [35232/42552]\n",
      "loss: 0.325107  [38432/42552]\n",
      "loss: 0.404372  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.232420 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.260025  [   32/42552]\n",
      "loss: 0.384952  [ 3232/42552]\n",
      "loss: 0.248491  [ 6432/42552]\n",
      "loss: 0.339481  [ 9632/42552]\n",
      "loss: 0.221829  [12832/42552]\n",
      "loss: 0.269404  [16032/42552]\n",
      "loss: 0.229245  [19232/42552]\n",
      "loss: 0.382604  [22432/42552]\n",
      "loss: 0.262247  [25632/42552]\n",
      "loss: 0.199404  [28832/42552]\n",
      "loss: 0.331497  [32032/42552]\n",
      "loss: 0.228462  [35232/42552]\n",
      "loss: 0.151070  [38432/42552]\n",
      "loss: 0.205034  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.229206 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.299792  [   32/42552]\n",
      "loss: 0.279836  [ 3232/42552]\n",
      "loss: 0.254249  [ 6432/42552]\n",
      "loss: 0.258040  [ 9632/42552]\n",
      "loss: 0.201894  [12832/42552]\n",
      "loss: 0.242288  [16032/42552]\n",
      "loss: 0.323727  [19232/42552]\n",
      "loss: 0.209735  [22432/42552]\n",
      "loss: 0.138683  [25632/42552]\n",
      "loss: 0.496841  [28832/42552]\n",
      "loss: 0.273053  [32032/42552]\n",
      "loss: 0.289956  [35232/42552]\n",
      "loss: 0.268391  [38432/42552]\n",
      "loss: 0.283861  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.230677 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.145298  [   32/42552]\n",
      "loss: 0.198517  [ 3232/42552]\n",
      "loss: 0.220719  [ 6432/42552]\n",
      "loss: 0.408443  [ 9632/42552]\n",
      "loss: 0.315430  [12832/42552]\n",
      "loss: 0.333699  [16032/42552]\n",
      "loss: 0.168901  [19232/42552]\n",
      "loss: 0.273540  [22432/42552]\n",
      "loss: 0.261785  [25632/42552]\n",
      "loss: 0.162999  [28832/42552]\n",
      "loss: 0.199762  [32032/42552]\n",
      "loss: 0.276934  [35232/42552]\n",
      "loss: 0.227547  [38432/42552]\n",
      "loss: 0.246622  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.228866 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.229038  [   32/42552]\n",
      "loss: 0.295665  [ 3232/42552]\n",
      "loss: 0.206123  [ 6432/42552]\n",
      "loss: 0.265033  [ 9632/42552]\n",
      "loss: 0.452910  [12832/42552]\n",
      "loss: 0.343133  [16032/42552]\n",
      "loss: 0.285160  [19232/42552]\n",
      "loss: 0.381427  [22432/42552]\n",
      "loss: 0.301218  [25632/42552]\n",
      "loss: 0.351623  [28832/42552]\n",
      "loss: 0.239366  [32032/42552]\n",
      "loss: 0.131409  [35232/42552]\n",
      "loss: 0.248939  [38432/42552]\n",
      "loss: 0.517640  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.226000 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.309651  [   32/42552]\n",
      "loss: 0.352949  [ 3232/42552]\n",
      "loss: 0.256822  [ 6432/42552]\n",
      "loss: 0.234630  [ 9632/42552]\n",
      "loss: 0.136267  [12832/42552]\n",
      "loss: 0.200335  [16032/42552]\n",
      "loss: 0.531787  [19232/42552]\n",
      "loss: 0.168469  [22432/42552]\n",
      "loss: 0.252552  [25632/42552]\n",
      "loss: 0.356217  [28832/42552]\n",
      "loss: 0.305508  [32032/42552]\n",
      "loss: 0.206026  [35232/42552]\n",
      "loss: 0.246390  [38432/42552]\n",
      "loss: 0.170417  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.226804 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.245523  [   32/42552]\n",
      "loss: 0.213375  [ 3232/42552]\n",
      "loss: 0.351248  [ 6432/42552]\n",
      "loss: 0.204056  [ 9632/42552]\n",
      "loss: 0.254193  [12832/42552]\n",
      "loss: 0.203880  [16032/42552]\n",
      "loss: 0.341267  [19232/42552]\n",
      "loss: 0.222280  [22432/42552]\n",
      "loss: 0.164203  [25632/42552]\n",
      "loss: 0.220945  [28832/42552]\n",
      "loss: 0.454709  [32032/42552]\n",
      "loss: 0.208150  [35232/42552]\n",
      "loss: 0.278290  [38432/42552]\n",
      "loss: 0.195109  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.232679 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.352326  [   32/42552]\n",
      "loss: 0.229486  [ 3232/42552]\n",
      "loss: 0.186008  [ 6432/42552]\n",
      "loss: 0.246835  [ 9632/42552]\n",
      "loss: 0.261379  [12832/42552]\n",
      "loss: 0.151645  [16032/42552]\n",
      "loss: 0.259544  [19232/42552]\n",
      "loss: 0.244258  [22432/42552]\n",
      "loss: 0.344393  [25632/42552]\n",
      "loss: 0.240105  [28832/42552]\n",
      "loss: 0.451619  [32032/42552]\n",
      "loss: 0.173795  [35232/42552]\n",
      "loss: 0.332995  [38432/42552]\n",
      "loss: 0.173454  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.235653 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.265993  [   32/42552]\n",
      "loss: 0.144699  [ 3232/42552]\n",
      "loss: 0.277135  [ 6432/42552]\n",
      "loss: 0.233361  [ 9632/42552]\n",
      "loss: 0.373137  [12832/42552]\n",
      "loss: 0.377836  [16032/42552]\n",
      "loss: 0.233014  [19232/42552]\n",
      "loss: 0.251248  [22432/42552]\n",
      "loss: 0.287728  [25632/42552]\n",
      "loss: 0.223267  [28832/42552]\n",
      "loss: 0.249374  [32032/42552]\n",
      "loss: 0.210954  [35232/42552]\n",
      "loss: 0.166906  [38432/42552]\n",
      "loss: 0.417709  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.232559 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.340228  [   32/42552]\n",
      "loss: 0.273399  [ 3232/42552]\n",
      "loss: 0.468416  [ 6432/42552]\n",
      "loss: 0.437014  [ 9632/42552]\n",
      "loss: 0.234879  [12832/42552]\n",
      "loss: 0.295189  [16032/42552]\n",
      "loss: 0.208568  [19232/42552]\n",
      "loss: 0.232774  [22432/42552]\n",
      "loss: 0.281496  [25632/42552]\n",
      "loss: 0.265640  [28832/42552]\n",
      "loss: 0.288926  [32032/42552]\n",
      "loss: 0.174085  [35232/42552]\n",
      "loss: 0.210234  [38432/42552]\n",
      "loss: 0.339055  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.238803 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.268516  [   32/42552]\n",
      "loss: 0.280816  [ 3232/42552]\n",
      "loss: 0.189902  [ 6432/42552]\n",
      "loss: 0.314557  [ 9632/42552]\n",
      "loss: 0.219800  [12832/42552]\n",
      "loss: 0.235905  [16032/42552]\n",
      "loss: 0.280963  [19232/42552]\n",
      "loss: 0.239610  [22432/42552]\n",
      "loss: 0.311729  [25632/42552]\n",
      "loss: 0.285376  [28832/42552]\n",
      "loss: 0.242349  [32032/42552]\n",
      "loss: 0.318160  [35232/42552]\n",
      "loss: 0.241242  [38432/42552]\n",
      "loss: 0.264884  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.243332 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.340958  [   32/42552]\n",
      "loss: 0.301952  [ 3232/42552]\n",
      "loss: 0.284135  [ 6432/42552]\n",
      "loss: 0.270964  [ 9632/42552]\n",
      "loss: 0.400482  [12832/42552]\n",
      "loss: 0.227487  [16032/42552]\n",
      "loss: 0.155982  [19232/42552]\n",
      "loss: 0.355975  [22432/42552]\n",
      "loss: 0.234961  [25632/42552]\n",
      "loss: 0.212694  [28832/42552]\n",
      "loss: 0.410999  [32032/42552]\n",
      "loss: 0.421496  [35232/42552]\n",
      "loss: 0.163184  [38432/42552]\n",
      "loss: 0.297484  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.232341 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.118911  [   32/42552]\n",
      "loss: 0.168322  [ 3232/42552]\n",
      "loss: 0.193338  [ 6432/42552]\n",
      "loss: 0.222043  [ 9632/42552]\n",
      "loss: 0.449977  [12832/42552]\n",
      "loss: 0.232789  [16032/42552]\n",
      "loss: 0.369869  [19232/42552]\n",
      "loss: 0.305856  [22432/42552]\n",
      "loss: 0.236678  [25632/42552]\n",
      "loss: 0.228072  [28832/42552]\n",
      "loss: 0.344382  [32032/42552]\n",
      "loss: 0.231424  [35232/42552]\n",
      "loss: 0.193358  [38432/42552]\n",
      "loss: 0.318891  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.234033 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.201313  [   32/42552]\n",
      "loss: 0.233737  [ 3232/42552]\n",
      "loss: 0.200089  [ 6432/42552]\n",
      "loss: 0.271692  [ 9632/42552]\n",
      "loss: 0.099548  [12832/42552]\n",
      "loss: 0.191762  [16032/42552]\n",
      "loss: 0.279306  [19232/42552]\n",
      "loss: 0.238769  [22432/42552]\n",
      "loss: 0.314324  [25632/42552]\n",
      "loss: 0.246386  [28832/42552]\n",
      "loss: 0.247522  [32032/42552]\n",
      "loss: 0.247719  [35232/42552]\n",
      "loss: 0.281433  [38432/42552]\n",
      "loss: 0.320800  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.225264 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.261912  [   32/42552]\n",
      "loss: 0.133502  [ 3232/42552]\n",
      "loss: 0.352321  [ 6432/42552]\n",
      "loss: 0.282500  [ 9632/42552]\n",
      "loss: 0.166500  [12832/42552]\n",
      "loss: 0.250189  [16032/42552]\n",
      "loss: 0.176813  [19232/42552]\n",
      "loss: 0.287974  [22432/42552]\n",
      "loss: 0.210679  [25632/42552]\n",
      "loss: 0.176173  [28832/42552]\n",
      "loss: 0.279881  [32032/42552]\n",
      "loss: 0.240553  [35232/42552]\n",
      "loss: 0.363682  [38432/42552]\n",
      "loss: 0.295063  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.228014 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.196954  [   32/42552]\n",
      "loss: 0.385828  [ 3232/42552]\n",
      "loss: 0.142473  [ 6432/42552]\n",
      "loss: 0.244660  [ 9632/42552]\n",
      "loss: 0.344943  [12832/42552]\n",
      "loss: 0.196951  [16032/42552]\n",
      "loss: 0.299501  [19232/42552]\n",
      "loss: 0.362234  [22432/42552]\n",
      "loss: 0.155758  [25632/42552]\n",
      "loss: 0.251324  [28832/42552]\n",
      "loss: 0.237543  [32032/42552]\n",
      "loss: 0.274320  [35232/42552]\n",
      "loss: 0.182486  [38432/42552]\n",
      "loss: 0.362251  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.234921 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.299799  [   32/42552]\n",
      "loss: 0.265430  [ 3232/42552]\n",
      "loss: 0.189741  [ 6432/42552]\n",
      "loss: 0.307748  [ 9632/42552]\n",
      "loss: 0.227197  [12832/42552]\n",
      "loss: 0.264709  [16032/42552]\n",
      "loss: 0.529859  [19232/42552]\n",
      "loss: 0.386283  [22432/42552]\n",
      "loss: 0.360418  [25632/42552]\n",
      "loss: 0.184785  [28832/42552]\n",
      "loss: 0.313899  [32032/42552]\n",
      "loss: 0.314635  [35232/42552]\n",
      "loss: 0.275740  [38432/42552]\n",
      "loss: 0.239908  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.224649 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.256560  [   32/42552]\n",
      "loss: 0.217538  [ 3232/42552]\n",
      "loss: 0.173963  [ 6432/42552]\n",
      "loss: 0.321182  [ 9632/42552]\n",
      "loss: 0.182994  [12832/42552]\n",
      "loss: 0.265076  [16032/42552]\n",
      "loss: 0.273202  [19232/42552]\n",
      "loss: 0.332301  [22432/42552]\n",
      "loss: 0.393884  [25632/42552]\n",
      "loss: 0.123084  [28832/42552]\n",
      "loss: 0.356125  [32032/42552]\n",
      "loss: 0.290714  [35232/42552]\n",
      "loss: 0.331824  [38432/42552]\n",
      "loss: 0.489711  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.225677 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.451220  [   32/42552]\n",
      "loss: 0.366125  [ 3232/42552]\n",
      "loss: 0.241603  [ 6432/42552]\n",
      "loss: 0.161904  [ 9632/42552]\n",
      "loss: 0.223247  [12832/42552]\n",
      "loss: 0.266667  [16032/42552]\n",
      "loss: 0.192393  [19232/42552]\n",
      "loss: 0.241045  [22432/42552]\n",
      "loss: 0.209806  [25632/42552]\n",
      "loss: 0.277249  [28832/42552]\n",
      "loss: 0.332710  [32032/42552]\n",
      "loss: 0.230707  [35232/42552]\n",
      "loss: 0.228407  [38432/42552]\n",
      "loss: 0.202565  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.228134 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.249484  [   32/42552]\n",
      "loss: 0.274814  [ 3232/42552]\n",
      "loss: 0.278934  [ 6432/42552]\n",
      "loss: 0.350510  [ 9632/42552]\n",
      "loss: 0.341737  [12832/42552]\n",
      "loss: 0.329571  [16032/42552]\n",
      "loss: 0.310348  [19232/42552]\n",
      "loss: 0.217845  [22432/42552]\n",
      "loss: 0.235039  [25632/42552]\n",
      "loss: 0.202999  [28832/42552]\n",
      "loss: 0.195066  [32032/42552]\n",
      "loss: 0.463817  [35232/42552]\n",
      "loss: 0.383834  [38432/42552]\n",
      "loss: 0.165083  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.226583 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.291921  [   32/42552]\n",
      "loss: 0.288343  [ 3232/42552]\n",
      "loss: 0.257906  [ 6432/42552]\n",
      "loss: 0.249719  [ 9632/42552]\n",
      "loss: 0.380040  [12832/42552]\n",
      "loss: 0.359634  [16032/42552]\n",
      "loss: 0.276460  [19232/42552]\n",
      "loss: 0.135753  [22432/42552]\n",
      "loss: 0.285838  [25632/42552]\n",
      "loss: 0.243172  [28832/42552]\n",
      "loss: 0.246345  [32032/42552]\n",
      "loss: 0.279129  [35232/42552]\n",
      "loss: 0.167532  [38432/42552]\n",
      "loss: 0.358501  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.228457 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.117282  [   32/42552]\n",
      "loss: 0.369647  [ 3232/42552]\n",
      "loss: 0.160483  [ 6432/42552]\n",
      "loss: 0.274935  [ 9632/42552]\n",
      "loss: 0.398649  [12832/42552]\n",
      "loss: 0.308713  [16032/42552]\n",
      "loss: 0.484923  [19232/42552]\n",
      "loss: 0.280428  [22432/42552]\n",
      "loss: 0.179319  [25632/42552]\n",
      "loss: 0.261301  [28832/42552]\n",
      "loss: 0.276070  [32032/42552]\n",
      "loss: 0.238786  [35232/42552]\n",
      "loss: 0.410762  [38432/42552]\n",
      "loss: 0.261206  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.230195 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.304242  [   32/42552]\n",
      "loss: 0.297108  [ 3232/42552]\n",
      "loss: 0.247183  [ 6432/42552]\n",
      "loss: 0.376004  [ 9632/42552]\n",
      "loss: 0.184701  [12832/42552]\n",
      "loss: 0.344505  [16032/42552]\n",
      "loss: 0.213176  [19232/42552]\n",
      "loss: 0.333524  [22432/42552]\n",
      "loss: 0.193706  [25632/42552]\n",
      "loss: 0.323709  [28832/42552]\n",
      "loss: 0.179826  [32032/42552]\n",
      "loss: 0.167513  [35232/42552]\n",
      "loss: 0.189763  [38432/42552]\n",
      "loss: 0.274734  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.232225 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.245542  [   32/42552]\n",
      "loss: 0.333467  [ 3232/42552]\n",
      "loss: 0.390784  [ 6432/42552]\n",
      "loss: 0.281116  [ 9632/42552]\n",
      "loss: 0.310418  [12832/42552]\n",
      "loss: 0.301077  [16032/42552]\n",
      "loss: 0.330984  [19232/42552]\n",
      "loss: 0.212644  [22432/42552]\n",
      "loss: 0.302923  [25632/42552]\n",
      "loss: 0.408151  [28832/42552]\n",
      "loss: 0.202528  [32032/42552]\n",
      "loss: 0.196815  [35232/42552]\n",
      "loss: 0.259412  [38432/42552]\n",
      "loss: 0.259846  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.230190 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.182272  [   32/42552]\n",
      "loss: 0.336548  [ 3232/42552]\n",
      "loss: 0.318230  [ 6432/42552]\n",
      "loss: 0.166141  [ 9632/42552]\n",
      "loss: 0.262174  [12832/42552]\n",
      "loss: 0.334290  [16032/42552]\n",
      "loss: 0.298037  [19232/42552]\n",
      "loss: 0.269009  [22432/42552]\n",
      "loss: 0.406548  [25632/42552]\n",
      "loss: 0.327746  [28832/42552]\n",
      "loss: 0.182543  [32032/42552]\n",
      "loss: 0.373296  [35232/42552]\n",
      "loss: 0.221030  [38432/42552]\n",
      "loss: 0.207739  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.226042 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.425606  [   32/42552]\n",
      "loss: 0.314777  [ 3232/42552]\n",
      "loss: 0.413647  [ 6432/42552]\n",
      "loss: 0.258918  [ 9632/42552]\n",
      "loss: 0.224519  [12832/42552]\n",
      "loss: 0.219335  [16032/42552]\n",
      "loss: 0.325704  [19232/42552]\n",
      "loss: 0.276862  [22432/42552]\n",
      "loss: 0.299789  [25632/42552]\n",
      "loss: 0.300123  [28832/42552]\n",
      "loss: 0.323563  [32032/42552]\n",
      "loss: 0.248226  [35232/42552]\n",
      "loss: 0.199703  [38432/42552]\n",
      "loss: 0.284857  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.227376 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.332357  [   32/42552]\n",
      "loss: 0.210672  [ 3232/42552]\n",
      "loss: 0.128083  [ 6432/42552]\n",
      "loss: 0.169932  [ 9632/42552]\n",
      "loss: 0.161068  [12832/42552]\n",
      "loss: 0.298653  [16032/42552]\n",
      "loss: 0.317462  [19232/42552]\n",
      "loss: 0.410106  [22432/42552]\n",
      "loss: 0.164092  [25632/42552]\n",
      "loss: 0.164132  [28832/42552]\n",
      "loss: 0.376664  [32032/42552]\n",
      "loss: 0.305375  [35232/42552]\n",
      "loss: 0.391816  [38432/42552]\n",
      "loss: 0.287075  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.225967 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.440466  [   32/42552]\n",
      "loss: 0.257608  [ 3232/42552]\n",
      "loss: 0.247573  [ 6432/42552]\n",
      "loss: 0.478388  [ 9632/42552]\n",
      "loss: 0.282503  [12832/42552]\n",
      "loss: 0.263987  [16032/42552]\n",
      "loss: 0.345275  [19232/42552]\n",
      "loss: 0.314481  [22432/42552]\n",
      "loss: 0.295302  [25632/42552]\n",
      "loss: 0.129046  [28832/42552]\n",
      "loss: 0.266502  [32032/42552]\n",
      "loss: 0.327032  [35232/42552]\n",
      "loss: 0.256420  [38432/42552]\n",
      "loss: 0.154034  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.228475 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.213004  [   32/42552]\n",
      "loss: 0.300925  [ 3232/42552]\n",
      "loss: 0.130071  [ 6432/42552]\n",
      "loss: 0.260299  [ 9632/42552]\n",
      "loss: 0.242888  [12832/42552]\n",
      "loss: 0.189231  [16032/42552]\n",
      "loss: 0.261737  [19232/42552]\n",
      "loss: 0.153124  [22432/42552]\n",
      "loss: 0.238543  [25632/42552]\n",
      "loss: 0.255510  [28832/42552]\n",
      "loss: 0.259816  [32032/42552]\n",
      "loss: 0.210601  [35232/42552]\n",
      "loss: 0.279161  [38432/42552]\n",
      "loss: 0.212781  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.226261 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.273859  [   32/42552]\n",
      "loss: 0.415960  [ 3232/42552]\n",
      "loss: 0.203251  [ 6432/42552]\n",
      "loss: 0.239958  [ 9632/42552]\n",
      "loss: 0.231851  [12832/42552]\n",
      "loss: 0.203256  [16032/42552]\n",
      "loss: 0.238025  [19232/42552]\n",
      "loss: 0.195795  [22432/42552]\n",
      "loss: 0.123415  [25632/42552]\n",
      "loss: 0.281273  [28832/42552]\n",
      "loss: 0.204661  [32032/42552]\n",
      "loss: 0.294609  [35232/42552]\n",
      "loss: 0.345450  [38432/42552]\n",
      "loss: 0.181167  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.234462 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.446354  [   32/42552]\n",
      "loss: 0.277097  [ 3232/42552]\n",
      "loss: 0.276858  [ 6432/42552]\n",
      "loss: 0.294115  [ 9632/42552]\n",
      "loss: 0.234716  [12832/42552]\n",
      "loss: 0.269640  [16032/42552]\n",
      "loss: 0.274928  [19232/42552]\n",
      "loss: 0.265820  [22432/42552]\n",
      "loss: 0.403710  [25632/42552]\n",
      "loss: 0.282631  [28832/42552]\n",
      "loss: 0.231575  [32032/42552]\n",
      "loss: 0.210675  [35232/42552]\n",
      "loss: 0.185732  [38432/42552]\n",
      "loss: 0.409425  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.229788 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.162222  [   32/42552]\n",
      "loss: 0.230225  [ 3232/42552]\n",
      "loss: 0.149433  [ 6432/42552]\n",
      "loss: 0.353578  [ 9632/42552]\n",
      "loss: 0.256791  [12832/42552]\n",
      "loss: 0.225218  [16032/42552]\n",
      "loss: 0.323338  [19232/42552]\n",
      "loss: 0.208968  [22432/42552]\n",
      "loss: 0.280827  [25632/42552]\n",
      "loss: 0.246084  [28832/42552]\n",
      "loss: 0.194918  [32032/42552]\n",
      "loss: 0.279085  [35232/42552]\n",
      "loss: 0.277808  [38432/42552]\n",
      "loss: 0.371439  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.225461 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.172721  [   32/42552]\n",
      "loss: 0.283258  [ 3232/42552]\n",
      "loss: 0.265979  [ 6432/42552]\n",
      "loss: 0.142562  [ 9632/42552]\n",
      "loss: 0.243409  [12832/42552]\n",
      "loss: 0.343131  [16032/42552]\n",
      "loss: 0.315678  [19232/42552]\n",
      "loss: 0.178667  [22432/42552]\n",
      "loss: 0.137941  [25632/42552]\n",
      "loss: 0.208532  [28832/42552]\n",
      "loss: 0.178874  [32032/42552]\n",
      "loss: 0.334267  [35232/42552]\n",
      "loss: 0.188700  [38432/42552]\n",
      "loss: 0.215889  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.231160 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.252947  [   32/42552]\n",
      "loss: 0.439147  [ 3232/42552]\n",
      "loss: 0.196419  [ 6432/42552]\n",
      "loss: 0.352321  [ 9632/42552]\n",
      "loss: 0.201597  [12832/42552]\n",
      "loss: 0.255393  [16032/42552]\n",
      "loss: 0.255797  [19232/42552]\n",
      "loss: 0.268558  [22432/42552]\n",
      "loss: 0.252615  [25632/42552]\n",
      "loss: 0.253121  [28832/42552]\n",
      "loss: 0.339918  [32032/42552]\n",
      "loss: 0.248122  [35232/42552]\n",
      "loss: 0.287284  [38432/42552]\n",
      "loss: 0.228476  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.235372 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.188039  [   32/42552]\n",
      "loss: 0.248907  [ 3232/42552]\n",
      "loss: 0.234277  [ 6432/42552]\n",
      "loss: 0.237819  [ 9632/42552]\n",
      "loss: 0.407124  [12832/42552]\n",
      "loss: 0.245947  [16032/42552]\n",
      "loss: 0.165106  [19232/42552]\n",
      "loss: 0.247682  [22432/42552]\n",
      "loss: 0.172133  [25632/42552]\n",
      "loss: 0.292285  [28832/42552]\n",
      "loss: 0.111749  [32032/42552]\n",
      "loss: 0.319969  [35232/42552]\n",
      "loss: 0.182970  [38432/42552]\n",
      "loss: 0.151071  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.229064 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.263448  [   32/42552]\n",
      "loss: 0.143313  [ 3232/42552]\n",
      "loss: 0.266708  [ 6432/42552]\n",
      "loss: 0.310653  [ 9632/42552]\n",
      "loss: 0.227752  [12832/42552]\n",
      "loss: 0.195883  [16032/42552]\n",
      "loss: 0.334148  [19232/42552]\n",
      "loss: 0.436451  [22432/42552]\n",
      "loss: 0.221401  [25632/42552]\n",
      "loss: 0.311976  [28832/42552]\n",
      "loss: 0.134050  [32032/42552]\n",
      "loss: 0.182635  [35232/42552]\n",
      "loss: 0.361580  [38432/42552]\n",
      "loss: 0.255546  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.226292 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.215202  [   32/42552]\n",
      "loss: 0.342418  [ 3232/42552]\n",
      "loss: 0.215005  [ 6432/42552]\n",
      "loss: 0.256094  [ 9632/42552]\n",
      "loss: 0.176503  [12832/42552]\n",
      "loss: 0.374669  [16032/42552]\n",
      "loss: 0.388916  [19232/42552]\n",
      "loss: 0.190009  [22432/42552]\n",
      "loss: 0.385971  [25632/42552]\n",
      "loss: 0.137505  [28832/42552]\n",
      "loss: 0.184004  [32032/42552]\n",
      "loss: 0.245278  [35232/42552]\n",
      "loss: 0.262734  [38432/42552]\n",
      "loss: 0.262790  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.234131 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.356998  [   32/42552]\n",
      "loss: 0.260131  [ 3232/42552]\n",
      "loss: 0.194203  [ 6432/42552]\n",
      "loss: 0.248175  [ 9632/42552]\n",
      "loss: 0.360644  [12832/42552]\n",
      "loss: 0.319824  [16032/42552]\n",
      "loss: 0.216836  [19232/42552]\n",
      "loss: 0.327025  [22432/42552]\n",
      "loss: 0.232415  [25632/42552]\n",
      "loss: 0.298899  [28832/42552]\n",
      "loss: 0.164545  [32032/42552]\n",
      "loss: 0.333294  [35232/42552]\n",
      "loss: 0.275694  [38432/42552]\n",
      "loss: 0.351914  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.244423 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.342936  [   32/42552]\n",
      "loss: 0.182979  [ 3232/42552]\n",
      "loss: 0.482901  [ 6432/42552]\n",
      "loss: 0.370025  [ 9632/42552]\n",
      "loss: 0.133529  [12832/42552]\n",
      "loss: 0.295007  [16032/42552]\n",
      "loss: 0.231145  [19232/42552]\n",
      "loss: 0.294396  [22432/42552]\n",
      "loss: 0.253365  [25632/42552]\n",
      "loss: 0.183839  [28832/42552]\n",
      "loss: 0.269667  [32032/42552]\n",
      "loss: 0.207925  [35232/42552]\n",
      "loss: 0.259623  [38432/42552]\n",
      "loss: 0.200811  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.226231 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.341572  [   32/42552]\n",
      "loss: 0.263313  [ 3232/42552]\n",
      "loss: 0.205375  [ 6432/42552]\n",
      "loss: 0.349795  [ 9632/42552]\n",
      "loss: 0.222263  [12832/42552]\n",
      "loss: 0.221696  [16032/42552]\n",
      "loss: 0.161125  [19232/42552]\n",
      "loss: 0.198328  [22432/42552]\n",
      "loss: 0.128985  [25632/42552]\n",
      "loss: 0.232134  [28832/42552]\n",
      "loss: 0.357462  [32032/42552]\n",
      "loss: 0.148153  [35232/42552]\n",
      "loss: 0.139883  [38432/42552]\n",
      "loss: 0.240047  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.232525 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.274088  [   32/42552]\n",
      "loss: 0.297240  [ 3232/42552]\n",
      "loss: 0.260169  [ 6432/42552]\n",
      "loss: 0.323194  [ 9632/42552]\n",
      "loss: 0.166533  [12832/42552]\n",
      "loss: 0.173454  [16032/42552]\n",
      "loss: 0.173978  [19232/42552]\n",
      "loss: 0.330665  [22432/42552]\n",
      "loss: 0.303886  [25632/42552]\n",
      "loss: 0.354393  [28832/42552]\n",
      "loss: 0.430460  [32032/42552]\n",
      "loss: 0.316633  [35232/42552]\n",
      "loss: 0.416511  [38432/42552]\n",
      "loss: 0.176103  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.236103 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.188713  [   32/42552]\n",
      "loss: 0.174275  [ 3232/42552]\n",
      "loss: 0.257448  [ 6432/42552]\n",
      "loss: 0.444939  [ 9632/42552]\n",
      "loss: 0.372257  [12832/42552]\n",
      "loss: 0.347607  [16032/42552]\n",
      "loss: 0.305408  [19232/42552]\n",
      "loss: 0.298031  [22432/42552]\n",
      "loss: 0.277161  [25632/42552]\n",
      "loss: 0.276808  [28832/42552]\n",
      "loss: 0.291490  [32032/42552]\n",
      "loss: 0.143663  [35232/42552]\n",
      "loss: 0.104616  [38432/42552]\n",
      "loss: 0.175496  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.228177 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.247572  [   32/42552]\n",
      "loss: 0.197552  [ 3232/42552]\n",
      "loss: 0.233358  [ 6432/42552]\n",
      "loss: 0.180469  [ 9632/42552]\n",
      "loss: 0.433913  [12832/42552]\n",
      "loss: 0.302396  [16032/42552]\n",
      "loss: 0.137870  [19232/42552]\n",
      "loss: 0.246494  [22432/42552]\n",
      "loss: 0.245385  [25632/42552]\n",
      "loss: 0.619615  [28832/42552]\n",
      "loss: 0.192584  [32032/42552]\n",
      "loss: 0.217858  [35232/42552]\n",
      "loss: 0.356148  [38432/42552]\n",
      "loss: 0.176075  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.224831 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.211294  [   32/42552]\n",
      "loss: 0.270189  [ 3232/42552]\n",
      "loss: 0.256010  [ 6432/42552]\n",
      "loss: 0.296019  [ 9632/42552]\n",
      "loss: 0.301323  [12832/42552]\n",
      "loss: 0.236833  [16032/42552]\n",
      "loss: 0.347471  [19232/42552]\n",
      "loss: 0.322512  [22432/42552]\n",
      "loss: 0.125716  [25632/42552]\n",
      "loss: 0.237728  [28832/42552]\n",
      "loss: 0.313580  [32032/42552]\n",
      "loss: 0.281121  [35232/42552]\n",
      "loss: 0.178206  [38432/42552]\n",
      "loss: 0.279018  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.225862 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.410746  [   32/42552]\n",
      "loss: 0.229847  [ 3232/42552]\n",
      "loss: 0.237023  [ 6432/42552]\n",
      "loss: 0.159371  [ 9632/42552]\n",
      "loss: 0.168002  [12832/42552]\n",
      "loss: 0.294186  [16032/42552]\n",
      "loss: 0.253222  [19232/42552]\n",
      "loss: 0.212198  [22432/42552]\n",
      "loss: 0.191708  [25632/42552]\n",
      "loss: 0.261142  [28832/42552]\n",
      "loss: 0.378211  [32032/42552]\n",
      "loss: 0.264861  [35232/42552]\n",
      "loss: 0.185252  [38432/42552]\n",
      "loss: 0.284846  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.224790 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.214190  [   32/42552]\n",
      "loss: 0.202984  [ 3232/42552]\n",
      "loss: 0.199397  [ 6432/42552]\n",
      "loss: 0.237253  [ 9632/42552]\n",
      "loss: 0.112336  [12832/42552]\n",
      "loss: 0.169357  [16032/42552]\n",
      "loss: 0.357530  [19232/42552]\n",
      "loss: 0.190186  [22432/42552]\n",
      "loss: 0.359269  [25632/42552]\n",
      "loss: 0.184351  [28832/42552]\n",
      "loss: 0.155456  [32032/42552]\n",
      "loss: 0.207453  [35232/42552]\n",
      "loss: 0.113283  [38432/42552]\n",
      "loss: 0.328027  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.224267 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.361822  [   32/42552]\n",
      "loss: 0.249158  [ 3232/42552]\n",
      "loss: 0.294054  [ 6432/42552]\n",
      "loss: 0.386217  [ 9632/42552]\n",
      "loss: 0.247859  [12832/42552]\n",
      "loss: 0.296466  [16032/42552]\n",
      "loss: 0.299855  [19232/42552]\n",
      "loss: 0.278666  [22432/42552]\n",
      "loss: 0.245396  [25632/42552]\n",
      "loss: 0.212693  [28832/42552]\n",
      "loss: 0.185114  [32032/42552]\n",
      "loss: 0.392962  [35232/42552]\n",
      "loss: 0.177770  [38432/42552]\n",
      "loss: 0.299377  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.235630 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.218919  [   32/42552]\n",
      "loss: 0.253680  [ 3232/42552]\n",
      "loss: 0.162305  [ 6432/42552]\n",
      "loss: 0.239283  [ 9632/42552]\n",
      "loss: 0.145076  [12832/42552]\n",
      "loss: 0.346772  [16032/42552]\n",
      "loss: 0.260600  [19232/42552]\n",
      "loss: 0.361414  [22432/42552]\n",
      "loss: 0.260984  [25632/42552]\n",
      "loss: 0.276382  [28832/42552]\n",
      "loss: 0.208088  [32032/42552]\n",
      "loss: 0.313055  [35232/42552]\n",
      "loss: 0.260838  [38432/42552]\n",
      "loss: 0.270398  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.226503 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.367628  [   32/42552]\n",
      "loss: 0.124833  [ 3232/42552]\n",
      "loss: 0.310679  [ 6432/42552]\n",
      "loss: 0.418000  [ 9632/42552]\n",
      "loss: 0.175428  [12832/42552]\n",
      "loss: 0.380811  [16032/42552]\n",
      "loss: 0.144712  [19232/42552]\n",
      "loss: 0.167247  [22432/42552]\n",
      "loss: 0.224597  [25632/42552]\n",
      "loss: 0.238531  [28832/42552]\n",
      "loss: 0.270966  [32032/42552]\n",
      "loss: 0.218087  [35232/42552]\n",
      "loss: 0.270155  [38432/42552]\n",
      "loss: 0.509722  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.229718 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.264770  [   32/42552]\n",
      "loss: 0.323509  [ 3232/42552]\n",
      "loss: 0.272168  [ 6432/42552]\n",
      "loss: 0.375631  [ 9632/42552]\n",
      "loss: 0.257614  [12832/42552]\n",
      "loss: 0.304236  [16032/42552]\n",
      "loss: 0.311902  [19232/42552]\n",
      "loss: 0.225535  [22432/42552]\n",
      "loss: 0.262682  [25632/42552]\n",
      "loss: 0.207554  [28832/42552]\n",
      "loss: 0.211819  [32032/42552]\n",
      "loss: 0.238475  [35232/42552]\n",
      "loss: 0.374038  [38432/42552]\n",
      "loss: 0.283880  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.232157 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.278136  [   32/42552]\n",
      "loss: 0.200507  [ 3232/42552]\n",
      "loss: 0.193598  [ 6432/42552]\n",
      "loss: 0.217764  [ 9632/42552]\n",
      "loss: 0.225587  [12832/42552]\n",
      "loss: 0.183980  [16032/42552]\n",
      "loss: 0.232944  [19232/42552]\n",
      "loss: 0.346545  [22432/42552]\n",
      "loss: 0.309129  [25632/42552]\n",
      "loss: 0.395672  [28832/42552]\n",
      "loss: 0.366630  [32032/42552]\n",
      "loss: 0.181959  [35232/42552]\n",
      "loss: 0.225516  [38432/42552]\n",
      "loss: 0.175287  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.229768 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.220626  [   32/42552]\n",
      "loss: 0.213168  [ 3232/42552]\n",
      "loss: 0.214782  [ 6432/42552]\n",
      "loss: 0.251589  [ 9632/42552]\n",
      "loss: 0.356851  [12832/42552]\n",
      "loss: 0.283593  [16032/42552]\n",
      "loss: 0.198277  [19232/42552]\n",
      "loss: 0.204927  [22432/42552]\n",
      "loss: 0.225355  [25632/42552]\n",
      "loss: 0.280367  [28832/42552]\n",
      "loss: 0.288736  [32032/42552]\n",
      "loss: 0.325236  [35232/42552]\n",
      "loss: 0.230812  [38432/42552]\n",
      "loss: 0.385288  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.226758 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.357141  [   32/42552]\n",
      "loss: 0.300270  [ 3232/42552]\n",
      "loss: 0.227963  [ 6432/42552]\n",
      "loss: 0.359210  [ 9632/42552]\n",
      "loss: 0.265123  [12832/42552]\n",
      "loss: 0.250506  [16032/42552]\n",
      "loss: 0.377418  [19232/42552]\n",
      "loss: 0.274964  [22432/42552]\n",
      "loss: 0.124259  [25632/42552]\n",
      "loss: 0.237739  [28832/42552]\n",
      "loss: 0.271844  [32032/42552]\n",
      "loss: 0.216151  [35232/42552]\n",
      "loss: 0.288874  [38432/42552]\n",
      "loss: 0.305878  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.228456 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.292303  [   32/42552]\n",
      "loss: 0.323000  [ 3232/42552]\n",
      "loss: 0.320975  [ 6432/42552]\n",
      "loss: 0.198335  [ 9632/42552]\n",
      "loss: 0.266303  [12832/42552]\n",
      "loss: 0.297987  [16032/42552]\n",
      "loss: 0.294999  [19232/42552]\n",
      "loss: 0.295631  [22432/42552]\n",
      "loss: 0.291586  [25632/42552]\n",
      "loss: 0.305543  [28832/42552]\n",
      "loss: 0.371006  [32032/42552]\n",
      "loss: 0.166006  [35232/42552]\n",
      "loss: 0.355963  [38432/42552]\n",
      "loss: 0.202824  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.231779 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.139592  [   32/42552]\n",
      "loss: 0.358049  [ 3232/42552]\n",
      "loss: 0.457265  [ 6432/42552]\n",
      "loss: 0.165439  [ 9632/42552]\n",
      "loss: 0.205849  [12832/42552]\n",
      "loss: 0.331330  [16032/42552]\n",
      "loss: 0.296877  [19232/42552]\n",
      "loss: 0.241686  [22432/42552]\n",
      "loss: 0.407112  [25632/42552]\n",
      "loss: 0.237848  [28832/42552]\n",
      "loss: 0.299824  [32032/42552]\n",
      "loss: 0.200773  [35232/42552]\n",
      "loss: 0.428343  [38432/42552]\n",
      "loss: 0.218077  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.224278 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.426803  [   32/42552]\n",
      "loss: 0.401974  [ 3232/42552]\n",
      "loss: 0.208233  [ 6432/42552]\n",
      "loss: 0.304015  [ 9632/42552]\n",
      "loss: 0.294221  [12832/42552]\n",
      "loss: 0.356938  [16032/42552]\n",
      "loss: 0.203216  [19232/42552]\n",
      "loss: 0.320524  [22432/42552]\n",
      "loss: 0.536349  [25632/42552]\n",
      "loss: 0.304506  [28832/42552]\n",
      "loss: 0.283302  [32032/42552]\n",
      "loss: 0.191209  [35232/42552]\n",
      "loss: 0.488473  [38432/42552]\n",
      "loss: 0.253424  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.230193 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.333273  [   32/42552]\n",
      "loss: 0.138794  [ 3232/42552]\n",
      "loss: 0.187746  [ 6432/42552]\n",
      "loss: 0.303839  [ 9632/42552]\n",
      "loss: 0.139376  [12832/42552]\n",
      "loss: 0.210107  [16032/42552]\n",
      "loss: 0.177745  [19232/42552]\n",
      "loss: 0.243553  [22432/42552]\n",
      "loss: 0.171803  [25632/42552]\n",
      "loss: 0.255866  [28832/42552]\n",
      "loss: 0.256568  [32032/42552]\n",
      "loss: 0.242868  [35232/42552]\n",
      "loss: 0.160089  [38432/42552]\n",
      "loss: 0.294878  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.223955 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.250522  [   32/42552]\n",
      "loss: 0.238006  [ 3232/42552]\n",
      "loss: 0.225697  [ 6432/42552]\n",
      "loss: 0.280782  [ 9632/42552]\n",
      "loss: 0.185387  [12832/42552]\n",
      "loss: 0.388562  [16032/42552]\n",
      "loss: 0.125886  [19232/42552]\n",
      "loss: 0.174096  [22432/42552]\n",
      "loss: 0.333119  [25632/42552]\n",
      "loss: 0.168578  [28832/42552]\n",
      "loss: 0.220120  [32032/42552]\n",
      "loss: 0.189515  [35232/42552]\n",
      "loss: 0.298140  [38432/42552]\n",
      "loss: 0.280980  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.225147 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.246238  [   32/42552]\n",
      "loss: 0.258273  [ 3232/42552]\n",
      "loss: 0.296869  [ 6432/42552]\n",
      "loss: 0.258493  [ 9632/42552]\n",
      "loss: 0.204145  [12832/42552]\n",
      "loss: 0.209573  [16032/42552]\n",
      "loss: 0.206524  [19232/42552]\n",
      "loss: 0.175733  [22432/42552]\n",
      "loss: 0.297576  [25632/42552]\n",
      "loss: 0.291323  [28832/42552]\n",
      "loss: 0.384591  [32032/42552]\n",
      "loss: 0.290143  [35232/42552]\n",
      "loss: 0.388454  [38432/42552]\n",
      "loss: 0.334793  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.232094 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.148651  [   32/42552]\n",
      "loss: 0.424301  [ 3232/42552]\n",
      "loss: 0.262549  [ 6432/42552]\n",
      "loss: 0.260045  [ 9632/42552]\n",
      "loss: 0.174028  [12832/42552]\n",
      "loss: 0.323947  [16032/42552]\n",
      "loss: 0.252191  [19232/42552]\n",
      "loss: 0.212971  [22432/42552]\n",
      "loss: 0.299059  [25632/42552]\n",
      "loss: 0.355141  [28832/42552]\n",
      "loss: 0.214438  [32032/42552]\n",
      "loss: 0.334120  [35232/42552]\n",
      "loss: 0.260626  [38432/42552]\n",
      "loss: 0.337261  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.225220 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.249754  [   32/42552]\n",
      "loss: 0.222220  [ 3232/42552]\n",
      "loss: 0.148984  [ 6432/42552]\n",
      "loss: 0.259280  [ 9632/42552]\n",
      "loss: 0.217761  [12832/42552]\n",
      "loss: 0.163389  [16032/42552]\n",
      "loss: 0.374403  [19232/42552]\n",
      "loss: 0.328966  [22432/42552]\n",
      "loss: 0.230247  [25632/42552]\n",
      "loss: 0.280036  [28832/42552]\n",
      "loss: 0.176312  [32032/42552]\n",
      "loss: 0.286946  [35232/42552]\n",
      "loss: 0.229646  [38432/42552]\n",
      "loss: 0.239566  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.223311 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.189648  [   32/42552]\n",
      "loss: 0.265148  [ 3232/42552]\n",
      "loss: 0.391964  [ 6432/42552]\n",
      "loss: 0.358357  [ 9632/42552]\n",
      "loss: 0.249266  [12832/42552]\n",
      "loss: 0.270382  [16032/42552]\n",
      "loss: 0.321131  [19232/42552]\n",
      "loss: 0.357569  [22432/42552]\n",
      "loss: 0.219071  [25632/42552]\n",
      "loss: 0.441714  [28832/42552]\n",
      "loss: 0.203285  [32032/42552]\n",
      "loss: 0.429894  [35232/42552]\n",
      "loss: 0.224696  [38432/42552]\n",
      "loss: 0.310193  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.227424 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.168880  [   32/42552]\n",
      "loss: 0.556724  [ 3232/42552]\n",
      "loss: 0.275005  [ 6432/42552]\n",
      "loss: 0.312787  [ 9632/42552]\n",
      "loss: 0.348928  [12832/42552]\n",
      "loss: 0.246779  [16032/42552]\n",
      "loss: 0.159606  [19232/42552]\n",
      "loss: 0.149465  [22432/42552]\n",
      "loss: 0.272824  [25632/42552]\n",
      "loss: 0.281182  [28832/42552]\n",
      "loss: 0.179048  [32032/42552]\n",
      "loss: 0.371667  [35232/42552]\n",
      "loss: 0.203506  [38432/42552]\n",
      "loss: 0.176799  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.242053 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.208011  [   32/42552]\n",
      "loss: 0.218769  [ 3232/42552]\n",
      "loss: 0.222025  [ 6432/42552]\n",
      "loss: 0.219767  [ 9632/42552]\n",
      "loss: 0.155833  [12832/42552]\n",
      "loss: 0.299012  [16032/42552]\n",
      "loss: 0.268946  [19232/42552]\n",
      "loss: 0.145188  [22432/42552]\n",
      "loss: 0.330466  [25632/42552]\n",
      "loss: 0.193266  [28832/42552]\n",
      "loss: 0.251535  [32032/42552]\n",
      "loss: 0.403702  [35232/42552]\n",
      "loss: 0.226479  [38432/42552]\n",
      "loss: 0.373200  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.224851 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.186807  [   32/42552]\n",
      "loss: 0.211761  [ 3232/42552]\n",
      "loss: 0.229443  [ 6432/42552]\n",
      "loss: 0.160098  [ 9632/42552]\n",
      "loss: 0.219857  [12832/42552]\n",
      "loss: 0.218902  [16032/42552]\n",
      "loss: 0.269546  [19232/42552]\n",
      "loss: 0.153750  [22432/42552]\n",
      "loss: 0.189029  [25632/42552]\n",
      "loss: 0.272900  [28832/42552]\n",
      "loss: 0.187406  [32032/42552]\n",
      "loss: 0.263431  [35232/42552]\n",
      "loss: 0.390830  [38432/42552]\n",
      "loss: 0.290376  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.226884 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.481944  [   32/42552]\n",
      "loss: 0.159722  [ 3232/42552]\n",
      "loss: 0.261238  [ 6432/42552]\n",
      "loss: 0.308113  [ 9632/42552]\n",
      "loss: 0.268340  [12832/42552]\n",
      "loss: 0.275767  [16032/42552]\n",
      "loss: 0.234559  [19232/42552]\n",
      "loss: 0.174900  [22432/42552]\n",
      "loss: 0.209578  [25632/42552]\n",
      "loss: 0.331943  [28832/42552]\n",
      "loss: 0.312744  [32032/42552]\n",
      "loss: 0.266543  [35232/42552]\n",
      "loss: 0.592680  [38432/42552]\n",
      "loss: 0.288379  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.228132 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.222295  [   32/42552]\n",
      "loss: 0.256215  [ 3232/42552]\n",
      "loss: 0.275035  [ 6432/42552]\n",
      "loss: 0.173423  [ 9632/42552]\n",
      "loss: 0.189244  [12832/42552]\n",
      "loss: 0.136516  [16032/42552]\n",
      "loss: 0.247707  [19232/42552]\n",
      "loss: 0.323279  [22432/42552]\n",
      "loss: 0.292530  [25632/42552]\n",
      "loss: 0.152853  [28832/42552]\n",
      "loss: 0.414029  [32032/42552]\n",
      "loss: 0.357827  [35232/42552]\n",
      "loss: 0.204501  [38432/42552]\n",
      "loss: 0.343750  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.228904 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.383575  [   32/42552]\n",
      "loss: 0.218852  [ 3232/42552]\n",
      "loss: 0.328875  [ 6432/42552]\n",
      "loss: 0.243881  [ 9632/42552]\n",
      "loss: 0.326880  [12832/42552]\n",
      "loss: 0.289728  [16032/42552]\n",
      "loss: 0.256649  [19232/42552]\n",
      "loss: 0.244467  [22432/42552]\n",
      "loss: 0.148715  [25632/42552]\n",
      "loss: 0.268847  [28832/42552]\n",
      "loss: 0.180450  [32032/42552]\n",
      "loss: 0.283916  [35232/42552]\n",
      "loss: 0.187184  [38432/42552]\n",
      "loss: 0.302216  [41632/42552]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.227746 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model, train_losses, val_losses = train_early_stopping(\n",
    "    train_loader, val_loader, model, loss_fn, optimizer, epochs=500, early_stopping=True,\n",
    "    threshold=0.2,\n",
    "    counter=True, clipping=True)\n",
    "\n",
    "# plot training and validation losses\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'models/display-box-18-rolling-means-half-noise-location.mdl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/gwjrml9d76v_zwbyb2x6yc0r0000gn/T/ipykernel_12677/2522857457.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CounterCNN().to(device)\n",
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.225552 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2255519240646434, 0.7028716454387367)"
      ]
     },
     "execution_count": 1629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out of sample test\n",
    "model.eval()\n",
    "# test(test_loader, model, loss_fn)\n",
    "test_counter(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 0, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 0, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 1, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 0, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 0, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 1, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n",
      "Predicted class: 0, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 1, Actual label: tensor([1., 0., 0.])\n",
      "Predicted class: 1, Actual label: tensor([0., 1., 0.])\n",
      "Predicted class: 2, Actual label: tensor([0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # Get a single example from the test dataset\n",
    "    example_data, example_label = test_dataset[i]\n",
    "\n",
    "    # Move the example data to the appropriate device\n",
    "    example_data = example_data.unsqueeze(0).to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get the model's prediction\n",
    "    with torch.no_grad():\n",
    "        example_data = example_data.to(device)\n",
    "        output = model(example_data)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "    # Print the predicted class and the actual label\n",
    "    print(f'Predicted class: {predicted_class}, Actual label: {example_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for X, _ in test_loader:\n",
    "        X = X.to(device)\n",
    "        output = model(X)\n",
    "        # predicted_classes = output.argmax(dim=1)\n",
    "        # predictions.extend(predicted_classes.cpu().numpy())\n",
    "        # predicted_zones = torch.round(torch.clamp(output, min=0, max=1))\n",
    "        predicted_zones = torch.round(torch.sigmoid(output))\n",
    "        predictions.extend(predicted_zones.cpu().numpy())\n",
    "\n",
    "# Convert predictions to a numpy array\n",
    "y_preds = np.array(predictions)\n",
    "print(y_preds)\n",
    "\n",
    "y_test = np.array([y for _, y in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# FULL DATASET\n",
    "# Create a DataLoader for the entire dataset\n",
    "full_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predictions\n",
    "predictions = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for X, _ in full_loader:\n",
    "        X = X.to(device)\n",
    "        output = model(X)\n",
    "        predicted_classes = output.argmax(dim=1)\n",
    "        predictions.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "# Convert outputs to a numpy array\n",
    "y_preds = np.array(predictions)\n",
    "print(y_preds)\n",
    "\n",
    "y_test = np.array([y for _, y in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test = np.array([y[0] for _, y in test_dataset]).astype(int)\n",
    "y_preds = np.array([p[0] for p in predictions]).astype(int)\n",
    "conf_matrix = confusion_matrix(y_test, y_preds)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Change figure size and increase dpi for better resolution\n",
    "plt.figure(figsize=(8,6), dpi=100)\n",
    "# Scale up the size of all text\n",
    " \n",
    "# Plot Confusion Matrix using Seaborn heatmap()\n",
    "# Parameters:\n",
    "# first param - confusion matrix in array format   \n",
    "# annot = True: show the numbers in each heatmap cell\n",
    "# fmt = 'd': show numbers as integers. \n",
    "ax = sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', )\n",
    " \n",
    "# set x-axis label and ticks. \n",
    "ax.set_xlabel(\"Predicted\", fontsize=14, labelpad=20)\n",
    "# tick_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "# tick_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# tick_labels = [0, 1, 2, 3, 4, 5, 6]\n",
    "# tick_labels = [0, 1, 2]\n",
    "tick_labels = [0, 1]\n",
    "ax.xaxis.set_ticklabels(tick_labels)\n",
    " \n",
    "# set y-axis label and ticks\n",
    "ax.set_ylabel(\"Actual\", fontsize=14, labelpad=20)\n",
    "ax.yaxis.set_ticklabels(tick_labels)\n",
    " \n",
    "# set plot title\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=14, pad=20)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = [\"000\", \"001\", \"010\", \"011\", \"100\", \"101\", \"110\", \"111\"]\n",
    "\n",
    "y_test = np.array([y for _, y in test_dataset]).astype(int)\n",
    "y_preds = np.array([p for p in predictions]).astype(int)\n",
    "\n",
    "y_test = [''.join(map(str, label)) for label in y_test]\n",
    "y_preds = [''.join(map(str, label)) for label in y_preds]\n",
    "\n",
    "\n",
    "# Compute confusion matrix with all possible labels\n",
    "conf_matrix = confusion_matrix(y_test, y_preds, labels=categories)\n",
    "row_sums = conf_matrix.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / row_sums\n",
    "conf_matrix_normalized[conf_matrix == 0] = np.nan\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8,6), dpi=100)\n",
    "ax = sns.heatmap(conf_matrix_normalized, annot=True, fmt=\".2f\", mask=np.isnan(conf_matrix_normalized))\n",
    "\n",
    "# Set tick labels\n",
    "ax.set_xticks(np.arange(len(categories)) + 0.5)\n",
    "ax.set_yticks(np.arange(len(categories)) + 0.5)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_yticklabels(categories)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
